import argparse
from pathlib import Path
import os
import warnings

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")

############################################################
# Helper: Hampel filter for outlier smoothing
############################################################

def hampel_filter(series: pd.Series, window_size: int = 24, n_sigmas: float = 3.0) -> pd.Series:
    """ê°„ë‹¨í•œ Hampel í•„í„° êµ¬í˜„ (window_size ì‹œê°„ ì°½ ê¸°ì¤€)"""
    rolling_median = series.rolling(window=window_size, center=True).median()
    diff = np.abs(series - rolling_median)
    mad = diff.rolling(window=window_size, center=True).median()
    threshold = n_sigmas * 1.4826 * mad  # 1.4826: MAD -> std ê·¼ì‚¬
    outlier_idx = diff > threshold
    series_clean = series.copy()
    series_clean[outlier_idx] = rolling_median[outlier_idx]
    return series_clean

############################################################
# Feature Engineering (ì‹œê°„ & ë‚ ì”¨) â€“ test5.py ì—ì„œ ì‚¬ìš©í•œ í•¨ìˆ˜ ì¼ë¶€ ì¬í™œìš©
############################################################

KR_HOLIDAYS = None  # holidays íŒ¨í‚¤ì§€ ì‚¬ìš©ì€ ì§€ì—° import(ì†ë„)


def add_time_features(df: pd.DataFrame) -> pd.DataFrame:
    dt = df["ì¼ì‹œ"]
    df["month"] = dt.dt.month
    df["day"] = dt.dt.day
    df["hour"] = dt.dt.hour
    df["weekday"] = dt.dt.weekday
    df["is_weekend"] = (df["weekday"] >= 5).astype(int)
    global KR_HOLIDAYS
    if KR_HOLIDAYS is None:
        import holidays
        KR_HOLIDAYS = holidays.KR()
    df["is_holiday"] = dt.dt.date.map(lambda d: int(d in KR_HOLIDAYS))
    # Fourier ë³€í™˜
    df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
    df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)
    df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12)
    df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12)
    df["weekday_sin"] = np.sin(2 * np.pi * df["weekday"] / 7)
    df["weekday_cos"] = np.cos(2 * np.pi * df["weekday"] / 7)
    # ì¶”ì„¸ìš©
    df["day_of_year"] = dt.dt.dayofyear
    df["week_of_year"] = dt.dt.isocalendar().week.astype(int)
    df["time_idx"] = (dt - dt.min()).dt.days
    return df


def add_weather_features(df: pd.DataFrame) -> pd.DataFrame:
    # ê°€ì •: ê¸°ì˜¨(Â°C), ìŠµë„(%), í’ì†(m/s) ì»¬ëŸ¼ì´ ì¡´ì¬
    df["THI"] = 9 / 5 * df["ê¸°ì˜¨(Â°C)"] - 0.55 * (1 - df["ìŠµë„(%)"] / 100) * (9 / 5 * df["ê¸°ì˜¨(Â°C)"] - 26) + 32
    df["dew_point"] = df["ê¸°ì˜¨(Â°C)"] - (100 - df["ìŠµë„(%)"]) / 5
    df["heat_index"] = 0.5 * (df["ê¸°ì˜¨(Â°C)"] + 61.0 + (df["ê¸°ì˜¨(Â°C)"] - 68.0) * 1.2 + df["ìŠµë„(%)"] * 0.094)
    # Degree Days
    df["HDD"] = (18 - df["ê¸°ì˜¨(Â°C)"]).clip(lower=0)
    df["CDD"] = (df["ê¸°ì˜¨(Â°C)"] - 22).clip(lower=0)
    # ì˜¨ë„ êµ¬ê°„ (Bin)
    bins = [-50, 10, 15, 20, 25, 30, 60]
    labels = ["temp_bin_1", "temp_bin_2", "temp_bin_3", "temp_bin_4", "temp_bin_5", "temp_bin_6"]
    df["temp_bin"] = pd.cut(df["ê¸°ì˜¨(Â°C)"], bins=bins, labels=labels)
    return df

############################################################
# Lag & Rolling ìƒì„±ê¸°
############################################################

def create_lag_rolling(df: pd.DataFrame, group_cols: list, target_cols: list) -> pd.DataFrame:
    """ì£¼ì–´ì§„ íƒ€ê²Ÿ ì»¬ëŸ¼ì— ëŒ€í•´ ë™ê³¼ ë¡¤ë§ í”¼ì²˜ë¥¼ ìƒì„±í•œë‹¤."""
    for col in target_cols:
        for lag in [1, 2, 3, 24, 48, 168]:
            df[f"{col}_lag_{lag}"] = df.groupby(group_cols)[col].shift(lag)
        # rolling 6/24 window mean & std (ë‹¨ê¸°, ì¼ê°„)
        for win in [6, 24]:
            df[f"{col}_roll_mean_{win}"] = (
                df.groupby(group_cols)[col].rolling(window=win).mean().reset_index(level=0, drop=True)
            )
            df[f"{col}_roll_std_{win}"] = (
                df.groupby(group_cols)[col].rolling(window=win).std().reset_index(level=0, drop=True)
            )
    return df

############################################################
# ë©”ì¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
############################################################

def preprocess(train_path: Path, test_path: Path, info_path: Path, output_dir: Path):
    print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘ ...")
    train_df = pd.read_csv(train_path, parse_dates=["ì¼ì‹œ"], low_memory=False)
    test_df = pd.read_csv(test_path, parse_dates=["ì¼ì‹œ"], low_memory=False)
    info_df = pd.read_csv(info_path)

    # dtype ì„¤ì •
    train_df["ê±´ë¬¼ë²ˆí˜¸"] = train_df["ê±´ë¬¼ë²ˆí˜¸"].astype("category")
    test_df["ê±´ë¬¼ë²ˆí˜¸"] = test_df["ê±´ë¬¼ë²ˆí˜¸"].astype("category")
    info_df["ê±´ë¬¼ë²ˆí˜¸"] = info_df["ê±´ë¬¼ë²ˆí˜¸"].astype("category")
    info_df["ê±´ë¬¼ìœ í˜•"] = info_df["ê±´ë¬¼ìœ í˜•"].astype("category")

    ########################################################
    # 1) ê²°ì¸¡ì¹˜ ê¸°í˜¸ "-" ì²˜ë¦¬ ë° í”Œë˜ê·¸ ìƒì„±
    ########################################################
    miss_cols = ["íƒœì–‘ê´‘ìš©ëŸ‰(kW)", "ESSì €ì¥ìš©ëŸ‰(kWh)", "PCSìš©ëŸ‰(kW)"]
    for c in miss_cols:
        info_df[f"{c}_missing"] = (info_df[c] == "-").astype(int)
    num_cols = ["ì—°ë©´ì (m2)", "ëƒ‰ë°©ë©´ì (m2)"] + miss_cols
    info_df[num_cols] = info_df[num_cols].replace("-", np.nan).astype(float)

    # ê±´ë¬¼ìœ í˜•ë³„ ì¤‘ì•™ê°’ìœ¼ë¡œ ë³´ê°„
    for col in num_cols:
        info_df[col] = info_df.groupby("ê±´ë¬¼ìœ í˜•")[col].transform(lambda x: x.fillna(x.median()))

    ########################################################
    # 2) train / test í•©ì¹˜ê¸° & ì „ë ¥ì†Œë¹„ í´ë¦¬í•‘
    ########################################################
    test_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"] = np.nan
    all_df = pd.concat([train_df, test_df], ignore_index=True)
    all_df = all_df.merge(info_df, on="ê±´ë¬¼ë²ˆí˜¸", how="left")

    # ìŒìˆ˜ ì œê±°, ê·¹ë‹¨ì¹˜ winsorize
    all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"] = all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].clip(lower=0)
    # winsorize per building (99.5 pct)
    def _winsorize(group):
        if group.isna().all():
            return group
        cap = group.quantile(0.995)
        return group.clip(upper=cap)
    all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"] = all_df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].transform(_winsorize)

    ########################################################
    # 3) Hampel í•„í„° (log-space) â€“ train ì˜ì—­ë§Œ ì ìš©
    ########################################################
    mask_train = ~all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].isna()
    temp_series = np.log1p(all_df.loc[mask_train, "ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"])
    all_df.loc[mask_train, "ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"] = np.expm1(hampel_filter(temp_series))

    ########################################################
    # 4) íƒ€ê²Ÿ ë³€í™˜ & ë©´ì /ìš©ëŸ‰ ë¡œê·¸ ë³€í™˜
    ########################################################
    all_df["log_power"] = np.log1p(all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"])
    for col in ["ì—°ë©´ì (m2)", "ëƒ‰ë°©ë©´ì (m2)", "íƒœì–‘ê´‘ìš©ëŸ‰(kW)", "ESSì €ì¥ìš©ëŸ‰(kWh)", "PCSìš©ëŸ‰(kW)"]:
        all_df[f"log_{col}"] = np.log1p(all_df[col])
    all_df["cooling_ratio"] = all_df["ëƒ‰ë°©ë©´ì (m2)"] / (all_df["ì—°ë©´ì (m2)"] + 1e-6)

    ########################################################
    # 5) ì‹œê°„ & ë‚ ì”¨ íŒŒìƒ
    ########################################################
    all_df = add_time_features(all_df)
    all_df = add_weather_features(all_df)

    ########################################################
    # 6) ì‹œì°¨ / ë¡¤ë§ í”¼ì²˜ (ì£¼ìš” ë³€ìˆ˜)
    ########################################################
    all_df = create_lag_rolling(
        all_df,
        group_cols=["ê±´ë¬¼ë²ˆí˜¸"],
        target_cols=["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)", "ê¸°ì˜¨(Â°C)", "ìŠµë„(%)"]
    )

    ########################################################
    # 7) ë²”ì£¼í˜• temp_bin, ê±´ë¬¼ìœ í˜•, ê±´ë¬¼ë²ˆí˜¸ ìœ ì§€
    ########################################################
    all_df["temp_bin"] = all_df["temp_bin"].astype("category")

    ########################################################
    # 8) ìºì‹± â€“ train / test ë¶„ë¦¬ ì €ì¥
    ########################################################
    output_dir.mkdir(parents=True, exist_ok=True)
    df_train = all_df[mask_train].copy()
    df_test = all_df[~mask_train].copy()
    train_out = output_dir / "train_preprocessed.parquet"
    test_out = output_dir / "test_preprocessed.parquet"
    df_train.to_parquet(train_out, index=False)
    df_test.to_parquet(test_out, index=False)
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ â†’ {train_out.name}, {test_out.name} ì €ì¥")

############################################################
# CLI
############################################################

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ì—ë„ˆì§€ ì†Œë¹„ ì˜ˆì¸¡ìš© ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (method_04)")
    parser.add_argument("--train", type=Path, default=Path("data/train.csv"))
    parser.add_argument("--test", type=Path, default=Path("data/test.csv"))
    parser.add_argument("--info", type=Path, default=Path("data/building_info.csv"))
    parser.add_argument("--out", type=Path, default=Path("method_04/cache"))
    args = parser.parse_args()

    preprocess(args.train, args.test, args.info, args.out) 