import argparse
from pathlib import Path
import warnings
import gc
from typing import List
import os

import numpy as np
import pandas as pd

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.linear_model import ElasticNetCV

import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, Pool

warnings.filterwarnings("ignore")

SEED = 42

############################################################
# GPU ìƒíƒœ í™•ì¸ ë° ìµœì í™” í•¨ìˆ˜
############################################################

def check_gpu_availability():
    """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ì™€ ìµœì  ì„¤ì •ì„ í™•ì¸"""
    gpu_info = {
        "lightgbm_device": "cpu",
        "xgb_tree_method": "hist", 
        "catboost_task_type": "CPU",
        "gpu_available": False
    }
    
    # CUDA ì²´í¬ (nvidia-ml-py ëŒ€ì‹  ì§ì ‘ ì²´í¬)
    cuda_available = False
    try:
        # CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì²´í¬
        cuda_paths = ["/usr/local/cuda/lib64", "/usr/lib/x86_64-linux-gnu"]
        for path in cuda_paths:
            if os.path.exists(f"{path}/libcuda.so") or os.path.exists(f"{path}/libcuda.so.1"):
                cuda_available = True
                break
        
        if cuda_available:
            print("ğŸ¯ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°œê²¬")
            gpu_info["xgb_tree_method"] = "gpu_hist"
            gpu_info["catboost_task_type"] = "GPU"
            gpu_info["gpu_available"] = True
            
            # LightGBMì€ OpenCL ë°±ì—”ë“œë¡œ ì‹œë„ (CUDA ë¹Œë“œ ë¬¸ì œ íšŒí”¼)
            gpu_info["lightgbm_device"] = "gpu"  # OpenCL ë°±ì—”ë“œ ìš°ì„  ì‹œë„
        else:
            print("âš ï¸ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
    except Exception as e:
        print(f"âš ï¸ CUDA ì²´í¬ ì‹¤íŒ¨: {e}")
    
    # OpenCL ì²´í¬
    try:
        import pyopencl as cl
        platforms = cl.get_platforms()
        if platforms:
            devices = []
            for platform in platforms:
                try:
                    platform_devices = platform.get_devices(cl.device_type.GPU)
                    devices.extend(platform_devices)
                except:
                    pass
            
            if devices:
                print(f"ğŸ¯ OpenCL GPU ë°œê²¬: {len(devices)}ê°œ ë””ë°”ì´ìŠ¤")
                gpu_info["lightgbm_device"] = "gpu"
                gpu_info["gpu_available"] = True
            else:
                print("âš ï¸ OpenCL GPU ë””ë°”ì´ìŠ¤ ì—†ìŒ")
    except ImportError:
        print("âš ï¸ pyopencl ì—†ìŒ - OpenCL GPU ì‚¬ìš© ë¶ˆê°€")
    except Exception as e:
        print(f"âš ï¸ OpenCL ì²´í¬ ì‹¤íŒ¨: {e}")
    
    return gpu_info

############################################################
# Helper: ì¹´í…Œê³ ë¦¬í˜• ì»¬ëŸ¼ì„ XGBoost ì…ë ¥ìš©ìœ¼ë¡œ int ì½”ë“œ ë³€í™˜
############################################################

def encode_categories(df: pd.DataFrame, cat_cols: List[str]) -> pd.DataFrame:
    """category dtype -> int ì½”ë“œ, NaNì€ -1"""
    df_enc = df.copy()
    for c in cat_cols:
        if str(df_enc[c].dtype) == "category":
            # cat.codesì—ì„œ -1(missing)ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€
            df_enc[c] = df_enc[c].cat.codes.astype("int32")
        else:
            # ì´ë¯¸ ìˆ«ìí˜•ì¸ ê²½ìš° (ê±´ë¬¼ë²ˆí˜¸ ë“±) ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ int32ë¡œ ë³€í™˜
            df_enc[c] = df_enc[c].astype("int32")
    return df_enc

############################################################
# í‰ê°€ ì§€í‘œ â€“ SMAPE (competition metric)
############################################################

def smape_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    eps = 1e-8
    y_pred = np.maximum(y_pred, 0)
    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + eps)) * 100

############################################################
# ê°œì„ ëœ í•™ìŠµ í•¨ìˆ˜
############################################################

def train_fold(X_tr, y_tr, X_val, y_val, categorical_features, gpu_info):
    models = {}
    predictions = {}
    # ë””ë²„ê·¸: í•™ìŠµ/ê²€ì¦ ë°ì´í„° í¬ê¸° ë¡œê·¸
    print(f"    â–¶ï¸ Train rows: {len(y_tr)}, Val rows: {len(y_val)}")
    
    # 1. LightGBM - ì¼ë‹¨ CPUë¡œ ê°•ì œ ì„¤ì • (GPU í•™ìŠµ ë¬¸ì œ ë•Œë¬¸)
    print("  ğŸš€ LightGBM í•™ìŠµ...")
    try:
        lgb_params = {
            "objective": "regression",
            "metric": "l1",
            "random_state": SEED,
            "learning_rate": 0.05,
            "num_leaves": 256,
            "max_depth": -1,
            "n_estimators": 8000,
            "device": "cpu",  # ê°•ì œ CPU ëª¨ë“œ
            "verbose": -1,
        }
        
        # CPU ëª¨ë“œì´ë¯€ë¡œ ì¶”ê°€ íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš”
        lgb_model = lgb.LGBMRegressor(**lgb_params)
        lgb_model.fit(
            X_tr,
            y_tr,
            eval_set=[(X_val, y_val)],
            eval_metric="mae",
            categorical_feature=categorical_features,
            callbacks=[lgb.early_stopping(300, verbose=False)],
        )
        
        models["lgb"] = lgb_model
        predictions["lgb"] = lgb_model.predict(X_val)
        print("    âœ… LightGBM ì™„ë£Œ (device: cpu)")
        # ë””ë²„ê·¸: best iteration ë° ì˜ˆì¸¡ ë¶„ì‚° í™•ì¸
        best_iter = getattr(lgb_model, "best_iteration_", lgb_model.n_estimators_)
        pred_std = np.std(predictions["lgb"])
        print(f"       â®‘ best_iter={best_iter}, val_pred_std={pred_std:.4f}")
        if pred_std < 1e-6:
            print("       âš ï¸ LightGBM ì˜ˆì¸¡ì´ ìƒìˆ˜ì— ê°€ê¹ìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"    âŒ LightGBM GPU ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {e}")
        # CPU ë°±ì—…
        lgb_params_cpu = {
            "objective": "regression",
            "metric": "l1",
            "random_state": SEED,
            "learning_rate": 0.05,
            "num_leaves": 256,
            "max_depth": -1,
            "n_estimators": 8000,
            "device": "cpu",
            "verbose": -1,
        }
        
        lgb_model = lgb.LGBMRegressor(**lgb_params_cpu)
        lgb_model.fit(
            X_tr, y_tr,
            eval_set=[(X_val, y_val)],
            eval_metric="mae", 
            categorical_feature=categorical_features,
            callbacks=[lgb.early_stopping(300, verbose=False)],
        )
        models["lgb"] = lgb_model
        predictions["lgb"] = lgb_model.predict(X_val)
        print("    âœ… LightGBM CPU ì™„ë£Œ")

    # 2. XGBoost with improved GPU settings  
    print("  ğŸš€ XGBoost í•™ìŠµ...")

    # ì¹´í…Œê³ ë¦¬ â†’ int ì½”ë“œ ë³€í™˜ (GPU categorical ë¯¸ì§€ì› ëŒ€ë¹„)
    X_tr_enc = encode_categories(X_tr, categorical_features)
    X_val_enc = encode_categories(X_val, categorical_features)

    try:
        xgb_params = {
            "objective": "reg:squarederror",
            "tree_method": gpu_info["xgb_tree_method"],
            "random_state": SEED,
            "learning_rate": 0.05,
            "max_depth": 8,
            "n_estimators": 8000,
            "early_stopping_rounds": 300,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "reg_lambda": 1.0,
            "reg_alpha": 0.0,
            "verbosity": 1,  # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ìš©
        }
        
        if gpu_info["xgb_tree_method"] == "gpu_hist":
            xgb_params.update({
                "predictor": "gpu_predictor",
                "gpu_id": 0,
            })
        
        xgb_model = xgb.XGBRegressor(**xgb_params)
        xgb_model.fit(X_tr_enc, y_tr, eval_set=[(X_val_enc, y_val)], verbose=False)
        
        models["xgb"] = xgb_model
        predictions["xgb"] = xgb_model.predict(X_val_enc)
        print(f"    âœ… XGBoost ì™„ë£Œ (method: {gpu_info['xgb_tree_method']})")
        # ë””ë²„ê·¸: best iteration ë° ì˜ˆì¸¡ ë¶„ì‚°
        best_iter_xgb = getattr(xgb_model, "best_iteration", None)
        pred_std_xgb = np.std(predictions["xgb"])
        print(f"       â®‘ best_iter={best_iter_xgb}, val_pred_std={pred_std_xgb:.4f}")
        if pred_std_xgb < 1e-6:
            print("       âš ï¸ XGBoost ì˜ˆì¸¡ì´ ìƒìˆ˜ì— ê°€ê¹ìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"    âŒ XGBoost GPU ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {e}")
        # CPU ë°±ì—…
        xgb_params["tree_method"] = "hist"
        xgb_params.pop("predictor", None)
        xgb_params.pop("gpu_id", None)
        
        xgb_model = xgb.XGBRegressor(**xgb_params)
        xgb_model.fit(X_tr_enc, y_tr, eval_set=[(X_val_enc, y_val)], verbose=False)
        models["xgb"] = xgb_model
        predictions["xgb"] = xgb_model.predict(X_val_enc)
        print("    âœ… XGBoost CPU ì™„ë£Œ")

    # 3. CatBoost with improved GPU settings
    print("  ğŸš€ CatBoost í•™ìŠµ...")
    try:
        cat_features_idx = [X_tr.columns.get_loc(col) for col in categorical_features]
        cat_params = {
            "loss_function": "MAE",
            "iterations": 8000,
            "early_stopping_rounds": 300,
            "learning_rate": 0.05,
            "depth": 8,
            "random_seed": SEED,
            "task_type": gpu_info["catboost_task_type"],
            "verbose": 100,  # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸ìš©
        }
        
        if gpu_info["catboost_task_type"] == "GPU":
            cat_params["devices"] = "0"
        
        cat_model = CatBoostRegressor(**cat_params)
        cat_model.fit(
            Pool(X_tr, y_tr, cat_features=cat_features_idx),
            eval_set=Pool(X_val, y_val, cat_features=cat_features_idx),
            verbose=False
        )
        
        models["cat"] = cat_model
        predictions["cat"] = cat_model.predict(X_val)
        print(f"    âœ… CatBoost ì™„ë£Œ (task_type: {gpu_info['catboost_task_type']})")
        # ë””ë²„ê·¸: best iteration ë° ì˜ˆì¸¡ ë¶„ì‚°
        best_iter_cat = cat_model.get_best_iteration()
        pred_std_cat = np.std(predictions["cat"])
        print(f"       â®‘ best_iter={best_iter_cat}, val_pred_std={pred_std_cat:.4f}")
        if pred_std_cat < 1e-6:
            print("       âš ï¸ CatBoost ì˜ˆì¸¡ì´ ìƒìˆ˜ì— ê°€ê¹ìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"    âŒ CatBoost GPU ì‹¤íŒ¨, CPUë¡œ ì¬ì‹œë„: {e}")
        # CPU ë°±ì—…
        cat_params["task_type"] = "CPU"
        cat_params.pop("devices", None)
        
        cat_model = CatBoostRegressor(**cat_params)
        cat_model.fit(
            Pool(X_tr, y_tr, cat_features=cat_features_idx),
            eval_set=Pool(X_val, y_val, cat_features=cat_features_idx),
            verbose=False
        )
        models["cat"] = cat_model
        predictions["cat"] = cat_model.predict(X_val)
        print("    âœ… CatBoost CPU ì™„ë£Œ")

    return models, np.column_stack([predictions["lgb"], predictions["xgb"], predictions["cat"]])

############################################################
# ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
############################################################

def main(train_path: str, test_path: str, submission_path: str):
    print("ğŸ” GPU í™˜ê²½ ì²´í¬ ì¤‘...")
    gpu_info = check_gpu_availability()
    
    if gpu_info["gpu_available"]:
        print("ğŸ¯ GPU ê°€ì† ëª¨ë“œ í™œì„±í™”!")
        print(f"  - LightGBM: {gpu_info['lightgbm_device']}")
        print(f"  - XGBoost: {gpu_info['xgb_tree_method']}")  
        print(f"  - CatBoost: {gpu_info['catboost_task_type']}")
    else:
        print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ (GPU ë¯¸ì‚¬ìš©)")
    print()
    
    print("ğŸ“¦ ë°ì´í„° ë¡œë“œ...")
    train_df = pd.read_parquet(train_path)
    test_df = pd.read_parquet(test_path)

    # test ë°ì´í„°ì— num_date_timeì´ ì—†ìœ¼ë©´ ìƒì„±
    if "num_date_time" not in test_df.columns:
        if "ì¼ì‹œ" in test_df.columns and "ê±´ë¬¼ë²ˆí˜¸" in test_df.columns:
            test_df["num_date_time"] = (
                test_df["ê±´ë¬¼ë²ˆí˜¸"].astype(str) + "_" + 
                test_df["ì¼ì‹œ"].dt.strftime("%Y%m%d %H")
            )
        else:
            raise ValueError("test ë°ì´í„°ì— num_date_time ì»¬ëŸ¼ì´ ì—†ê³  ì¼ì‹œ/ê±´ë¬¼ë²ˆí˜¸ë¡œ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íƒ€ê²Ÿ & í”¼ì²˜ ë¶„ë¦¬
    # ì „ì²˜ë¦¬ì—ì„œ ìƒì„±í•œ ë¡œê·¸ ë³€í™˜ íƒ€ê²Ÿ ì»¬ëŸ¼ëª… í™•ì¸
    if "log_power" in train_df.columns:
        target_col = "log_power"
    elif "log_ì „ë ¥ì†Œë¹„ëŸ‰(kWh)" in train_df.columns:
        target_col = "log_ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"
    else:
        raise ValueError("ë¡œê·¸ ë³€í™˜ëœ íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    drop_cols = ["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)", "ì¼ì‹œ", "num_date_time"]  # ì›ë³¸ íƒ€ê²ŸÂ·ì‹œê°„Â·ID ì—´ ì œì™¸
    feature_cols = [c for c in train_df.columns if c not in drop_cols + [target_col]]

    # ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ìë™ ê°ì§€ (dtype == 'category')
    categorical_cols = [c for c in feature_cols if str(train_df[c].dtype) == "category"]
    # ê±´ë¬¼ë²ˆí˜¸ëŠ” í•„ìˆ˜ ë²”ì£¼í˜• í”¼ì²˜ë¡œ ì¶”ê°€ (ì „ì²˜ë¦¬ì—ì„œ categoryë¡œ ì„¤ì •)
    if "ê±´ë¬¼ë²ˆí˜¸" in feature_cols and "ê±´ë¬¼ë²ˆí˜¸" not in categorical_cols:
        categorical_cols.append("ê±´ë¬¼ë²ˆí˜¸")
    print(f"Categorical features: {categorical_cols}")

    X = train_df[feature_cols]
    y = train_df[target_col]

    print("ğŸš€ 5-Fold êµì°¨ê²€ì¦...")
    tscv = TimeSeriesSplit(n_splits=5, test_size=None)
    oof_preds = np.zeros((len(y), 3))  # LGB, XGB, Cat ì˜ˆì¸¡ê°’
    test_preds = np.zeros((len(test_df), 3))
    base_models_per_fold = []  # ê° foldì˜ ëª¨ë¸ë“¤ì„ ì €ì¥
    
    n_splits = 5
    for fold, (tr_idx, val_idx) in enumerate(tscv.split(X)):
        print(f"ğŸš€ Fold {fold+1}/{n_splits}")
        
        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]
        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]

        models, pred_val = train_fold(X_tr, y_tr, X_val, y_val, categorical_cols, gpu_info)
        oof_preds[val_idx, :] = pred_val

        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (í‰ê· )
        # LightGBM & CatBoost: ì›ë³¸ test_df ì‚¬ìš©, XGBoost: ì¸ì½”ë”© í•„ìš”
        test_df_enc = encode_categories(test_df[feature_cols], categorical_cols)
        preds_list = []
        for name, model in models.items():
            if name == "xgb":
                preds_list.append(model.predict(test_df_enc))
            else:
                preds_list.append(model.predict(test_df[feature_cols]))
        fold_test_pred = np.column_stack(preds_list)
        test_preds += fold_test_pred / n_splits

        base_models_per_fold.append(models)
        gc.collect()

        smape_lgb = smape_np(np.expm1(y_val), np.expm1(pred_val[:, 0]))
        smape_xgb = smape_np(np.expm1(y_val), np.expm1(pred_val[:, 1]))
        smape_cat = smape_np(np.expm1(y_val), np.expm1(pred_val[:, 2]))
        print(f"   SMAPE â€“ LGB {smape_lgb:.3f} | XGB {smape_xgb:.3f} | CAT {smape_cat:.3f}")

    # Meta model (ElasticNet)
    print("\nğŸ”— ìŠ¤íƒœí‚¹ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ (ElasticNetCV)...")
    enet = ElasticNetCV(l1_ratio=[0.01, 0.5, 1.0], cv=3, random_state=SEED)
    enet.fit(oof_preds, y)
    oof_meta = enet.predict(oof_preds)
    score_meta = smape_np(np.expm1(y), np.expm1(oof_meta))
    print(f"âœ… Meta SMAPE: {score_meta:.3f}%")

    # Test meta predictions
    test_meta = enet.predict(test_preds)
    # log_power = log1p(ì „ë ¥ì†Œë¹„ëŸ‰)ì´ë¯€ë¡œ expm1ë¡œ ì—­ë³€í™˜í•˜ë©´ ë¨
    # ì—°ë©´ì ì„ ê³±í•  í•„ìš” ì—†ìŒ (ì´ë¯¸ ì „ë ¥ì†Œë¹„ëŸ‰ ìì²´ë¥¼ ì˜ˆì¸¡)
    final_pred_kwh = np.expm1(test_meta)

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.DataFrame({
        "num_date_time": test_df["num_date_time"],
        "answer": np.clip(final_pred_kwh, 0, None)
    })
    submission.to_csv(submission_path, index=False)
    print(f"ğŸ‰ Submission saved to {submission_path}")

############################################################
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GPU Stacking Model Trainer (method_04)")
    parser.add_argument("--train", type=Path, default=Path("method_04/cache/train_preprocessed.parquet"))
    parser.add_argument("--test", type=Path, default=Path("method_04/cache/test_preprocessed.parquet"))
    parser.add_argument("--sub", type=Path, default=Path("submission_stack.csv"))
    args = parser.parse_args()

    main(args.train, args.test, args.sub) 