import argparse
from pathlib import Path
import warnings
import gc
from typing import List

import numpy as np
import pandas as pd

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.linear_model import ElasticNetCV

import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, Pool

warnings.filterwarnings("ignore")

SEED = 42

############################################################
# í‰ê°€ ì§€í‘œ â€“ SMAPE (competition metric)
############################################################

def smape_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    eps = 1e-8
    y_pred = np.maximum(y_pred, 0)
    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + eps)) * 100

############################################################
# í•™ìŠµ í•¨ìˆ˜ (fold ë‹¨ìœ„)
############################################################

def train_fold(X_tr, y_tr, X_val, y_val, categorical_features: List[str]):
    """ê° ëª¨ë¸ì„ í•™ìŠµí•˜ê³  validation ì˜ˆì¸¡ ë°˜í™˜"""

    ##### LightGBM #####
    lgb_params = {
        "objective": "regression_l1",
        "metric": "mae",
        "random_state": SEED,
        "learning_rate": 0.05,
        "num_leaves": 256,
        "max_depth": -1,
        "n_estimators": 8000,
        "device": "gpu",
        "gpu_use_dp": True,
        "gpu_platform_id": 0,
        "gpu_device_id": 0,
        "max_bin": 255,
        "verbose": -1,
    }
    lgb_model = lgb.LGBMRegressor(**lgb_params)
    lgb_model.fit(
        X_tr,
        y_tr,
        eval_set=[(X_val, y_val)],
        eval_metric="mae",
        categorical_feature=categorical_features,
        callbacks=[lgb.early_stopping(300, verbose=False)],
    )
    pred_lgb = lgb_model.predict(X_val)

    ##### XGBoost #####
    xgb_model = xgb.XGBRegressor(
        objective="reg:squarederror",
        tree_method="gpu_hist",
        predictor="gpu_predictor",
        gpu_id=0,
        random_state=SEED,
        learning_rate=0.05,
        max_depth=8,
        n_estimators=8000,
        early_stopping_rounds=300,  # XGBoost 3.xì—ì„œëŠ” ìƒì„±ìì—ì„œ ì„¤ì •
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=1.0,
        reg_alpha=0.0,
        verbosity=0,
    )
    xgb_model.fit(
        X_tr,
        y_tr,
        eval_set=[(X_val, y_val)],
        verbose=False
    )
    pred_xgb = xgb_model.predict(X_val)

    ##### CatBoost #####
    cat_features_idx = [X_tr.columns.get_loc(col) for col in categorical_features]
    cat_model = CatBoostRegressor(
        loss_function="MAE",
        iterations=8000,
        early_stopping_rounds=300,  # CatBoostë„ ìƒì„±ìì—ì„œ ì„¤ì •
        learning_rate=0.05,
        depth=8,
        random_seed=SEED,
        task_type="GPU",
        devices="0",
        verbose=False,
    )
    cat_model.fit(
        Pool(X_tr, y_tr, cat_features=cat_features_idx),
        eval_set=Pool(X_val, y_val, cat_features=cat_features_idx),
        verbose=False
    )
    pred_cat = cat_model.predict(X_val)

    # return models and predictions
    return (lgb_model, xgb_model, cat_model), np.vstack([pred_lgb, pred_xgb, pred_cat]).T

############################################################
# ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
############################################################

def main(train_path: Path, test_path: Path, out_path: Path):
    print("ğŸ“¦ ë°ì´í„° ë¡œë“œ...")
    train_df = pd.read_parquet(train_path)
    test_df = pd.read_parquet(test_path)

    # test ë°ì´í„°ì— num_date_timeì´ ì—†ìœ¼ë©´ ìƒì„±
    if "num_date_time" not in test_df.columns:
        if "ì¼ì‹œ" in test_df.columns and "ê±´ë¬¼ë²ˆí˜¸" in test_df.columns:
            test_df["num_date_time"] = (
                test_df["ê±´ë¬¼ë²ˆí˜¸"].astype(str) + "_" + 
                test_df["ì¼ì‹œ"].dt.strftime("%Y%m%d %H")
            )
        else:
            raise ValueError("test ë°ì´í„°ì— num_date_time ì»¬ëŸ¼ì´ ì—†ê³  ì¼ì‹œ/ê±´ë¬¼ë²ˆí˜¸ë¡œ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # íƒ€ê²Ÿ & í”¼ì²˜ ë¶„ë¦¬
    # ì „ì²˜ë¦¬ì—ì„œ ìƒì„±í•œ ë¡œê·¸ ë³€í™˜ íƒ€ê²Ÿ ì»¬ëŸ¼ëª… í™•ì¸
    if "log_power" in train_df.columns:
        target_col = "log_power"
    elif "log_ì „ë ¥ì†Œë¹„ëŸ‰(kWh)" in train_df.columns:
        target_col = "log_ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"
    else:
        raise ValueError("ë¡œê·¸ ë³€í™˜ëœ íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    drop_cols = ["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)", "ì¼ì‹œ", "num_date_time"]  # ì›ë³¸ íƒ€ê²ŸÂ·ì‹œê°„Â·ID ì—´ ì œì™¸
    feature_cols = [c for c in train_df.columns if c not in drop_cols + [target_col]]

    # ì¹´í…Œê³ ë¦¬ í”¼ì²˜ ìë™ ê°ì§€ (dtype == 'category')
    categorical_cols = [c for c in feature_cols if str(train_df[c].dtype) == "category"]
    # ê±´ë¬¼ë²ˆí˜¸ëŠ” í•„ìˆ˜ ë²”ì£¼í˜• í”¼ì²˜ë¡œ ì¶”ê°€ (ì „ì²˜ë¦¬ì—ì„œ categoryë¡œ ì„¤ì •)
    if "ê±´ë¬¼ë²ˆí˜¸" in feature_cols and "ê±´ë¬¼ë²ˆí˜¸" not in categorical_cols:
        categorical_cols.append("ê±´ë¬¼ë²ˆí˜¸")
    print(f"Categorical features: {categorical_cols}")

    X = train_df[feature_cols]
    y = train_df[target_col]

    n_splits = 5
    tscv = TimeSeriesSplit(n_splits=n_splits)

    oof_preds = np.zeros((len(train_df), 3))  # 3 base models
    test_preds = np.zeros((len(test_df), 3))
    base_models_per_fold = []

    fold = 0
    for tr_idx, val_idx in tscv.split(X):
        fold += 1
        print(f"\nğŸš€ Fold {fold}/{n_splits}")
        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]

        models, pred_val = train_fold(X_tr, y_tr, X_val, y_val, categorical_cols)
        oof_preds[val_idx, :] = pred_val

        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (í‰ê· )
        fold_test_pred = np.column_stack([m.predict(test_df[feature_cols]) for m in models])
        test_preds += fold_test_pred / n_splits

        base_models_per_fold.append(models)
        gc.collect()

        smape_lgb = smape_np(np.expm1(y_val), np.expm1(pred_val[:, 0]))
        smape_xgb = smape_np(np.expm1(y_val), np.expm1(pred_val[:, 1]))
        smape_cat = smape_np(np.expm1(y_val), np.expm1(pred_val[:, 2]))
        print(f"   SMAPE â€“ LGB {smape_lgb:.3f} | XGB {smape_xgb:.3f} | CAT {smape_cat:.3f}")

    # Meta model (ElasticNet)
    print("\nğŸ”— ìŠ¤íƒœí‚¹ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ (ElasticNetCV)...")
    enet = ElasticNetCV(l1_ratio=[0.0, 0.5, 1.0], cv=3, random_state=SEED)
    enet.fit(oof_preds, y)
    oof_meta = enet.predict(oof_preds)
    score_meta = smape_np(np.expm1(y), np.expm1(oof_meta))
    print(f"âœ… Meta SMAPE: {score_meta:.3f}%")

    # Test meta predictions
    test_meta = enet.predict(test_preds)
    # log_power = log(power_per_area) ì´ë¯€ë¡œ ë©´ì ì„ ê³±í•´ì•¼ í•¨
    # ë©´ì ë„ ë¡œê·¸ ë³€í™˜ë˜ì—ˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
    if "log_ì—°ë©´ì (m2)" in test_df.columns:
        area_col = np.expm1(test_df["log_ì—°ë©´ì (m2)"])
    else:
        area_col = test_df["ì—°ë©´ì (m2)"]
    final_pred_kwh = np.expm1(test_meta) * area_col

    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.DataFrame({
        "num_date_time": test_df["num_date_time"],
        "answer": final_pred_kwh.clip(lower=0)
    })
    submission.to_csv(out_path, index=False)
    print(f"ğŸ‰ Submission saved to {out_path}")

############################################################
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GPU Stacking Model Trainer (method_04)")
    parser.add_argument("--train", type=Path, default=Path("method_04/cache/train_preprocessed.parquet"))
    parser.add_argument("--test", type=Path, default=Path("method_04/cache/test_preprocessed.parquet"))
    parser.add_argument("--sub", type=Path, default=Path("submission_stack.csv"))
    args = parser.parse_args()

    main(args.train, args.test, args.sub) 