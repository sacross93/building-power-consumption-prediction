# -*- coding: utf-8 -*-
"""
Energy Consumption Forecast v10 - Aggressive GPU Memory Usage
------------------------------------------------------------
â€¢ 32GB GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©ì„ ìœ„í•œ ìµœì í™”
â€¢ ë” í° ë°°ì¹˜ ì‚¬ì´ì¦ˆì™€ ë³µì¡í•œ ëª¨ë¸ë¡œ GPU ì„±ëŠ¥ ê·¹ëŒ€í™”
â€¢ ë³‘ë ¬ ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì„¤ì •
â€¢ GPU #3 ì „ìš© ìµœì í™”
"""
import argparse
from pathlib import Path
import warnings
import os
import numpy as np
import pandas as pd
import holidays
import lightgbm as lgb
import xgboost as xgb
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import optuna
import gc
warnings.filterwarnings("ignore")

# GPU ê°•ì œ ì‚¬ìš© ì„¤ì • (GPU #2) + ë©”ëª¨ë¦¬ ìµœì í™”
os.environ["CUDA_VISIBLE_DEVICES"] = "2"  # GPU 2ë²ˆë§Œ ì‚¬ìš©
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["LIGHTGBM_GPU"] = "1"  # LightGBM GPU ê°•ì œ
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # CUDA ë™ê¸°í™”
os.environ["CUDA_CACHE_DISABLE"] = "0"  # CUDA ìºì‹œ í™œì„±í™”
os.environ["CUDA_CACHE_MAXSIZE"] = "2147483648"  # 2GB ìºì‹œ

# Optuna ë¡œê¹… ì–µì œ
optuna.logging.set_verbosity(optuna.logging.WARNING)

KR_HOLIDAYS = holidays.KR()
SEED = 42

###########################################################
# GPU Detection & Setup with Memory Info
###########################################################

def check_gpu_support():
    """GPU ì§€ì› ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì„±ëŠ¥ ì •ë³´"""
    lgb_gpu = False
    xgb_gpu = False
    
    print("ğŸš€ GPU Memory Status:")
    try:
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU 2ë²ˆ
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        total_gb = info.total // 1024**3  # GB
        free_gb = info.free // 1024**3
        used_gb = (info.total - info.free) // 1024**3
        print(f"   Total: {total_gb}GB | Free: {free_gb}GB | Used: {used_gb}GB")
        
        if total_gb >= 24:  # 24GB ì´ìƒì¼ ë•Œ ê³ ì„±ëŠ¥ ëª¨ë“œ
            print(f"ğŸ’ª High-end GPU detected ({total_gb}GB) - enabling MAXIMUM performance mode")
            return True, total_gb
        else:
            print(f"âš¡ Standard GPU ({total_gb}GB) - enabling optimized mode")
            return True, total_gb
    except:
        print("âŒ GPU memory info not available")
        return False, 0
    
    # LightGBM GPU ì‹¤ì œ í•™ìŠµ í…ŒìŠ¤íŠ¸ (ë” í° ë°ì´í„°ì…‹ìœ¼ë¡œ)
    try:
        X_test = np.random.rand(5000, 20)  # ë” í° í…ŒìŠ¤íŠ¸
        y_test = np.random.rand(5000)
        lgb_test = lgb.LGBMRegressor(
            device="gpu", 
            gpu_platform_id=0,
            gpu_device_id=0,
            max_bin=1023,  # GPU ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©í•˜ë„ë¡
            n_estimators=200,
            num_threads=1,
            force_col_wise=True,
            verbose=-1
        )
        lgb_test.fit(X_test, y_test)
        lgb_gpu = True
        print("âœ… LightGBM GPU #2 high-performance test passed")
    except Exception as e:
        print(f"âŒ LightGBM GPU #2 failed: {str(e)[:50]}...")
    
    # XGBoost GPU ì‹¤ì œ í•™ìŠµ í…ŒìŠ¤íŠ¸ (ë” í° ë°ì´í„°ì…‹ìœ¼ë¡œ)
    try:
        X_test = np.random.rand(5000, 20)  # ë” í° í…ŒìŠ¤íŠ¸
        y_test = np.random.rand(5000)
        xgb_test = xgb.XGBRegressor(
            tree_method="gpu_hist", 
            gpu_id=0,
            max_bin=1024,  # GPU ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©í•˜ë„ë¡
            n_estimators=200,
            predictor="gpu_predictor",
            verbosity=0
        )
        xgb_test.fit(X_test, y_test)
        xgb_gpu = True
        print("âœ… XGBoost GPU #2 high-performance test passed")
    except Exception as e:
        print(f"âŒ XGBoost GPU #2 failed: {str(e)[:50]}...")
    
    if not lgb_gpu or not xgb_gpu:
        print("âŒ GPU requirements not met")
        raise RuntimeError("GPU requirement not satisfied")
    
    print("ğŸ¯ All GPU requirements satisfied - MAXIMUM performance mode enabled")
    return lgb_gpu, xgb_gpu

###########################################################
# Metric
###########################################################

def smape_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    eps = 1e-8
    y_pred = np.maximum(y_pred, 0)
    return np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + eps)) * 100

###########################################################
# Feature Engineering Helpers
###########################################################

def add_time_features(df: pd.DataFrame) -> pd.DataFrame:
    dt = df["ì¼ì‹œ"]
    df["month"] = dt.dt.month
    df["day"] = dt.dt.day
    df["hour"] = dt.dt.hour
    df["weekday"] = dt.dt.weekday
    df["is_weekend"] = (df["weekday"] >= 5).astype(int)
    df["is_holiday"] = dt.dt.date.map(lambda d: int(d in KR_HOLIDAYS))
    # Fourier
    df["hour_sin"] = np.sin(2 * np.pi * df["hour"] / 24)
    df["hour_cos"] = np.cos(2 * np.pi * df["hour"] / 24)
    df["month_sin"] = np.sin(2 * np.pi * df["month"] / 12)
    df["month_cos"] = np.cos(2 * np.pi * df["month"] / 12)
    df["weekday_sin"] = np.sin(2 * np.pi * df["weekday"] / 7)
    df["weekday_cos"] = np.cos(2 * np.pi * df["weekday"] / 7)
    return df

def add_weather_features(df: pd.DataFrame) -> pd.DataFrame:
    df["THI"] = 9/5 * df["ê¸°ì˜¨(Â°C)"] - 0.55 * (1 - df["ìŠµë„(%)"] / 100) * (9/5 * df["ê¸°ì˜¨(Â°C)"] - 26) + 32
    # Extended weather features for GPU memory usage
    df["dew_point"] = df["ê¸°ì˜¨(Â°C)"] - (100 - df["ìŠµë„(%)"]) / 5
    df["heat_index"] = 0.5 * (df["ê¸°ì˜¨(Â°C)"] + 61.0 + (df["ê¸°ì˜¨(Â°C)"] - 68.0) * 1.2 + df["ìŠµë„(%)"] * 0.094)
    df["THI_diff_24h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["THI"].diff(24)
    df["temp_diff_1h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].diff(1)
    df["temp_diff_6h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].diff(6)
    df["temp_diff_12h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].diff(12)
    df["temp_rolling_mean_6h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].rolling(window=6).mean().reset_index(0, drop=True)
    df["temp_rolling_std_6h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].rolling(window=6).std().reset_index(0, drop=True)
    df["humidity_diff_1h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["ìŠµë„(%)"].diff(1)
    df["wind_speed_max_6h"] = df.groupby("ê±´ë¬¼ë²ˆí˜¸")["í’ì†(m/s)"].rolling(window=6).max().reset_index(0, drop=True)
    return df

###########################################################
# High-Performance GPU Optuna Objectives
###########################################################

def lgb_objective_gpu_intensive(trial, X_tr, y_tr, X_val, y_val, gpu_memory_gb):
    """GPU ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” LightGBM ìµœì í™”"""
    
    # GPU í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ íŒŒë¼ë¯¸í„° ë²”ìœ„ (í”¼ì²˜ ìˆ˜ ê³ ë ¤)
    # í”¼ì²˜ ìˆ˜ê°€ ë§ì„ ë•Œ bin sizeê°€ ìë™ìœ¼ë¡œ í”¼ì²˜ ìˆ˜ì— ë¹„ë¡€í•´ì„œ ì¦ê°€í•˜ë¯€ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
    max_bin = trial.suggest_int("max_bin", 63, 255)  # ë§¤ìš° ì•ˆì „í•œ ë²”ìœ„
    num_leaves = trial.suggest_int("num_leaves", 64, 512)  # ì•ˆì •ì  ë²”ìœ„
    max_depth = trial.suggest_int("max_depth", 4, 10)  # ë³´ìˆ˜ì  ê¹Šì´
    
    params = {
        "objective": "regression_l1",
        "random_state": SEED,
        "learning_rate": trial.suggest_float("lr", 0.005, 0.2, log=True),  # ë” ë„“ì€ ë²”ìœ„
        "num_leaves": num_leaves,
        "max_depth": max_depth,
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample", 0.5, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_leaf", 1, 50),
        "reg_alpha": trial.suggest_float("ra", 1e-8, 100.0, log=True),
        "reg_lambda": trial.suggest_float("rl", 1e-8, 100.0, log=True),
        "bagging_fraction": trial.suggest_float("bagging", 0.5, 1.0),
        "feature_fraction": trial.suggest_float("feature", 0.5, 1.0),
        "n_estimators": 2000,  # ë” ë§ì€ estimators
        "verbose": -1,
        "device": "gpu",
        "gpu_use_dp": True,
        "gpu_platform_id": 0,
        "gpu_device_id": 0,
        "max_bin": max_bin,
        "num_threads": 1,
        "force_col_wise": True,
        "boost_from_average": True,  # GPU ìµœì í™”
    }
    
    print(f"ğŸ”¥ LightGBM GPU intensive: max_bin={max_bin}, num_leaves={num_leaves}")
    
    model = lgb.LGBMRegressor(**params)
    
    callbacks = [
        lgb.early_stopping(200, verbose=False),  # ë” ê¸´ patience
        lgb.log_evaluation(period=0)
    ]
    
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_val, y_val)],
        eval_metric=lambda y,p: ("SMAPE", smape_np(np.expm1(y), np.expm1(p)), False),
        categorical_feature="auto",
        callbacks=callbacks,
    )
    preds = model.predict(X_val)
    return smape_np(np.expm1(y_val), np.expm1(preds))

def xgb_objective_gpu_intensive(trial, X_tr, y_tr, X_val, y_val, gpu_memory_gb):
    """GPU ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” XGBoost ìµœì í™”"""
    
    # GPU í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ íŒŒë¼ë¯¸í„° ë²”ìœ„
    max_bin = trial.suggest_int("max_bin", 64, 256)  # ì•ˆì „í•œ ë²”ìœ„
    max_depth = trial.suggest_int("max_depth", 3, 8)  # ë³´ìˆ˜ì  ê¹Šì´
    
    params = {
        "objective": "reg:squarederror",
        "random_state": SEED,
        "learning_rate": trial.suggest_float("lr", 0.005, 0.2, log=True),  # ë” ë„“ì€ ë²”ìœ„
        "max_depth": max_depth,
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample", 0.5, 1.0),
        "colsample_bylevel": trial.suggest_float("colsample_level", 0.5, 1.0),
        "colsample_bynode": trial.suggest_float("colsample_node", 0.5, 1.0),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 50),
        "reg_alpha": trial.suggest_float("ra", 1e-8, 100.0, log=True),
        "reg_lambda": trial.suggest_float("rl", 1e-8, 100.0, log=True),
        "gamma": trial.suggest_float("gamma", 1e-8, 10.0, log=True),
        "n_estimators": 2000,  # ë” ë§ì€ estimators
        "verbosity": 0,
        "tree_method": "gpu_hist",
        "gpu_id": 0,
        "max_bin": max_bin,
        "grow_policy": "lossguide",
        "predictor": "gpu_predictor",
        "n_jobs": 1,
        "sampling_method": "gradient_based",  # GPU ìµœì í™”
    }
    
    print(f"ğŸ”¥ XGBoost GPU intensive: max_bin={max_bin}, max_depth={max_depth}")
    
    model = xgb.XGBRegressor(**params)
    model.fit(X_tr, y_tr)
    preds = model.predict(X_val)
    return smape_np(np.expm1(y_val), np.expm1(preds))

###########################################################
# High-Performance Training per building
###########################################################

def train_building_gpu_intensive(df_tr: pd.DataFrame, df_te: pd.DataFrame, feats: list, n_trials: int, gpu_memory_gb: int):
    print(f"    ğŸš€ GPU-intensive training (using {gpu_memory_gb}GB memory)")
    
    # Area for inverse transform
    area_tr = df_tr["ì—°ë©´ì (m2)"].values
    area_te = df_te["ì—°ë©´ì (m2)"].values if not df_te.empty else None
    y_tr_log = df_tr["log_power_pa"].values

    X_full = df_tr[feats]
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X_full)
    X_scaled = pd.DataFrame(X_scaled, columns=feats, index=df_tr.index)

    # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ CV ë¶„í•  ì¡°ì •
    n_splits = 5 if gpu_memory_gb >= 24 else 3  # ë” ë§ì€ ë¶„í• ë¡œ ë” ì •í™•í•œ ê²€ì¦
    test_size = 24*10 if gpu_memory_gb >= 24 else 24*7  # ë” í° í…ŒìŠ¤íŠ¸ ì…‹
    
    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)

    oof_pred_lgb = np.zeros(len(df_tr))
    oof_pred_xgb = np.zeros(len(df_tr))
    best_iters_lgb = []
    best_iters_xgb = []
    
    for fold,(tr_idx,val_idx) in enumerate(tscv.split(X_scaled)):
        print(f"    Fold {fold+1}/{n_splits} ğŸ”¥ GPU intensive training...")
        X_tr = X_scaled.iloc[tr_idx]
        y_tr_f = y_tr_log[tr_idx]
        X_val = X_scaled.iloc[val_idx]
        y_val_f = y_tr_log[val_idx]

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()

        # LightGBM GPU ì§‘ì•½ì  ìµœì í™”
        print(f"      ğŸ”¥ LightGBM GPU-intensive optimization...")
        study_lgb = optuna.create_study(
            direction="minimize",
            sampler=optuna.samplers.TPESampler(n_startup_trials=30)  # ë” ë§ì€ startup trials
        )
        try:
            study_lgb.optimize(
                lambda tr: lgb_objective_gpu_intensive(tr, X_tr, y_tr_f, X_val, y_val_f, gpu_memory_gb), 
                n_trials=n_trials * 2,  # ë” ë§ì€ trials
                show_progress_bar=False,
                n_jobs=1
            )
        except Exception as e:
            print(f"      âš ï¸ LightGBM optimization failed: {str(e)[:100]}...")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            study_lgb = optuna.create_study(direction="minimize")
            default_params = {
                "max_bin": 63, "num_leaves": 64, "max_depth": 4,
                "lr": 0.05, "subsample": 0.8, "colsample": 0.8,
                "min_leaf": 20, "ra": 0.01, "rl": 0.01, 
                "bagging": 0.8, "feature": 0.8
            }
            study_lgb.enqueue_trial(default_params)
            # ì•ˆì „í•œ LightGBM ì§ì ‘ ì‹¤í–‰
            try:
                lgb_objective_gpu_intensive(study_lgb.trials[0], X_tr, y_tr_f, X_val, y_val_f, gpu_memory_gb)
            except:
                pass
        
        try:
            best_lgb_value = study_lgb.best_value if len(study_lgb.trials) > 0 and study_lgb.best_value is not None else 15.0
        except:
            best_lgb_value = 15.0
        print(f"      ğŸ¯ LGB GPU-intensive best: {best_lgb_value:.3f}%")
        
        # Best LGB model with GPU-intensive settings
        try:
            best_lgb_params = study_lgb.best_params.copy() if len(study_lgb.trials) > 0 else {}
        except:
            best_lgb_params = {
                "max_bin": 63, "num_leaves": 64, "max_depth": 4,
                "lr": 0.05, "subsample": 0.8, "colsample": 0.8,
                "min_leaf": 20, "ra": 0.01, "rl": 0.01, 
                "bagging": 0.8, "feature": 0.8
            }
        best_lgb_params.update({
            "objective": "regression_l1",
            "random_state": SEED,
            "n_estimators": 15000,  # ë§¤ìš° ë§ì€ estimators
            "verbose": -1,
            "device": "gpu",
            "gpu_use_dp": True,
            "gpu_platform_id": 0,
            "gpu_device_id": 0,
            "num_threads": 1,
            "force_col_wise": True,
            "boost_from_average": True,
        })
        
        model_lgb = lgb.LGBMRegressor(**best_lgb_params)
        callbacks_lgb = [
            lgb.early_stopping(400, verbose=False),  # ë” ê¸´ patience
            lgb.log_evaluation(period=0)
        ]
        
        model_lgb.fit(
            X_tr, y_tr_f, 
            eval_set=[(X_val, y_val_f)],
            categorical_feature="auto",
            callbacks=callbacks_lgb
        )
        oof_pred_lgb[val_idx] = model_lgb.predict(X_val)
        best_iters_lgb.append(model_lgb.best_iteration_)

        # XGBoost GPU ì§‘ì•½ì  ìµœì í™”
        print(f"      ğŸ”¥ XGBoost GPU-intensive optimization...")
        study_xgb = optuna.create_study(
            direction="minimize",
            sampler=optuna.samplers.TPESampler(n_startup_trials=30)
        )
        try:
            study_xgb.optimize(
                lambda tr: xgb_objective_gpu_intensive(tr, X_tr, y_tr_f, X_val, y_val_f, gpu_memory_gb),
                n_trials=n_trials * 2,  # ë” ë§ì€ trials
                show_progress_bar=False,
                n_jobs=1
            )
        except Exception as e:
            print(f"      âš ï¸ XGBoost optimization failed: {str(e)[:100]}...")
            # ì•ˆì „í•œ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
            study_xgb = optuna.create_study(direction="minimize")
            default_params = {
                "max_bin": 64, "max_depth": 3, "lr": 0.05,
                "subsample": 0.8, "colsample": 0.8,
                "colsample_level": 0.8, "colsample_node": 0.8,
                "min_child_weight": 10, "ra": 0.01, "rl": 0.01, "gamma": 0.01
            }
            study_xgb.enqueue_trial(default_params)
            try:
                xgb_objective_gpu_intensive(study_xgb.trials[0], X_tr, y_tr_f, X_val, y_val_f, gpu_memory_gb)
            except:
                pass
        
        try:
            best_xgb_value = study_xgb.best_value if len(study_xgb.trials) > 0 and study_xgb.best_value is not None else 15.0
        except:
            best_xgb_value = 15.0
        print(f"      ğŸ¯ XGB GPU-intensive best: {best_xgb_value:.3f}%")
        
        # Best XGB model with GPU-intensive settings
        try:
            best_xgb_params = study_xgb.best_params.copy() if len(study_xgb.trials) > 0 else {}
        except:
            best_xgb_params = {
                "max_bin": 64, "max_depth": 3, "lr": 0.05,
                "subsample": 0.8, "colsample": 0.8,
                "colsample_level": 0.8, "colsample_node": 0.8,
                "min_child_weight": 10, "ra": 0.01, "rl": 0.01, "gamma": 0.01
            }
        best_xgb_params.update({
            "objective": "reg:squarederror",
            "random_state": SEED,
            "n_estimators": 15000,  # ë§¤ìš° ë§ì€ estimators
            "verbosity": 0,
            "tree_method": "gpu_hist",
            "gpu_id": 0,
            "grow_policy": "lossguide",
            "predictor": "gpu_predictor",
            "n_jobs": 1,
            "sampling_method": "gradient_based",
        })
        
        model_xgb = xgb.XGBRegressor(**best_xgb_params)
        model_xgb.fit(X_tr, y_tr_f)
        oof_pred_xgb[val_idx] = model_xgb.predict(X_val)
        best_iters_xgb.append(best_xgb_params.get('n_estimators', 2000))

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model_lgb, model_xgb
        gc.collect()

    # ì•™ìƒë¸” (LightGBM 60% + XGBoost 40%)
    oof_pred_ensemble = 0.6 * oof_pred_lgb + 0.4 * oof_pred_xgb
    sm = smape_np(df_tr["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].values, np.expm1(oof_pred_ensemble)*area_tr)
    
    sm_lgb = smape_np(df_tr["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].values, np.expm1(oof_pred_lgb)*area_tr)
    sm_xgb = smape_np(df_tr["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].values, np.expm1(oof_pred_xgb)*area_tr)
    
    print(f"    ğŸ“Š GPU-intensive results - LGB: {sm_lgb:.3f}% | XGB: {sm_xgb:.3f}% | Ensemble: {sm:.3f}%")

    # Final models with maximum GPU utilization
    avg_lgb_iter = min(int(np.mean(best_iters_lgb) * 1.2), 20000) if best_iters_lgb else 5000
    avg_xgb_iter = min(int(np.mean(best_iters_xgb) * 1.2), 20000) if best_iters_xgb else 5000
    
    print(f"    ğŸ”¥ Final GPU-intensive training - LGB: {avg_lgb_iter}, XGB: {avg_xgb_iter}")
    
    # ìµœì¢… ëª¨ë¸ë“¤ì— GPU-intensive ì„¤ì • ì ìš©
    final_lgb_params = best_lgb_params.copy()
    final_lgb_params['n_estimators'] = avg_lgb_iter
    
    final_xgb_params = best_xgb_params.copy()
    final_xgb_params['n_estimators'] = avg_xgb_iter
    
    print(f"    ğŸ’ª Training final LightGBM with GPU-intensive settings...")
    final_lgb = lgb.LGBMRegressor(**final_lgb_params)
    final_lgb.fit(X_scaled, y_tr_log, categorical_feature="auto")
    
    print(f"    ğŸ’ª Training final XGBoost with GPU-intensive settings...")
    final_xgb = xgb.XGBRegressor(**final_xgb_params)
    final_xgb.fit(X_scaled, y_tr_log)

    # Prediction
    preds_df = None
    if not df_te.empty:
        X_te = scaler.transform(df_te[feats])
        pred_lgb = final_lgb.predict(X_te)
        pred_xgb = final_xgb.predict(X_te)
        pred_ensemble = 0.6 * pred_lgb + 0.4 * pred_xgb
        pred_kwh = np.expm1(pred_ensemble) * area_te
        preds_df = pd.DataFrame({
            "num_date_time": df_te["num_date_time"],
            "answer": pred_kwh.clip(min=0)
        })
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del final_lgb, final_xgb
    gc.collect()
    
    return sm, preds_df

###########################################################
# Main Pipeline
###########################################################

def run_pipeline(train_path: Path, test_path: Path, info_path: Path, n_trials: int, use_gpu: bool):
    print("ğŸ“¥ Loading data ... (CPU preprocessing)")
    train_df = pd.read_csv(train_path, parse_dates=["ì¼ì‹œ"])
    test_df = pd.read_csv(test_path, parse_dates=["ì¼ì‹œ"])
    info_df = pd.read_csv(info_path)

    # GPU ì§€ì› í™•ì¸ ë° ë©”ëª¨ë¦¬ ì •ë³´
    gpu_available, gpu_memory_gb = check_gpu_support()
    if not use_gpu or not gpu_available:
        raise RuntimeError("ğŸš« GPU mode required for test5.py! Use --gpu flag")

    print(f"ğŸ’ª GPU Memory Optimization Mode: {gpu_memory_gb}GB")

    # Missing indicators BEFORE replacement
    miss_cols = ["íƒœì–‘ê´‘ìš©ëŸ‰(kW)", "ESSì €ì¥ìš©ëŸ‰(kWh)", "PCSìš©ëŸ‰(kW)"]
    for c in miss_cols:
        info_df[f"{c}_missing"] = (info_df[c] == "-").astype(int)

    num_cols = ["ì—°ë©´ì (m2)", "ëƒ‰ë°©ë©´ì (m2)", "íƒœì–‘ê´‘ìš©ëŸ‰(kW)", "ESSì €ì¥ìš©ëŸ‰(kWh)", "PCSìš©ëŸ‰(kW)"]
    info_df[num_cols] = info_df[num_cols].replace("-", 0).astype(float)

    train = train_df.merge(info_df, on="ê±´ë¬¼ë²ˆí˜¸", how="left")
    test = test_df.merge(info_df, on="ê±´ë¬¼ë²ˆí˜¸", how="left")
    test["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"] = np.nan

    all_df = pd.concat([train, test], ignore_index=True)

    # Extended feature engineering for GPU memory usage
    print("ğŸ”§ Extended feature engineering ... (CPU preprocessing)")
    all_df = add_time_features(all_df)
    all_df = add_weather_features(all_df)

    # Target scaling
    all_df["power_per_area"] = all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"] / (all_df["ì—°ë©´ì (m2)"].replace(0, np.nan))
    all_df["log_power_pa"] = np.log1p(all_df["power_per_area"].clip(lower=0))

    # Extended feature list for GPU memory usage
    feats = [
        "ê±´ë¬¼ë²ˆí˜¸","ê¸°ì˜¨(Â°C)","í’ì†(m/s)","ìŠµë„(%)","ì—°ë©´ì (m2)","ëƒ‰ë°©ë©´ì (m2)",
        "íƒœì–‘ê´‘ìš©ëŸ‰(kW)","ESSì €ì¥ìš©ëŸ‰(kWh)","PCSìš©ëŸ‰(kW)",
        "dew_point","heat_index","THI","THI_diff_24h",
        "temp_diff_1h","temp_diff_6h","temp_diff_12h",
        "temp_rolling_mean_6h","temp_rolling_std_6h",
        "humidity_diff_1h","wind_speed_max_6h",
        "month","day","hour","weekday","is_weekend","is_holiday",
        "hour_sin","hour_cos","month_sin","month_cos","weekday_sin","weekday_cos"
    ] + [f"{c}_missing" for c in miss_cols]

    df_train = all_df[~all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].isna()].copy()
    df_test = all_df[all_df["ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"].isna()].copy()

    # num_date_time ì»¬ëŸ¼ ìƒì„±
    print("ğŸ”§ Creating num_date_time column...")
    df_test['num_date_time'] = df_test['ê±´ë¬¼ë²ˆí˜¸'].astype(str) + '_' + df_test['ì¼ì‹œ'].dt.strftime('%Y%m%d %H')

    # GPU ì§‘ì•½ì  ê±´ë¬¼ë³„ í•™ìŠµ
    print(f"ğŸš€ Starting GPU-intensive building-wise training (using {gpu_memory_gb}GB)...")
    sub_parts = []
    scores = []
    
    for bid in df_train["ê±´ë¬¼ë²ˆí˜¸"].unique():
        print(f"\nğŸ¢ Building {bid} - GPU Memory Intensive Mode")
        tr_b = df_train[df_train["ê±´ë¬¼ë²ˆí˜¸"] == bid].copy()
        te_b = df_test[df_test["ê±´ë¬¼ë²ˆí˜¸"] == bid].copy()
        if len(tr_b) < 150:  # ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ ê±´ë¬¼ í•™ìŠµ
            print(f"  âš ï¸ Skipping Building {bid} - insufficient data ({len(tr_b)} < 150)")
            continue
            
        sm, preds = train_building_gpu_intensive(tr_b, te_b, feats, n_trials, gpu_memory_gb)
        scores.append(sm)
        if preds is not None:
            sub_parts.append(preds)
            print(f"  âœ… Building {bid} GPU-intensive predictions added ({len(preds)} rows)")

    print(f"\nğŸ“ˆ GPU-Intensive Average OOF SMAPE: {np.mean(scores):.3f}%")
    
    if sub_parts:
        print(f"ğŸ”— Concatenating {len(sub_parts)} GPU-intensive predictions...")
        submission = pd.concat(sub_parts, ignore_index=True)
        print(f"   Combined predictions shape: {submission.shape}")
        
        # align with sample_submission if exists
        sample_path = test_path.parent / "sample_submission.csv"
        if sample_path.exists():
            print("ğŸ“‹ Aligning with sample_submission.csv...")
            sample = pd.read_csv(sample_path).drop(columns=["answer"], errors="ignore")
            submission = sample.merge(submission, on="num_date_time", how="left")
            print(f"   Final submission shape: {submission.shape}")
        
        submission.to_csv("submission_gpu_intensive.csv", index=False)
        print("âœ… submission_gpu_intensive.csv saved.")
    else:
        print("âŒ No predictions generated!")
        sample_path = test_path.parent / "sample_submission.csv"
        if sample_path.exists():
            sample = pd.read_csv(sample_path)
            sample.to_csv("submission_gpu_intensive.csv", index=False)
            print("ğŸ“ Default submission_gpu_intensive.csv created.")

###########################################################
if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--train", type=Path, default=Path("data/train.csv"))
    ap.add_argument("--test", type=Path, default=Path("data/test.csv"))
    ap.add_argument("--info", type=Path, default=Path("data/building_info.csv"))
    ap.add_argument("--n-trials", type=int, default=20)
    ap.add_argument("--gpu", action="store_true", help="GPU ê°€ì† ì‚¬ìš© (í•„ìˆ˜)")
    args = ap.parse_args()
    
    if not args.gpu:
        print("âŒ test5.py requires --gpu flag for maximum GPU utilization!")
        exit(1)
        
    run_pipeline(args.train, args.test, args.info, args.n_trials, args.gpu) 