"""
Ultimate Tuning Solution
========================

모든 개선사항을 통합한 최종 튜닝 솔루션:
1. 시각화 분석 기반 전처리 개선
2. Optuna 하이퍼파라미터 최적화
3. CatBoost 추가 3-model 앙상블
4. 고급 피처 엔지니어링
5. Stacking 앙상블

목표: 11.8 SMAPE → 7-8 SMAPE
"""

import pandas as pd
import numpy as np
from pathlib import Path
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import VotingRegressor, StackingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import LabelEncoder
import optuna
import warnings
warnings.filterwarnings('ignore')

from solution import load_data, smape
from ts_validation import TimeSeriesCV
from improved_preprocessing import ImprovedPreprocessor
from advanced_feature_engineering import AdvancedFeatureEngineer


class UltimateTuningSolution:
    """최종 튜닝 솔루션 클래스."""
    
    def __init__(self, quick_mode=False, max_trials=None):
        self.quick_mode = quick_mode  # 빠른 테스트용
        self.max_trials = max_trials or (20 if quick_mode else 100)  # trial 수 대폭 증가
        self.best_params = {}
        self.best_models = {}
        self.validation_results = {}
        
    def optimize_hyperparameters_fast(self, X, y, datetime_series):
        """빠른 하이퍼파라미터 최적화."""
        print("🔍 Fast hyperparameter optimization...")
        
        ts_cv = TimeSeriesCV(n_splits=2, test_size_days=7, gap_days=1)
        temp_df = pd.DataFrame({'datetime': datetime_series})
        splits = list(ts_cv.split(temp_df, 'datetime'))
        
        def cross_validate_model(model, X, y, splits):
            scores = []
            for train_idx, val_idx in splits:
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_val_fold = y.iloc[val_idx]
                
                try:
                    model.fit(X_train_fold, y_train_fold)
                    y_pred = model.predict(X_val_fold)
                    
                    # 로그 변환 역변환
                    y_val_original = np.expm1(y_val_fold)
                    y_pred_original = np.expm1(y_pred)
                    
                    fold_smape = smape(y_val_original.values, y_pred_original)
                    scores.append(fold_smape)
                except:
                    scores.append(float('inf'))
            
            return np.mean(scores)
        
        # XGBoost 최적화
        def xgb_objective(trial):
            params = {
                'max_depth': trial.suggest_int('max_depth', 8, 15),  # 더 깊은 트리
                'n_estimators': trial.suggest_int('n_estimators', 800, 2000, step=200),  # 더 많은 트리
                'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.1),  # 더 정교한 학습
                'subsample': trial.suggest_float('subsample', 0.7, 0.95),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 0.9),  # 추가 파라미터
                'colsample_bynode': trial.suggest_float('colsample_bynode', 0.6, 0.9),   # 추가 파라미터
                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # 추가 파라미터
                'tree_method': 'gpu_hist',  # GPU 가속
                'gpu_id': 0,
                'max_bin': 512,  # GPU 메모리 활용도 증가
                'random_state': 42,
                'verbosity': 0
            }
            model = xgb.XGBRegressor(**params)
            return cross_validate_model(model, X, y, splits)
        
        # LightGBM 최적화
        def lgb_objective(trial):
            params = {
                'max_depth': trial.suggest_int('max_depth', 10, 20),  # 더 깊은 트리
                'n_estimators': trial.suggest_int('n_estimators', 1000, 3000, step=500),  # 더 많은 트리
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),  # 더 정교한 학습
                'subsample': trial.suggest_float('subsample', 0.7, 0.95),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),
                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5.0),
                'num_leaves': trial.suggest_int('num_leaves', 200, 1000),  # 더 많은 잎
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),  # 추가 파라미터
                'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 0.1),  # 추가 파라미터
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # 추가 파라미터
                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.9),  # 추가 파라미터
                'device': 'gpu',  # GPU 가속
                'gpu_use_dp': True,
                'max_bin': 1023,  # GPU 메모리 활용도 증가
                'random_state': 42,
                'verbosity': -1
            }
            model = lgb.LGBMRegressor(**params)
            return cross_validate_model(model, X, y, splits)
        
        # CatBoost 최적화
        def cb_objective(trial):
            params = {
                'depth': trial.suggest_int('depth', 8, 16),  # 더 깊은 트리
                'iterations': trial.suggest_int('iterations', 1000, 3000, step=500),  # 더 많은 반복
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),  # 더 정교한 학습
                'bootstrap_type': 'Bernoulli',  # subsample과 호환되는 bootstrap 타입
                'subsample': trial.suggest_float('subsample', 0.7, 0.95),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),
                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
                'max_leaves': trial.suggest_int('max_leaves', 256, 2048),  # 더 많은 잎
                'border_count': trial.suggest_int('border_count', 128, 512),  # 추가 파라미터
                'feature_border_type': 'GreedyLogSum',  # GPU 최적화
                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),  # 추가 파라미터
                'task_type': 'GPU',  # GPU 가속
                'gpu_ram_part': 0.8,  # GPU 메모리 활용도 증가
                'random_seed': 42,
                'verbose': False
            }
            model = cb.CatBoostRegressor(**params)
            return cross_validate_model(model, X, y, splits)
        
        # 각 모델 최적화 실행
        n_trials = self.max_trials
        
        print("  Optimizing XGBoost...")
        xgb_study = optuna.create_study(direction='minimize')
        xgb_study.optimize(xgb_objective, n_trials=n_trials, show_progress_bar=False)
        self.best_params['xgboost'] = xgb_study.best_params
        
        print("  Optimizing LightGBM...")
        lgb_study = optuna.create_study(direction='minimize')
        lgb_study.optimize(lgb_objective, n_trials=n_trials, show_progress_bar=False)
        self.best_params['lightgbm'] = lgb_study.best_params
        
        print("  Optimizing CatBoost...")
        cb_study = optuna.create_study(direction='minimize')
        cb_study.optimize(cb_objective, n_trials=n_trials, show_progress_bar=False)
        self.best_params['catboost'] = cb_study.best_params
        
        print(f"✅ Hyperparameter optimization completed")
        print(f"   XGBoost best: {xgb_study.best_value:.4f}")
        print(f"   LightGBM best: {lgb_study.best_value:.4f}")
        print(f"   CatBoost best: {cb_study.best_value:.4f}")
        
        return self.best_params
    
    def create_optimized_models(self):
        """최적화된 모델들 생성."""
        print("🔧 Creating optimized models...")
        
        # 개별 최적화된 모델들 (GPU 설정 포함)
        xgb_params = self.best_params['xgboost'].copy()
        xgb_params.update({'tree_method': 'gpu_hist', 'gpu_id': 0})
        self.best_models['xgboost'] = xgb.XGBRegressor(**xgb_params)
        
        lgb_params = self.best_params['lightgbm'].copy()
        lgb_params.update({'device': 'gpu', 'gpu_use_dp': True})
        self.best_models['lightgbm'] = lgb.LGBMRegressor(**lgb_params)
        
        cb_params = self.best_params['catboost'].copy()
        cb_params.update({'task_type': 'GPU', 'bootstrap_type': 'Bernoulli'})
        self.best_models['catboost'] = cb.CatBoostRegressor(**cb_params)
        
        # 앙상블 모델들
        self.best_models['voting_ensemble'] = VotingRegressor([
            ('xgb', self.best_models['xgboost']),
            ('lgb', self.best_models['lightgbm']),
            ('cb', self.best_models['catboost'])
        ], weights=[0.4, 0.35, 0.25])
        
        self.best_models['stacking_ensemble'] = StackingRegressor(
            estimators=[
                ('xgb', self.best_models['xgboost']),
                ('lgb', self.best_models['lightgbm']),
                ('cb', self.best_models['catboost'])
            ],
            final_estimator=Ridge(alpha=1.0),
            cv=3
        )
        
        print(f"✅ Created {len(self.best_models)} optimized models")
    
    def validate_all_models(self, X, y, datetime_series):
        """모든 모델 검증."""
        print("📊 Validating all models...")
        
        ts_cv = TimeSeriesCV(n_splits=3, test_size_days=7, gap_days=1)
        temp_df = pd.DataFrame({'datetime': datetime_series})
        splits = ts_cv.split(temp_df, 'datetime')
        
        for model_name, model in self.best_models.items():
            print(f"  Validating {model_name}...")
            
            fold_scores = []
            for fold, (train_idx, val_idx) in enumerate(splits):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_val_fold = y.iloc[val_idx]
                
                try:
                    model.fit(X_train_fold, y_train_fold)
                    y_pred = model.predict(X_val_fold)
                    
                    # 로그 변환 역변환
                    y_val_original = np.expm1(y_val_fold)
                    y_pred_original = np.expm1(y_pred)
                    
                    fold_smape = smape(y_val_original.values, y_pred_original)
                    fold_scores.append(fold_smape)
                    
                except Exception as e:
                    print(f"    ❌ Error in {model_name} fold {fold}: {e}")
                    fold_scores.append(float('inf'))
            
            mean_score = np.mean(fold_scores) if fold_scores else float('inf')
            std_score = np.std(fold_scores) if fold_scores else 0
            
            self.validation_results[model_name] = {
                'mean': mean_score,
                'std': std_score,
                'folds': fold_scores
            }
            
            print(f"    {model_name}: {mean_score:.4f} (±{std_score:.4f})")
    
    def generate_ultimate_submission(self, X_train, X_test, y_train, test_df):
        """최고 성능 모델로 최종 제출 파일 생성."""
        print("🎯 Generating ultimate submission...")
        
        # 최고 성능 모델 선택
        valid_results = {k: v for k, v in self.validation_results.items() 
                        if v['mean'] != float('inf')}
        
        if not valid_results:
            print("❌ No valid models found!")
            return None, None
        
        best_model_name = min(valid_results.keys(), 
                            key=lambda x: valid_results[x]['mean'])
        best_model = self.best_models[best_model_name]
        best_score = valid_results[best_model_name]['mean']
        
        print(f"🏆 Selected model: {best_model_name}")
        print(f"🎯 Expected SMAPE: {best_score:.4f}")
        
        # 전체 데이터로 재학습
        print("  Training on full dataset...")
        best_model.fit(X_train, y_train)
        
        # 테스트 예측
        print("  Making predictions...")
        test_predictions_log = best_model.predict(X_test)
        test_predictions = np.expm1(test_predictions_log)
        test_predictions = np.maximum(test_predictions, 0)
        
        # 제출 파일 생성
        submission = pd.DataFrame({
            'num_date_time': test_df['num_date_time'],
            'answer': test_predictions
        })
        
        submission_file = f'submission_ultimate_{best_model_name}.csv'
        submission.to_csv(submission_file, index=False)
        
        print(f"💾 Ultimate submission saved: {submission_file}")
        print(f"📊 Predictions range: {test_predictions.min():.2f} - {test_predictions.max():.2f}")
        print(f"📊 Predictions mean: {test_predictions.mean():.2f}")
        
        return submission, best_model_name
    
    def create_ultimate_report(self, output_dir='./visualizations/'):
        """최종 튜닝 리포트."""
        report_path = Path(output_dir) / 'ultimate_tuning_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Ultimate Tuning Solution Report\n\n")
            
            f.write("## Applied Optimizations\n\n")
            f.write("1. **Visualization-based preprocessing improvements**\n")
            f.write("   - Target log transformation\n")
            f.write("   - VIF-based multicollinearity removal\n")
            f.write("   - RobustScaler feature scaling\n")
            f.write("   - IQR-based outlier treatment\n\n")
            
            f.write("2. **Advanced feature engineering**\n")
            f.write("   - Building clustering (5 clusters)\n")
            f.write("   - Advanced time features (15min, 30min patterns)\n")
            f.write("   - Enhanced weather features (apparent temp, heat index)\n")
            f.write("   - Multi-way interaction features\n\n")
            
            f.write("3. **Hyperparameter optimization**\n")
            f.write("   - Optuna-based systematic search\n")
            f.write("   - Individual optimization for XGBoost, LightGBM, CatBoost\n")
            f.write("   - Time series cross-validation\n\n")
            
            f.write("4. **Advanced ensemble methods**\n")
            f.write("   - Weighted voting ensemble\n")
            f.write("   - Ridge-based stacking ensemble\n")
            f.write("   - Optimized model weights\n\n")
            
            f.write("## Model Performance Results\n\n")
            f.write("| Model | Mean SMAPE | Std SMAPE | Status |\n")
            f.write("|-------|------------|-----------|--------|\n")
            
            baseline = 11.8
            for model_name, result in self.validation_results.items():
                if result['mean'] != float('inf'):
                    improvement = (baseline - result['mean']) / baseline * 100
                    status = "✅ Improved" if result['mean'] < baseline else "❌ Degraded"
                    f.write(f"| {model_name} | {result['mean']:.4f} | {result['std']:.4f} | {status} |\n")
            
            # 최고 성능 모델
            valid_results = {k: v for k, v in self.validation_results.items() 
                           if v['mean'] != float('inf')}
            
            if valid_results:
                best_model = min(valid_results.items(), key=lambda x: x[1]['mean'])
                best_improvement = (baseline - best_model[1]['mean']) / baseline * 100
                
                f.write(f"\n## Best Result\n\n")
                f.write(f"- **Model**: {best_model[0]}\n")
                f.write(f"- **SMAPE**: {best_model[1]['mean']:.4f}\n")
                f.write(f"- **Improvement**: {best_improvement:.2f}% vs baseline (11.8)\n")
                
                if best_model[1]['mean'] < 8.0:
                    f.write("- **Status**: 🎉 Excellent! Sub-8 SMAPE achieved!\n")
                elif best_model[1]['mean'] < 9.0:
                    f.write("- **Status**: ✅ Great! Sub-9 SMAPE achieved!\n")
                elif best_model[1]['mean'] < 10.0:
                    f.write("- **Status**: ✅ Good! Sub-10 SMAPE achieved!\n")
                else:
                    f.write("- **Status**: ⚠️ Moderate improvement\n")
            
            f.write(f"\n## Performance Evolution\n\n")
            f.write(f"1. **Original baseline**: ~53 SMAPE (data leakage)\n")
            f.write(f"2. **Safe baseline**: ~20 SMAPE (no leakage)\n")
            f.write(f"3. **Improved preprocessing**: 11.8 SMAPE (16.62% improvement)\n")
            f.write(f"4. **Ultimate tuning**: {best_model[1]['mean']:.1f} SMAPE ({best_improvement:.1f}% total improvement)\n")
            
            f.write(f"\n## Production Deployment\n\n")
            f.write(f"- **Recommended model**: {best_model[0]}\n")
            f.write(f"- **Expected performance**: {best_model[1]['mean']:.1f} ± {best_model[1]['std']:.1f} SMAPE\n")
            f.write(f"- **Confidence level**: High (validated with time series CV)\n")
            f.write(f"- **Submission file**: `submission_ultimate_{best_model[0]}.csv`\n")
        
        print(f"📄 Ultimate tuning report saved: {report_path}")
    
    def run_ultimate_tuning(self):
        """최종 튜닝 솔루션 실행."""
        print("=" * 80)
        print("ULTIMATE TUNING SOLUTION")
        print("=" * 80)
        print("Combining all optimizations for maximum performance")
        print(f"Quick mode: {'ON' if self.quick_mode else 'OFF'}")
        
        # 1. 데이터 로드
        print("\n1. Loading and preprocessing data...")
        data_dir = Path('../data')
        train_path = data_dir / 'train.csv'
        test_path = data_dir / 'test.csv'
        building_path = data_dir / 'building_info.csv'
        
        train_df, test_df = load_data(train_path, test_path, building_path)
        
        # 2. 고급 피처 엔지니어링
        print("\n2. Advanced feature engineering...")
        engineer = AdvancedFeatureEngineer()
        train_advanced, test_advanced, _ = engineer.apply_advanced_feature_engineering(
            train_df, test_df
        )
        
        # 3. 개선된 전처리 적용 (RobustScaler - 성능 검증됨)
        print("\n3. Applying improved preprocessing...")
        preprocessor = ImprovedPreprocessor(scaler_type='robust')  # RobustScaler로 복원
        X_train, X_test, y_train = preprocessor.fit_transform(train_advanced, test_advanced)
        
        # datetime 시리즈 (교차검증용)
        from solution import engineer_features
        train_processed, _ = engineer_features(train_df.copy(), test_df.copy())
        datetime_series = train_processed['datetime']
        
        print(f"📊 Final dataset: {len(X_train)} samples, {len(X_train.columns)} features")
        
        # 4. 하이퍼파라미터 최적화
        print("\n4. Hyperparameter optimization...")
        self.optimize_hyperparameters_fast(X_train, y_train, datetime_series)
        
        # 5. 최적화된 모델들 생성
        print("\n5. Creating optimized models...")
        self.create_optimized_models()
        
        # 6. 모든 모델 검증
        print("\n6. Validating all models...")
        self.validate_all_models(X_train, y_train, datetime_series)
        
        # 7. 최종 제출 파일 생성
        print("\n7. Generating ultimate submission...")
        submission, best_model_name = self.generate_ultimate_submission(
            X_train, X_test, y_train, test_df
        )
        
        # 8. 리포트 생성
        print("\n8. Creating final report...")
        self.create_ultimate_report()
        
        # 9. 최종 결과
        print(f"\n" + "=" * 80)
        print("ULTIMATE TUNING RESULTS")
        print("=" * 80)
        
        baseline = 11.8
        valid_results = {k: v for k, v in self.validation_results.items() 
                        if v['mean'] != float('inf')}
        
        if valid_results:
            best_score = min(v['mean'] for v in valid_results.values())
            total_improvement = (baseline - best_score) / baseline * 100
            
            print(f"🎯 Best SMAPE: {best_score:.4f}")
            print(f"🚀 Total improvement: {total_improvement:.2f}%")
            print(f"🏆 Best model: {best_model_name}")
            print(f"📈 Performance evolution: 53 → 11.8 → {best_score:.1f}")
            
            if best_score < 8.0:
                print("🎉 OUTSTANDING! Sub-8 SMAPE achieved!")
            elif best_score < 9.0:
                print("✅ EXCELLENT! Sub-9 SMAPE achieved!")
            elif best_score < 10.0:
                print("✅ GREAT! Sub-10 SMAPE achieved!")
            
            print(f"💾 Submission: submission_ultimate_{best_model_name}.csv")
        
        return self.validation_results, best_model_name


if __name__ == "__main__":
    # 고성능 최적화 모드 (GPU 서버용 - VRAM 최대 활용)
    solution = UltimateTuningSolution(quick_mode=False, max_trials=100)  # VRAM 최대 활용
    results, best_model = solution.run_ultimate_tuning()
    
    print(f"\n🎯 Ultimate tuning solution completed!")
    print(f"🚀 Ready for GPU server deployment!")