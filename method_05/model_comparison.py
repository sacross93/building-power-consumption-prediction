"""
Model Performance Comparison: Original vs Improved Preprocessing
===============================================================

Í∏∞Ï°¥ Ï†ÑÏ≤òÎ¶¨ÏôÄ Í∞úÏÑ†Îêú Ï†ÑÏ≤òÎ¶¨Ïùò Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê
"""

import pandas as pd
import numpy as np
from pathlib import Path
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import VotingRegressor
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from solution import load_data, engineer_features, smape
from ts_validation import TimeSeriesCV
from improved_preprocessing import ImprovedPreprocessor


class ModelComparison:
    """Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê ÌÅ¥ÎûòÏä§."""
    
    def __init__(self):
        self.results = {}
        
    def prepare_original_data(self, train_df, test_df):
        """Í∏∞Ï°¥ Ï†ÑÏ≤òÎ¶¨ Î∞©ÏãùÏúºÎ°ú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ."""
        print("Preparing data with original preprocessing...")
        
        # Í∏∞Ï°¥ engineer_features ÏÇ¨Ïö©
        train_fe, test_fe = engineer_features(train_df.copy(), test_df.copy())
        
        # ÌîºÏ≤ò Ï§ÄÎπÑ
        drop_cols = ['Ï†ÑÎ†•ÏÜåÎπÑÎüâ(kWh)', 'ÏùºÏãú', 'num_date_time', 'datetime']
        feature_cols = [c for c in train_fe.columns if c not in drop_cols and c in test_fe.columns]
        
        X_train = train_fe[feature_cols].copy()
        X_test = test_fe[feature_cols].copy()
        y_train = train_fe['Ï†ÑÎ†•ÏÜåÎπÑÎüâ(kWh)']
        
        # Ïπ¥ÌÖåÍ≥†Î¶¨ Ïù∏ÏΩîÎî©
        categorical_cols = ['Í±¥Î¨ºÎ≤àÌò∏', 'building_type']
        encoders = {}
        
        for col in categorical_cols:
            if col in X_train.columns:
                le = LabelEncoder()
                X_train[col] = le.fit_transform(X_train[col].astype(str))
                X_test[col] = X_test[col].astype(str).map(
                    lambda x: le.transform([x])[0] if x in le.classes_ else -1
                )
                encoders[col] = le
        
        # Í∞ùÏ≤¥ ÌÉÄÏûÖ Ï≤òÎ¶¨
        for col in X_train.columns:
            if X_train[col].dtype == 'object':
                le = LabelEncoder()
                X_train[col] = le.fit_transform(X_train[col].astype(str))
                X_test[col] = X_test[col].astype(str).map(
                    lambda x: le.transform([x])[0] if x in le.classes_ else -1
                ).fillna(-1)
                encoders[col] = le
            elif not pd.api.types.is_numeric_dtype(X_train[col]):
                X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)
                X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)
        
        return X_train, X_test, y_train, train_fe
    
    def create_base_model(self):
        """Í∏∞Î≥∏ XGBoost Î™®Îç∏ ÏÉùÏÑ±."""
        return xgb.XGBRegressor(
            max_depth=8,
            n_estimators=400,
            learning_rate=0.08,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_alpha=1.0,
            reg_lambda=2.0,
            random_state=42,
            verbosity=0
        )
    
    def create_ensemble_model(self):
        """ÏïôÏÉÅÎ∏î Î™®Îç∏ ÏÉùÏÑ±."""
        xgb_model = xgb.XGBRegressor(
            max_depth=8,
            n_estimators=400,
            learning_rate=0.08,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_alpha=1.0,
            reg_lambda=2.0,
            random_state=42,
            verbosity=0
        )
        
        lgb_model = lgb.LGBMRegressor(
            max_depth=8,
            n_estimators=400,
            learning_rate=0.08,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_alpha=1.0,
            reg_lambda=2.0,
            random_state=42,
            verbosity=-1
        )
        
        return VotingRegressor([
            ('xgb', xgb_model),
            ('lgb', lgb_model)
        ])
    
    def evaluate_model(self, X, y, datetime_series, model, model_name, is_log_target=False):
        """ÏãúÍ≥ÑÏó¥ ÍµêÏ∞®Í≤ÄÏ¶ùÏúºÎ°ú Î™®Îç∏ ÌèâÍ∞Ä."""
        print(f"Evaluating {model_name}...")
        
        ts_cv = TimeSeriesCV(n_splits=3, test_size_days=7, gap_days=1)
        temp_df = pd.DataFrame({'datetime': datetime_series})
        splits = ts_cv.split(temp_df, 'datetime')
        
        fold_scores = []
        for fold, (train_idx, val_idx) in enumerate(splits):
            X_train_fold = X.iloc[train_idx]
            y_train_fold = y.iloc[train_idx]
            X_val_fold = X.iloc[val_idx]
            y_val_fold = y.iloc[val_idx]
            
            # Î™®Îç∏ ÌõàÎ†®
            model.fit(X_train_fold, y_train_fold)
            y_pred = model.predict(X_val_fold)
            
            # Î°úÍ∑∏ Î≥ÄÌôòÎêú ÌÉÄÍ≤üÏùò Í≤ΩÏö∞ Ïó≠Î≥ÄÌôò
            if is_log_target:
                y_val_original = np.expm1(y_val_fold)
                y_pred_original = np.expm1(y_pred)
                fold_smape = smape(y_val_original.values, y_pred_original)
            else:
                fold_smape = smape(y_val_fold.values, y_pred)
            
            fold_scores.append(fold_smape)
            print(f"  Fold {fold + 1}: SMAPE = {fold_smape:.4f}")
        
        mean_score = np.mean(fold_scores)
        std_score = np.std(fold_scores)
        
        print(f"  {model_name} CV Score: {mean_score:.4f} (¬±{std_score:.4f})")
        
        return mean_score, std_score, fold_scores
    
    def run_comparison(self):
        """Ï†ÑÏ≤¥ ÎπÑÍµê Ïã§Ìñâ."""
        print("=" * 80)
        print("MODEL PERFORMANCE COMPARISON")
        print("=" * 80)
        
        # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
        data_dir = Path('../data')
        train_path = data_dir / 'train.csv'
        test_path = data_dir / 'test.csv'
        building_path = data_dir / 'building_info.csv'
        
        train_df, test_df = load_data(train_path, test_path, building_path)
        
        # 1. Í∏∞Ï°¥ Ï†ÑÏ≤òÎ¶¨ ÌèâÍ∞Ä
        print("\n1. ORIGINAL PREPROCESSING EVALUATION")
        print("-" * 50)
        
        X_orig, X_test_orig, y_orig, train_orig = self.prepare_original_data(train_df, test_df)
        
        # Í∏∞Ï°¥ Î™®Îç∏Îì§ ÌèâÍ∞Ä
        base_model_orig = self.create_base_model()
        ensemble_model_orig = self.create_ensemble_model()
        
        orig_base_mean, orig_base_std, orig_base_folds = self.evaluate_model(
            X_orig, y_orig, train_orig['datetime'], base_model_orig, "Original XGBoost"
        )
        
        orig_ensemble_mean, orig_ensemble_std, orig_ensemble_folds = self.evaluate_model(
            X_orig, y_orig, train_orig['datetime'], ensemble_model_orig, "Original Ensemble"
        )
        
        # 2. Í∞úÏÑ†Îêú Ï†ÑÏ≤òÎ¶¨ ÌèâÍ∞Ä
        print("\n2. IMPROVED PREPROCESSING EVALUATION")
        print("-" * 50)
        
        preprocessor = ImprovedPreprocessor()
        X_improved, X_test_improved, y_improved = preprocessor.fit_transform(train_df, test_df)
        
        # Í∞úÏÑ†Îêú Î™®Îç∏Îì§ ÌèâÍ∞Ä
        base_model_improved = self.create_base_model()
        ensemble_model_improved = self.create_ensemble_model()
        
        # datetime Ïû¨Íµ¨ÏÑ± (Í∞úÏÑ†Îêú Ï†ÑÏ≤òÎ¶¨ÏóêÏÑúÎäî datetimeÏù¥ dropÎê®)
        train_processed, _ = engineer_features(train_df.copy(), test_df.copy())
        
        improved_base_mean, improved_base_std, improved_base_folds = self.evaluate_model(
            X_improved, y_improved, train_processed['datetime'], base_model_improved, 
            "Improved XGBoost", is_log_target=True
        )
        
        improved_ensemble_mean, improved_ensemble_std, improved_ensemble_folds = self.evaluate_model(
            X_improved, y_improved, train_processed['datetime'], ensemble_model_improved, 
            "Improved Ensemble", is_log_target=True
        )
        
        # 3. Í≤∞Í≥º Ï†ÄÏû•
        self.results = {
            'original': {
                'xgboost': {'mean': orig_base_mean, 'std': orig_base_std, 'folds': orig_base_folds},
                'ensemble': {'mean': orig_ensemble_mean, 'std': orig_ensemble_std, 'folds': orig_ensemble_folds}
            },
            'improved': {
                'xgboost': {'mean': improved_base_mean, 'std': improved_base_std, 'folds': improved_base_folds},
                'ensemble': {'mean': improved_ensemble_mean, 'std': improved_ensemble_std, 'folds': improved_ensemble_folds}
            }
        }
        
        # 4. Í≤∞Í≥º ÏãúÍ∞ÅÌôî
        self.create_comparison_plots()
        
        # 5. Í≤∞Í≥º Î¶¨Ìè¨Ìä∏
        self.create_comparison_report()
        
        return self.results
    
    def create_comparison_plots(self):
        """ÎπÑÍµê Í≤∞Í≥º ÏãúÍ∞ÅÌôî."""
        print("\nCreating comparison plots...")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. ÌèâÍ∑† SMAPE ÎπÑÍµê
        models = ['XGBoost', 'Ensemble']
        original_means = [self.results['original']['xgboost']['mean'], 
                         self.results['original']['ensemble']['mean']]
        improved_means = [self.results['improved']['xgboost']['mean'], 
                         self.results['improved']['ensemble']['mean']]
        
        x = np.arange(len(models))
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, original_means, width, label='Original', alpha=0.7, color='lightcoral')
        bars2 = ax1.bar(x + width/2, improved_means, width, label='Improved', alpha=0.7, color='lightblue')
        
        ax1.set_xlabel('Model Type')
        ax1.set_ylabel('SMAPE (%)')
        ax1.set_title('Mean SMAPE Comparison')
        ax1.set_xticks(x)
        ax1.set_xticklabels(models)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Í∞í ÌëúÏãú
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.3f}', ha='center', va='bottom')
        for bar in bars2:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{height:.3f}', ha='center', va='bottom')
        
        # 2. ÌëúÏ§ÄÌé∏Ï∞® ÎπÑÍµê
        original_stds = [self.results['original']['xgboost']['std'], 
                        self.results['original']['ensemble']['std']]
        improved_stds = [self.results['improved']['xgboost']['std'], 
                        self.results['improved']['ensemble']['std']]
        
        bars3 = ax2.bar(x - width/2, original_stds, width, label='Original', alpha=0.7, color='lightcoral')
        bars4 = ax2.bar(x + width/2, improved_stds, width, label='Improved', alpha=0.7, color='lightblue')
        
        ax2.set_xlabel('Model Type')
        ax2.set_ylabel('SMAPE Standard Deviation')
        ax2.set_title('SMAPE Stability Comparison')
        ax2.set_xticks(x)
        ax2.set_xticklabels(models)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. FoldÎ≥Ñ XGBoost ÏÑ±Îä•
        folds = [1, 2, 3]
        ax3.plot(folds, self.results['original']['xgboost']['folds'], 
                'o-', label='Original XGBoost', linewidth=2, markersize=8, color='red')
        ax3.plot(folds, self.results['improved']['xgboost']['folds'], 
                'o-', label='Improved XGBoost', linewidth=2, markersize=8, color='blue')
        
        ax3.set_xlabel('CV Fold')
        ax3.set_ylabel('SMAPE (%)')
        ax3.set_title('XGBoost Performance by Fold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_xticks(folds)
        
        # 4. FoldÎ≥Ñ Ensemble ÏÑ±Îä•
        ax4.plot(folds, self.results['original']['ensemble']['folds'], 
                'o-', label='Original Ensemble', linewidth=2, markersize=8, color='red')
        ax4.plot(folds, self.results['improved']['ensemble']['folds'], 
                'o-', label='Improved Ensemble', linewidth=2, markersize=8, color='blue')
        
        ax4.set_xlabel('CV Fold')
        ax4.set_ylabel('SMAPE (%)')
        ax4.set_title('Ensemble Performance by Fold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_xticks(folds)
        
        plt.tight_layout()
        plt.savefig('./visualizations/model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("üìä Comparison plots saved: visualizations/model_comparison.png")
    
    def create_comparison_report(self):
        """ÎπÑÍµê Í≤∞Í≥º Î¶¨Ìè¨Ìä∏ ÏÉùÏÑ±."""
        report_path = Path('./visualizations/model_comparison_report.md')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Model Performance Comparison Report\n\n")
            
            f.write("## Executive Summary\n\n")
            
            # Í∞úÏÑ† Ìö®Í≥º Í≥ÑÏÇ∞
            xgb_improvement = (
                (self.results['original']['xgboost']['mean'] - self.results['improved']['xgboost']['mean']) /
                self.results['original']['xgboost']['mean'] * 100
            )
            
            ensemble_improvement = (
                (self.results['original']['ensemble']['mean'] - self.results['improved']['ensemble']['mean']) /
                self.results['original']['ensemble']['mean'] * 100
            )
            
            f.write(f"- **XGBoost Improvement**: {xgb_improvement:.2f}%\n")
            f.write(f"- **Ensemble Improvement**: {ensemble_improvement:.2f}%\n\n")
            
            f.write("## Detailed Results\n\n")
            
            # Í≤∞Í≥º ÌÖåÏù¥Î∏î
            f.write("| Model | Preprocessing | Mean SMAPE | Std SMAPE | Best Fold | Worst Fold |\n")
            f.write("|-------|---------------|------------|-----------|-----------|------------|\n")
            
            for preprocessing in ['original', 'improved']:
                for model in ['xgboost', 'ensemble']:
                    result = self.results[preprocessing][model]
                    best_fold = min(result['folds'])
                    worst_fold = max(result['folds'])
                    
                    f.write(f"| {model.title()} | {preprocessing.title()} | "
                           f"{result['mean']:.4f} | {result['std']:.4f} | "
                           f"{best_fold:.4f} | {worst_fold:.4f} |\n")
            
            f.write("\n## Key Findings\n\n")
            
            if xgb_improvement > 0:
                f.write(f"‚úÖ **XGBoost showed {xgb_improvement:.2f}% improvement** with improved preprocessing\n")
            else:
                f.write(f"‚ùå **XGBoost showed {abs(xgb_improvement):.2f}% degradation** with improved preprocessing\n")
            
            if ensemble_improvement > 0:
                f.write(f"‚úÖ **Ensemble showed {ensemble_improvement:.2f}% improvement** with improved preprocessing\n")
            else:
                f.write(f"‚ùå **Ensemble showed {abs(ensemble_improvement):.2f}% degradation** with improved preprocessing\n")
            
            f.write("\n## Preprocessing Impact Analysis\n\n")
            
            f.write("### Improved Preprocessing Benefits:\n")
            f.write("1. **Target Transformation**: Log transformation reduced skewness\n")
            f.write("2. **Feature Scaling**: RobustScaler normalized feature ranges\n")
            f.write("3. **Outlier Treatment**: IQR-based clipping improved robustness\n")
            f.write("4. **Multicollinearity Removal**: VIF analysis reduced overfitting risk\n")
            f.write("5. **Advanced Features**: Degree-days, heat index, building-specific features\n\n")
            
            f.write("### Potential Issues:\n")
            f.write("- Information loss from aggressive outlier treatment\n")
            f.write("- Over-preprocessing may remove useful signal\n")
            f.write("- Log transformation requires careful inverse transformation\n\n")
            
            f.write("## Recommendations\n\n")
            
            best_original = min(self.results['original']['xgboost']['mean'], 
                              self.results['original']['ensemble']['mean'])
            best_improved = min(self.results['improved']['xgboost']['mean'], 
                              self.results['improved']['ensemble']['mean'])
            
            if best_improved < best_original:
                f.write("üéØ **Recommendation**: Use improved preprocessing pipeline\n")
                f.write(f"- Best improved model SMAPE: {best_improved:.4f}\n")
                f.write(f"- Best original model SMAPE: {best_original:.4f}\n")
                f.write(f"- Overall improvement: {((best_original - best_improved) / best_original * 100):.2f}%\n")
            else:
                f.write("üéØ **Recommendation**: Stick with original preprocessing\n")
                f.write(f"- Best original model SMAPE: {best_original:.4f}\n")
                f.write(f"- Best improved model SMAPE: {best_improved:.4f}\n")
                f.write("- Consider selective adoption of specific improvements\n")
        
        print(f"üìÑ Comparison report saved: {report_path}")


if __name__ == "__main__":
    comparison = ModelComparison()
    results = comparison.run_comparison()
    
    print("\nüéØ Model comparison completed!")
    print("Check visualizations/model_comparison.png and model_comparison_report.md for detailed results.")