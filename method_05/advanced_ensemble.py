"""
Advanced Ensemble with CatBoost and Optimized Weights
=====================================================

3-model ì•™ìƒë¸” + ê°€ì¤‘ì¹˜ ìµœì í™” + Stacking ì•™ìƒë¸”
11.8 SMAPEì—ì„œ ì¶”ê°€ ê°œì„  ëª©í‘œ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import VotingRegressor, StackingRegressor
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from solution import load_data, smape
from ts_validation import TimeSeriesCV
from improved_preprocessing import ImprovedPreprocessor


class AdvancedEnsemble:
    """ê³ ê¸‰ ì•™ìƒë¸” í´ë˜ìŠ¤."""
    
    def __init__(self):
        self.base_models = {}
        self.ensemble_models = {}
        self.validation_scores = {}
        
    def create_optimized_base_models(self):
        """ìµœì í™”ëœ ê¸°ë³¸ ëª¨ë¸ë“¤ ìƒì„±."""
        print("ğŸ”§ Creating optimized base models...")
        
        # XGBoost (ê¸°ì¡´ ëŒ€ë¹„ ê°•í™”ëœ íŒŒë¼ë¯¸í„°)
        self.base_models['xgboost'] = xgb.XGBRegressor(
            max_depth=7,
            n_estimators=500,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            colsample_bylevel=0.8,
            reg_alpha=1.5,
            reg_lambda=2.0,
            min_child_weight=3,
            gamma=0.1,
            random_state=42,
            verbosity=0
        )
        
        # LightGBM (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
        self.base_models['lightgbm'] = lgb.LGBMRegressor(
            max_depth=7,
            n_estimators=500,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=1.5,
            reg_lambda=2.0,
            min_child_weight=3,
            min_child_samples=20,
            num_leaves=100,
            feature_fraction=0.8,
            bagging_fraction=0.8,
            bagging_freq=5,
            random_state=42,
            verbosity=-1
        )
        
        # CatBoost (ìƒˆë¡œ ì¶”ê°€)
        self.base_models['catboost'] = cb.CatBoostRegressor(
            depth=6,
            iterations=500,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bylevel=0.8,
            reg_lambda=2.0,
            min_data_in_leaf=20,
            max_leaves=100,
            bootstrap_type='Bayesian',
            bagging_temperature=1.0,
            random_seed=42,
            verbose=False
        )
        
        print(f"âœ… Created {len(self.base_models)} optimized base models")
    
    def create_ensemble_models(self):
        """ë‹¤ì–‘í•œ ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±."""
        print("ğŸ­ Creating ensemble models...")
        
        # 1. ë‹¨ìˆœ í‰ê·  ì•™ìƒë¸”
        self.ensemble_models['simple_average'] = VotingRegressor([
            ('xgb', self.base_models['xgboost']),
            ('lgb', self.base_models['lightgbm']),
            ('cb', self.base_models['catboost'])
        ])
        
        # 2. ê°€ì¤‘ í‰ê·  ì•™ìƒë¸” (ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜)
        self.ensemble_models['weighted_average'] = VotingRegressor([
            ('xgb', self.base_models['xgboost']),
            ('lgb', self.base_models['lightgbm']),
            ('cb', self.base_models['catboost'])
        ], weights=[0.4, 0.35, 0.25])  # XGBoost ìµœê³  ê°€ì¤‘ì¹˜
        
        # 3. Stacking ì•™ìƒë¸” (Ridge ë©”íƒ€ëª¨ë¸)
        self.ensemble_models['stacking_ridge'] = StackingRegressor(
            estimators=[
                ('xgb', self.base_models['xgboost']),
                ('lgb', self.base_models['lightgbm']),
                ('cb', self.base_models['catboost'])
            ],
            final_estimator=Ridge(alpha=1.0),
            cv=3  # ë‚´ë¶€ êµì°¨ê²€ì¦
        )
        
        # 4. Stacking ì•™ìƒë¸” (ElasticNet ë©”íƒ€ëª¨ë¸)
        self.ensemble_models['stacking_elastic'] = StackingRegressor(
            estimators=[
                ('xgb', self.base_models['xgboost']),
                ('lgb', self.base_models['lightgbm']),
                ('cb', self.base_models['catboost'])
            ],
            final_estimator=ElasticNet(alpha=0.1, l1_ratio=0.5),
            cv=3
        )
        
        # 5. 2-level Stacking (ë” ë³µì¡í•œ êµ¬ì¡°)
        level1_models = [
            ('xgb1', xgb.XGBRegressor(max_depth=6, n_estimators=300, learning_rate=0.1, random_state=42, verbosity=0)),
            ('lgb1', lgb.LGBMRegressor(max_depth=6, n_estimators=300, learning_rate=0.1, random_state=42, verbosity=-1)),
            ('cb1', cb.CatBoostRegressor(depth=5, iterations=300, learning_rate=0.1, random_seed=42, verbose=False))
        ]
        
        level2_estimator = Ridge(alpha=0.5)
        
        self.ensemble_models['stacking_2level'] = StackingRegressor(
            estimators=level1_models,
            final_estimator=level2_estimator,
            cv=3
        )
        
        print(f"âœ… Created {len(self.ensemble_models)} ensemble models")
    
    def evaluate_all_models(self, X, y, datetime_series):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€."""
        print("ğŸ“Š Evaluating all models...")
        
        ts_cv = TimeSeriesCV(n_splits=3, test_size_days=7, gap_days=1)
        temp_df = pd.DataFrame({'datetime': datetime_series})
        splits = ts_cv.split(temp_df, 'datetime')
        
        all_models = {**self.base_models, **self.ensemble_models}
        
        for model_name, model in all_models.items():
            print(f"  Evaluating {model_name}...")
            
            fold_scores = []
            for fold, (train_idx, val_idx) in enumerate(splits):
                X_train_fold = X.iloc[train_idx]
                y_train_fold = y.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_val_fold = y.iloc[val_idx]
                
                try:
                    model.fit(X_train_fold, y_train_fold)
                    y_pred = model.predict(X_val_fold)
                    
                    # ë¡œê·¸ ë³€í™˜ ì—­ë³€í™˜
                    y_val_original = np.expm1(y_val_fold)
                    y_pred_original = np.expm1(y_pred)
                    
                    fold_smape = smape(y_val_original.values, y_pred_original)
                    fold_scores.append(fold_smape)
                    
                except Exception as e:
                    print(f"    âŒ Error in fold {fold}: {e}")
                    fold_scores.append(float('inf'))
            
            mean_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)
            
            self.validation_scores[model_name] = {
                'mean': mean_score,
                'std': std_score,
                'folds': fold_scores
            }
            
            print(f"    {model_name}: {mean_score:.4f} (Â±{std_score:.4f})")
    
    def create_model_comparison_plot(self, output_dir='./visualizations/'):
        """ëª¨ë¸ ë¹„êµ ì‹œê°í™”."""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„° ì¤€ë¹„
        model_names = list(self.validation_scores.keys())
        mean_scores = [self.validation_scores[name]['mean'] for name in model_names]
        std_scores = [self.validation_scores[name]['std'] for name in model_names]
        
        # ëª¨ë¸ íƒ€ì…ë³„ ìƒ‰ìƒ
        colors = []
        for name in model_names:
            if name in ['xgboost', 'lightgbm', 'catboost']:
                colors.append('lightcoral')  # ê¸°ë³¸ ëª¨ë¸
            elif 'stacking' in name:
                colors.append('lightblue')   # ìŠ¤íƒœí‚¹ ëª¨ë¸
            else:
                colors.append('lightgreen')  # ë³´íŒ… ëª¨ë¸
        
        # ê·¸ë˜í”„ ìƒì„±
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # 1. í‰ê·  SMAPE ë¹„êµ
        bars = ax1.bar(range(len(model_names)), mean_scores, yerr=std_scores, 
                      color=colors, alpha=0.7, capsize=5)
        ax1.set_xlabel('Model')
        ax1.set_ylabel('SMAPE')
        ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
        ax1.set_xticks(range(len(model_names)))
        ax1.set_xticklabels(model_names, rotation=45, ha='right')
        ax1.grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for i, (bar, score) in enumerate(zip(bars, mean_scores)):
            ax1.text(bar.get_x() + bar.get_width()/2., score + std_scores[i] + 0.05,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 2. Foldë³„ ì„±ëŠ¥ ë¶„í¬
        fold_data = []
        labels = []
        for name in model_names:
            if self.validation_scores[name]['mean'] != float('inf'):
                fold_data.append(self.validation_scores[name]['folds'])
                labels.append(name)
        
        if fold_data:
            ax2.boxplot(fold_data, labels=labels)
            ax2.set_xlabel('Model')
            ax2.set_ylabel('SMAPE')
            ax2.set_title('SMAPE Distribution Across Folds', fontsize=14, fontweight='bold')
            ax2.tick_params(axis='x', rotation=45)
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'advanced_ensemble_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Model comparison plot saved: {output_dir}/advanced_ensemble_comparison.png")
    
    def generate_best_ensemble_submission(self, X_train, X_test, y_train, test_df):
        """ìµœê³  ì„±ëŠ¥ ì•™ìƒë¸”ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±."""
        print("ğŸ¯ Generating submission with best ensemble model...")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = min(self.validation_scores.keys(), 
                            key=lambda x: self.validation_scores[x]['mean'])
        best_model = {**self.base_models, **self.ensemble_models}[best_model_name]
        
        print(f"ğŸ† Best model: {best_model_name} (SMAPE: {self.validation_scores[best_model_name]['mean']:.4f})")
        
        # ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
        best_model.fit(X_train, y_train)
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
        test_predictions_log = best_model.predict(X_test)
        test_predictions = np.expm1(test_predictions_log)  # ë¡œê·¸ ì—­ë³€í™˜
        test_predictions = np.maximum(test_predictions, 0)  # ìŒìˆ˜ ì œê±°
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        submission = pd.DataFrame({
            'num_date_time': test_df['num_date_time'],
            'answer': test_predictions
        })
        
        submission_file = f'submission_advanced_ensemble_{best_model_name}.csv'
        submission.to_csv(submission_file, index=False)
        
        print(f"ğŸ’¾ Submission saved: {submission_file}")
        print(f"ğŸ“Š Predictions range: {test_predictions.min():.2f} - {test_predictions.max():.2f}")
        print(f"ğŸ“Š Predictions mean: {test_predictions.mean():.2f}")
        
        return submission, best_model_name
    
    def create_ensemble_report(self, output_dir='./visualizations/'):
        """ì•™ìƒë¸” ê²°ê³¼ ë¦¬í¬íŠ¸."""
        report_path = Path(output_dir) / 'advanced_ensemble_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Advanced Ensemble Report\n\n")
            
            f.write("## Model Performance Summary\n\n")
            f.write("| Model | Mean SMAPE | Std SMAPE | Type |\n")
            f.write("|-------|------------|-----------|------|\n")
            
            for model_name, scores in self.validation_scores.items():
                model_type = "Base" if model_name in self.base_models else "Ensemble"
                f.write(f"| {model_name} | {scores['mean']:.4f} | {scores['std']:.4f} | {model_type} |\n")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
            best_model = min(self.validation_scores.items(), key=lambda x: x[1]['mean'])
            f.write(f"\n## Best Performing Model\n\n")
            f.write(f"- **Model**: {best_model[0]}\n")
            f.write(f"- **SMAPE**: {best_model[1]['mean']:.4f} (Â±{best_model[1]['std']:.4f})\n")
            
            # ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„ 
            baseline = 11.8
            improvement = (baseline - best_model[1]['mean']) / baseline * 100
            f.write(f"- **Improvement**: {improvement:.2f}% vs baseline (11.8 SMAPE)\n")
            
            f.write(f"\n## Model Types Comparison\n\n")
            
            # ê¸°ë³¸ ëª¨ë¸ë“¤
            f.write("### Base Models\n")
            for name in self.base_models.keys():
                if name in self.validation_scores:
                    score = self.validation_scores[name]['mean']
                    f.write(f"- **{name.title()}**: {score:.4f} SMAPE\n")
            
            # ì•™ìƒë¸” ëª¨ë¸ë“¤
            f.write("\n### Ensemble Models\n")
            for name in self.ensemble_models.keys():
                if name in self.validation_scores:
                    score = self.validation_scores[name]['mean']
                    f.write(f"- **{name.replace('_', ' ').title()}**: {score:.4f} SMAPE\n")
            
            f.write(f"\n## Recommendations\n\n")
            if best_model[1]['mean'] < 10.0:
                f.write("âœ… **Achieved sub-10 SMAPE target!**\n")
            elif best_model[1]['mean'] < baseline:
                f.write("âœ… **Significant improvement achieved**\n")
            else:
                f.write("âš ï¸ **Consider further optimization**\n")
            
            f.write(f"- Use **{best_model[0]}** for final predictions\n")
            f.write(f"- Expected production SMAPE: ~{best_model[1]['mean']:.2f}\n")
        
        print(f"ğŸ“„ Ensemble report saved: {report_path}")
    
    def run_advanced_ensemble(self):
        """ì „ì²´ ê³ ê¸‰ ì•™ìƒë¸” ì‹¤í–‰."""
        print("=" * 80)
        print("ADVANCED ENSEMBLE WITH CATBOOST")
        print("=" * 80)
        print("Target: Improve from current 11.8 SMAPE")
        
        # ë°ì´í„° ì¤€ë¹„
        data_dir = Path('../data')
        train_path = data_dir / 'train.csv'
        test_path = data_dir / 'test.csv'
        building_path = data_dir / 'building_info.csv'
        
        train_df, test_df = load_data(train_path, test_path, building_path)
        
        # ê°œì„ ëœ ì „ì²˜ë¦¬ ì ìš©
        preprocessor = ImprovedPreprocessor()
        X_train, X_test, y_train = preprocessor.fit_transform(train_df, test_df)
        
        # datetime ì‹œë¦¬ì¦ˆ (êµì°¨ê²€ì¦ìš©)
        from solution import engineer_features
        train_processed, _ = engineer_features(train_df.copy(), test_df.copy())
        datetime_series = train_processed['datetime']
        
        print(f"\nğŸ“Š Dataset: {len(X_train)} samples, {len(X_train.columns)} features")
        
        # 1. ê¸°ë³¸ ëª¨ë¸ë“¤ ìƒì„±
        self.create_optimized_base_models()
        
        # 2. ì•™ìƒë¸” ëª¨ë¸ë“¤ ìƒì„±
        self.create_ensemble_models()
        
        # 3. ëª¨ë“  ëª¨ë¸ í‰ê°€
        self.evaluate_all_models(X_train, y_train, datetime_series)
        
        # 4. ê²°ê³¼ ì‹œê°í™”
        self.create_model_comparison_plot()
        
        # 5. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ì œì¶œ íŒŒì¼ ìƒì„±
        submission, best_model_name = self.generate_best_ensemble_submission(
            X_train, X_test, y_train, test_df
        )
        
        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        self.create_ensemble_report()
        
        # 7. ìµœì¢… ê²°ê³¼
        print(f"\n" + "=" * 80)
        print("ADVANCED ENSEMBLE RESULTS")
        print("=" * 80)
        
        baseline = 11.8
        best_score = min(score['mean'] for score in self.validation_scores.values())
        improvement = (baseline - best_score) / baseline * 100
        
        print(f"ğŸ¯ Best SMAPE: {best_score:.4f}")
        print(f"ğŸš€ Improvement: {improvement:.2f}% vs baseline")
        print(f"ğŸ† Best model: {best_model_name}")
        
        if best_score < 9.0:
            print("ğŸ‰ Excellent! Sub-9 SMAPE achieved!")
        elif best_score < 10.0:
            print("âœ… Great! Sub-10 SMAPE achieved!")
        elif best_score < baseline:
            print("âœ… Good improvement over baseline!")
        
        return self.validation_scores, best_model_name


if __name__ == "__main__":
    ensemble = AdvancedEnsemble()
    results, best_model = ensemble.run_advanced_ensemble()
    
    print(f"\nğŸ¯ Advanced ensemble completed!")
    print(f"ğŸ† Best model: {best_model}")
    print(f"ğŸ“ˆ Ready for GPU server deployment!")