"""
Advanced solution with feature importance analysis and deeper models
8.0877 SMAPE â†’ 5-6 SMAPE ëª©í‘œ
ì²´ê³„ì ì¸ ë¶„ì„ê³¼ ê°œì„ 
"""

import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import VotingRegressor, StackingRegressor
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Compute the Symmetric Mean Absolute Percentage Error (SMAPE)."""
    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))

def load_data(train_path: Path, test_path: Path, building_path: Path) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Load and merge data."""
    rename_map = {
        'ê¸°ì˜¨(Â°C)': 'temp',
        'ê°•ìˆ˜ëŸ‰(mm)': 'rainfall',
        'í’ì†(m/s)': 'wind_speed',
        'ìŠµë„(%)': 'humidity',
        'ì¼ì¡°(hr)': 'sunshine_hours',
        'ì¼ì‚¬(MJ/m2)': 'solar_radiation',
        'ì—°ë©´ì (m2)': 'total_area',
        'ëƒ‰ë°©ë©´ì (m2)': 'cooling_area',
        'íƒœì–‘ê´‘ìš©ëŸ‰(kW)': 'pv_capacity',
        'ESSì €ì¥ìš©ëŸ‰(kWh)': 'ess_capacity',
        'PCSìš©ëŸ‰(kW)': 'pcs_capacity',
        'ê±´ë¬¼ìœ í˜•': 'building_type',
    }
    
    train = pd.read_csv(train_path, encoding='utf-8-sig')
    test = pd.read_csv(test_path, encoding='utf-8-sig')
    building_info = pd.read_csv(building_path, encoding='utf-8-sig')
    
    train.rename(columns=rename_map, inplace=True)
    test.rename(columns=rename_map, inplace=True)
    building_info.rename(columns=rename_map, inplace=True)
    
    train = train.merge(building_info, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    test = test.merge(building_info, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    
    return train, test

def advanced_feature_engineering(train: pd.DataFrame, test: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - ë” ë§ì€ í”¼ì²˜ ìƒì„±."""
    print("Applying advanced feature engineering...")
    
    # ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
    train['datetime'] = pd.to_datetime(train['ì¼ì‹œ'], format='%Y%m%d %H')
    test['datetime'] = pd.to_datetime(test['ì¼ì‹œ'], format='%Y%m%d %H')
    
    for df in (train, test):
        df['year'] = df['datetime'].dt.year
        df['month'] = df['datetime'].dt.month
        df['day'] = df['datetime'].dt.day
        df['hour'] = df['datetime'].dt.hour
        df['weekday'] = df['datetime'].dt.weekday
        df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)
        df['day_of_year'] = df['datetime'].dt.dayofyear
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    for col in ['total_area', 'cooling_area', 'pv_capacity', 'ess_capacity', 'pcs_capacity']:
        train[col] = pd.to_numeric(train[col].replace('-', np.nan), errors='coerce')
        test[col] = pd.to_numeric(test[col].replace('-', np.nan), errors='coerce')
        median = train[col].median()
        train[col] = train[col].fillna(median)
        test[col] = test[col].fillna(median)
    
    # ì¼ì¡°/ì¼ì‚¬ ì¶”ì •
    train_august = train[train['month'] == 8]
    avg_sunshine = train_august.groupby('hour')['sunshine_hours'].mean()
    avg_solar = train_august.groupby('hour')['solar_radiation'].mean()
    
    train['sunshine_est'] = train['sunshine_hours']
    train['solar_est'] = train['solar_radiation']
    test['sunshine_est'] = test['hour'].map(avg_sunshine)
    test['solar_est'] = test['hour'].map(avg_solar)
    
    train.drop(columns=['sunshine_hours', 'solar_radiation'], inplace=True)
    test.drop(columns=['sunshine_hours', 'solar_radiation'], inplace=True, errors='ignore')
    
    # ê±´ë¬¼ë³„ í†µê³„ (ê¸°ì¡´)
    building_mean = train.groupby('ê±´ë¬¼ë²ˆí˜¸')['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].mean()
    
    # ê±´ë¬¼ë³„-ì‹œê°„ í†µê³„
    bld_hour_mean = (
        train.groupby(['ê±´ë¬¼ë²ˆí˜¸', 'hour'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].mean()
        .reset_index().rename(columns={'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'bld_hour_mean'})
    )
    train = train.merge(bld_hour_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'hour'], how='left')
    test = test.merge(bld_hour_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'hour'], how='left')
    test['bld_hour_mean'] = test['bld_hour_mean'].fillna(test['ê±´ë¬¼ë²ˆí˜¸'].map(building_mean))
    
    # ê±´ë¬¼ë³„-ì£¼ë§ í†µê³„
    bld_wd_mean = (
        train.groupby(['ê±´ë¬¼ë²ˆí˜¸', 'weekday'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].mean()
        .reset_index().rename(columns={'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'bld_wd_mean'})
    )
    train = train.merge(bld_wd_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'weekday'], how='left')
    test = test.merge(bld_wd_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'weekday'], how='left')
    test['bld_wd_mean'] = test['bld_wd_mean'].fillna(test['ê±´ë¬¼ë²ˆí˜¸'].map(building_mean))
    
    # ê±´ë¬¼ë³„-ì›” í†µê³„  
    bld_month_mean = (
        train.groupby(['ê±´ë¬¼ë²ˆí˜¸', 'month'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].mean()
        .reset_index().rename(columns={'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'bld_month_mean'})
    )
    train = train.merge(bld_month_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'month'], how='left')
    test = test.merge(bld_month_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'month'], how='left')
    test['bld_month_mean'] = test['bld_month_mean'].fillna(test['ê±´ë¬¼ë²ˆí˜¸'].map(building_mean))
    
    # ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    for df in (train, test):
        # 1. ê¸°ì¡´ í”¼ì²˜ë“¤
        df['area_ratio'] = df['cooling_area'] / (df['total_area'] + 1)
        df['pv_per_area'] = df['pv_capacity'] / (df['total_area'] + 1)
        df['humidity_temp'] = df['humidity'] * df['temp']
        df['rain_wind'] = df['rainfall'] * df['wind_speed']
        
        # 2. ìˆœí™˜ ì‹œê°„ í”¼ì²˜ (ë” ë§ì€ ì£¼ê¸°)
        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)
        df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)
        df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
        df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
        
        # 3. ì˜¨ë„ ê´€ë ¨ ê³ ê¸‰ í”¼ì²˜
        df['temp_squared'] = df['temp'] ** 2
        df['temp_cubed'] = df['temp'] ** 3
        df['temp_category'] = pd.cut(df['temp'], bins=[-np.inf, 5, 15, 25, 35, np.inf], 
                                   labels=['freezing', 'cold', 'mild', 'warm', 'hot'])
        
        # 4. ìŠµë„ ê´€ë ¨ í”¼ì²˜
        df['humidity_squared'] = df['humidity'] ** 2
        df['humidity_category'] = pd.cut(df['humidity'], bins=[0, 30, 50, 70, 100], 
                                       labels=['dry', 'normal', 'humid', 'very_humid'])
        
        # 5. ë³µí•© ê¸°ìƒ í”¼ì²˜
        df['heat_index'] = df['temp'] + 0.5 * (df['humidity'] / 100 - 1) * (df['temp'] - 14.5)
        df['comfort_index'] = df['temp'] * (1 - df['humidity'] / 100)
        df['weather_stress'] = np.abs(df['temp'] - 22) + np.abs(df['humidity'] - 50) / 100
        
        # 6. ê±´ë¬¼ íš¨ìœ¨ì„± í”¼ì²˜
        df['energy_efficiency'] = (df['pv_capacity'] + 1) / (df['total_area'] + 1) * 1000
        df['cooling_efficiency'] = df['cooling_area'] / (df['total_area'] + 1)
        df['storage_ratio'] = df['ess_capacity'] / (df['pv_capacity'] + 1)
        
        # 7. ì‹œê°„ë³„ ë³µí•© í”¼ì²˜
        df['is_business_hour'] = ((df['hour'] >= 9) & (df['hour'] <= 18) & (~df['is_weekend'])).astype(int)
        df['is_peak_hour'] = df['hour'].isin([10, 11, 14, 15, 16, 17]).astype(int)
        df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)
        
        # 8. ê³„ì ˆì„± í”¼ì²˜
        df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int) 
        df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)
        df['is_transition'] = df['month'].isin([3, 4, 5, 9, 10, 11]).astype(int)
        
        # 9. ìƒí˜¸ì‘ìš© í”¼ì²˜
        df['bld_hour_temp'] = df['bld_hour_mean'] * df['temp'] / 30
        df['bld_hour_humidity'] = df['bld_hour_mean'] * df['humidity'] / 100
        df['temp_area_interaction'] = df['temp'] * df['total_area'] / 10000
        df['hour_area_interaction'] = df['hour'] * df['total_area'] / 10000
        
        # 10. ë¡œê·¸ ë³€í™˜ í”¼ì²˜ (í° ê°’ë“¤)
        df['log_total_area'] = np.log1p(df['total_area'])
        df['log_cooling_area'] = np.log1p(df['cooling_area'])
        df['log_pv_capacity'] = np.log1p(df['pv_capacity'])
    
    print(f"Advanced FE completed: {train.shape[1]} features")
    return train, test

def build_deep_ensemble(train: pd.DataFrame, test: pd.DataFrame, output_dir: Path) -> dict:
    """ë” ê¹Šê³  ë³µì¡í•œ ì•™ìƒë¸” ëª¨ë¸ êµ¬ì¶•."""
    print("Building deep ensemble model...")
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(exist_ok=True)
    
    # í”¼ì²˜ ì¤€ë¹„
    feature_cols = [col for col in train.columns 
                   if col not in ['num_date_time', 'ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ', 'datetime', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']]
    
    X = train[feature_cols]
    y = train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
    X_test = test[feature_cols]
    
    print(f"Features used: {len(feature_cols)}")
    
    # ì „ì²˜ë¦¬
    categorical_features = ['building_type', 'temp_category', 'humidity_category']
    categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first')
    numeric_transformer = StandardScaler()
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', categorical_transformer, categorical_features),
            ('num', numeric_transformer, [col for col in feature_cols if col not in categorical_features])
        ]
    )
    
    # 1. ë” ê¹Šì€ XGBoost
    xgb_model = xgb.XGBRegressor(
        max_depth=15,  # ë” ê¹Šê²Œ
        n_estimators=2500,  # ë” ë§ì€ íŠ¸ë¦¬
        learning_rate=0.01,  # ë” ë‚®ì€ í•™ìŠµë¥ 
        subsample=0.8,
        colsample_bytree=0.8,
        colsample_bylevel=0.8,
        colsample_bynode=0.8,
        reg_alpha=0.5,
        reg_lambda=3.0,
        min_child_weight=5,
        gamma=0.1,
        objective='reg:squarederror',
        tree_method='gpu_hist',
        gpu_id=0,
        random_state=42,
    )
    
    # 2. ë” ê¹Šì€ LightGBM
    lgb_model = lgb.LGBMRegressor(
        max_depth=20,  # ë” ê¹Šê²Œ
        n_estimators=3000,  # ë” ë§ì€ íŠ¸ë¦¬
        learning_rate=0.008,  # ë” ë‚®ì€ í•™ìŠµë¥ 
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.8,
        reg_lambda=3.5,
        num_leaves=500,  # ë” ë§ì€ ì
        min_child_samples=10,
        min_child_weight=0.01,
        device='gpu',
        gpu_use_dp=True,
        random_state=42,
        verbosity=-1
    )
    
    # 3. ë” ê¹Šì€ CatBoost
    cat_model = cb.CatBoostRegressor(
        depth=12,  # ë” ê¹Šê²Œ
        iterations=2000,  # ë” ë§ì€ ë°˜ë³µ
        learning_rate=0.01,  # ë” ë‚®ì€ í•™ìŠµë¥ 
        bootstrap_type='Bernoulli',
        subsample=0.8,
        reg_lambda=3.0,
        min_data_in_leaf=5,
        max_leaves=1000,  # ë” ë§ì€ ì
        task_type='GPU',
        gpu_ram_part=0.7,
        random_seed=42,
        verbose=False
    )
    
    # 4. Stacking Ensemble (ë” ë³µì¡í•œ ë©”íƒ€ëŸ¬ë„ˆ)
    stacking_model = StackingRegressor(
        estimators=[
            ('xgb', Pipeline([('preprocess', preprocessor), ('model', xgb_model)])),
            ('lgb', Pipeline([('preprocess', preprocessor), ('model', lgb_model)])),
            ('cat', Pipeline([('preprocess', preprocessor), ('model', cat_model)]))
        ],
        final_estimator=Ridge(alpha=10.0),  # ì •ê·œí™”ëœ ë©”íƒ€ëŸ¬ë„ˆ
        cv=3
    )
    
    # ê²€ì¦ ë¶„í• 
    cutoff = pd.Timestamp('2024-08-18')
    train_mask = train['datetime'] < cutoff
    val_mask = ~train_mask
    X_train, y_train = X.loc[train_mask], y.loc[train_mask]
    X_val, y_val = X.loc[val_mask], y.loc[val_mask]
    
    print(f"Training: {X_train.shape}, Validation: {X_val.shape}")
    
    # ëª¨ë¸ í•™ìŠµ
    print("Training deep stacking ensemble...")
    stacking_model.fit(X_train, y_train)
    
    # ê²€ì¦
    val_pred = stacking_model.predict(X_val)
    val_smape = smape(y_val.values, val_pred)
    
    print(f"Validation SMAPE: {val_smape:.4f}")
    
    # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    print("Analyzing feature importance...")
    analyze_feature_importance(stacking_model, feature_cols, categorical_features, output_dir)
    
    # ìµœì¢… í•™ìŠµ
    print("Final training on full dataset...")
    stacking_model.fit(X, y)
    
    # ì˜ˆì¸¡
    test_pred = stacking_model.predict(X_test)
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'validation_smape': val_smape,
        'model': stacking_model,
        'feature_cols': feature_cols,
        'predictions': test_pred
    }
    
    # ê²€ì¦ ê²°ê³¼ ì €ì¥
    with open(output_dir / 'validation_results.txt', 'w') as f:
        f.write(f'Deep Ensemble Validation SMAPE: {val_smape:.6f}\n')
        f.write(f'Target: 5-6 SMAPE\n')
        f.write(f'Features used: {len(feature_cols)}\n')
        f.write(f'Model: Stacking(XGBoost + LightGBM + CatBoost)\n')
    
    # ì œì¶œ íŒŒì¼ ì €ì¥
    submission = test[['num_date_time']].copy()
    submission['answer'] = test_pred
    submission.to_csv(output_dir / 'submission_deep_ensemble.csv', index=False)
    
    return results

def analyze_feature_importance(model, feature_cols, categorical_features, output_dir):
    """í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ë° ì‹œê°í™”."""
    try:
        # XGBoost ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ
        xgb_model = model.named_estimators_['xgb']['model']
        
        # ì „ì²˜ë¦¬ëœ í”¼ì²˜ëª… ìƒì„±
        cat_transformer = model.named_estimators_['xgb']['preprocess'].named_transformers_['cat']
        if hasattr(cat_transformer, 'get_feature_names_out'):
            cat_features = cat_transformer.get_feature_names_out(categorical_features)
        else:
            cat_features = [f"cat_{i}" for i in range(len(categorical_features) * 2)]  # ì¶”ì •
        
        numeric_features = [col for col in feature_cols if col not in categorical_features]
        all_features = list(cat_features) + numeric_features
        
        # í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ
        importance = xgb_model.feature_importances_
        
        # DataFrame ìƒì„±
        importance_df = pd.DataFrame({
            'feature': all_features[:len(importance)],
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        # Top 30 í”¼ì²˜ ì‹œê°í™”
        plt.figure(figsize=(12, 10))
        top_features = importance_df.head(30)
        sns.barplot(data=top_features, x='importance', y='feature')
        plt.title('Top 30 Feature Importance (XGBoost)')
        plt.tight_layout()
        plt.savefig(output_dir / 'feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # CSV ì €ì¥
        importance_df.to_csv(output_dir / 'feature_importance.csv', index=False)
        
        print(f"Feature importance analysis saved to {output_dir}")
        print("Top 10 most important features:")
        for i, row in importance_df.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")
            
    except Exception as e:
        print(f"Feature importance analysis failed: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜."""
    print("=" * 70)
    print("ADVANCED DEEP ENSEMBLE: 8.08 SMAPE â†’ 5-6 SMAPE")
    print("More features + Deeper models + Feature analysis")
    print("=" * 70)
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    output_dir = Path('solution_optimized_result')
    output_dir.mkdir(exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ
    base_dir = Path('../data')
    train_path = base_dir / 'train.csv'
    test_path = base_dir / 'test.csv'
    building_path = base_dir / 'building_info.csv'
    
    print("Loading and engineering features...")
    train_df, test_df = load_data(train_path, test_path, building_path)
    train_fe, test_fe = advanced_feature_engineering(train_df, test_df)
    
    print("Building deep ensemble model...")
    results = build_deep_ensemble(train_fe, test_fe, output_dir)
    
    # ê²°ê³¼ ì¶œë ¥
    val_smape = results['validation_smape']
    print(f"\nğŸ¯ Deep Ensemble Results:")
    print(f"ğŸ“Š Validation SMAPE: {val_smape:.4f}")
    print(f"ğŸ¯ Target: 5-6 SMAPE")
    print(f"ğŸ“ Results saved to: {output_dir}")
    
    if val_smape < 6.0:
        print("ğŸ‰ SUCCESS! Target achieved!")
    elif val_smape < 7.0:
        print("âœ… Good progress! Very close to target.")
    else:
        print("ğŸ“ˆ Need further optimization.")
    
    print(f"\nğŸ“‹ Files generated:")
    print(f"  - submission_deep_ensemble.csv")
    print(f"  - feature_importance.csv")
    print(f"  - feature_importance.png")
    print(f"  - validation_results.txt")

if __name__ == "__main__":
    main()