#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœì¢… ëª¨ë¸ - ë¹ ë¥¸ ìµœì í™”ì™€ ì•™ìƒë¸”
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
import warnings

warnings.filterwarnings('ignore')

def smape(y_true, y_pred, epsilon=1e-8):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denominator = np.maximum(denominator, epsilon)
    
    smape_val = np.mean(np.abs(y_true - y_pred) / denominator) * 100
    return smape_val

def safe_exp_transform(y_log):
    y_log_clipped = np.clip(y_log, -10, 10)
    return np.expm1(y_log_clipped)

def prepare_data():
    """ë°ì´í„° ì¤€ë¹„"""
    print("ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    train_df = pd.read_csv('../processed_data/train_processed_iqr_log.csv')
    test_df = pd.read_csv('../processed_data/test_processed_iqr_log.csv')
    
    y_train = train_df['power_transformed']
    
    # ì¤‘ìš”í•œ í”¼ì²˜ë“¤ë§Œ ì„ íƒ
    important_features = [
        'hour', 'weekday', 'month', 'is_weekend', 'is_holiday',
        'ê¸°ì˜¨(Â°C)', 'ìŠµë„(%)', 'í’ì†(m/s)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)',
        'discomfort_index', 'apparent_temp', 'cooling_degree_days', 'heating_degree_days',
        'cooling_area_ratio', 'building_type_frequency'
    ]
    
    # ì‹œì°¨ ë³€ìˆ˜ë“¤ (ê°€ì¥ ì¤‘ìš”)
    lag_features = [
        'power_lag_1h', 'power_lag_24h', 'power_lag_168h',
        'power_rolling_mean_24h', 'power_rolling_std_24h', 'power_rolling_mean_7d'
    ]
    
    # ê±´ë¬¼ ìœ í˜• ë”ë¯¸ ë³€ìˆ˜ë“¤
    building_type_features = [col for col in train_df.columns if col.startswith('ê±´ë¬¼ìœ í˜•_')]
    
    all_features = important_features + lag_features + building_type_features
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì„ íƒ
    available_features = [col for col in all_features if col in train_df.columns]
    
    X_train = train_df[available_features].fillna(0)
    
    # Test ë°ì´í„°ë„ ë™ì¼í•œ í”¼ì²˜ë¡œ
    test_available_features = [col for col in available_features if col in test_df.columns]
    X_test = test_df[test_available_features].fillna(0)
    
    print(f"Train í”¼ì²˜: {X_train.shape[1]}, Test í”¼ì²˜: {X_test.shape[1]}")
    print(f"ì‚¬ìš©ëœ í”¼ì²˜: {test_available_features}")
    
    return X_train, y_train, X_test, test_df['num_date_time']

def test_model_performance(X_train, y_train):
    """ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # ì‹œê³„ì—´ ë¶„í•  (ë§ˆì§€ë§‰ 20%ë¥¼ validationìœ¼ë¡œ)
    split_idx = int(len(X_train) * 0.8)
    X_tr, X_val = X_train.iloc[:split_idx], X_train.iloc[split_idx:]
    y_tr, y_val = y_train.iloc[:split_idx], y_train.iloc[split_idx:]
    
    models = {}
    scores = {}
    
    # LightGBM (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
    lgb_params = {
        'objective': 'regression',
        'metric': 'rmse',
        'num_leaves': 120,
        'learning_rate': 0.08,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_child_samples': 20,
        'lambda_l1': 2,
        'lambda_l2': 2,
        'verbosity': -1,
        'random_state': 42
    }
    
    models['lgb'] = lgb.LGBMRegressor(**lgb_params, n_estimators=800)
    models['lgb'].fit(X_tr, y_tr, eval_set=[(X_val, y_val)],
                     callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
    
    # XGBoost (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
    xgb_params = {
        'objective': 'reg:squarederror',
        'max_depth': 6,
        'learning_rate': 0.08,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 5,
        'reg_alpha': 2,
        'reg_lambda': 2,
        'random_state': 42,
        'verbosity': 0
    }
    
    models['xgb'] = xgb.XGBRegressor(**xgb_params, n_estimators=800)
    models['xgb'].fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)
    
    # CatBoost
    models['cat'] = CatBoostRegressor(
        iterations=800,
        depth=6,
        learning_rate=0.08,
        l2_leaf_reg=3,
        random_seed=42,
        verbose=False
    )
    models['cat'].fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)
    
    # ê° ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    predictions = {}
    for name, model in models.items():
        y_pred = model.predict(X_val)
        y_val_orig = safe_exp_transform(y_val.values)
        y_pred_orig = safe_exp_transform(y_pred)
        y_pred_orig = np.maximum(y_pred_orig, 0)
        
        score = smape(y_val_orig, y_pred_orig)
        scores[name] = score
        predictions[name] = y_pred_orig
        
        print(f"{name.upper()} SMAPE: {score:.4f}")
    
    # ì•™ìƒë¸”
    ensemble_pred = (predictions['lgb'] * 0.4 + 
                    predictions['xgb'] * 0.4 + 
                    predictions['cat'] * 0.2)
    
    ensemble_score = smape(y_val_orig, ensemble_pred)
    scores['ensemble'] = ensemble_score
    
    print(f"Ensemble SMAPE: {ensemble_score:.4f}")
    
    return models, scores

def train_final_models_and_predict(X_train, y_train, X_test):
    """ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡"""
    print("ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    
    predictions = {}
    
    # LightGBM
    lgb_model = lgb.LGBMRegressor(
        objective='regression',
        metric='rmse',
        num_leaves=120,
        learning_rate=0.08,
        feature_fraction=0.8,
        bagging_fraction=0.8,
        bagging_freq=5,
        min_child_samples=20,
        lambda_l1=2,
        lambda_l2=2,
        verbosity=-1,
        random_state=42,
        n_estimators=1200
    )
    lgb_model.fit(X_train, y_train)
    lgb_pred = safe_exp_transform(lgb_model.predict(X_test))
    predictions['lgb'] = np.maximum(lgb_pred, 0)
    
    # XGBoost
    xgb_model = xgb.XGBRegressor(
        objective='reg:squarederror',
        max_depth=6,
        learning_rate=0.08,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=5,
        reg_alpha=2,
        reg_lambda=2,
        random_state=42,
        verbosity=0,
        n_estimators=1200
    )
    xgb_model.fit(X_train, y_train)
    xgb_pred = safe_exp_transform(xgb_model.predict(X_test))
    predictions['xgb'] = np.maximum(xgb_pred, 0)
    
    # CatBoost
    cat_model = CatBoostRegressor(
        iterations=1200,
        depth=6,
        learning_rate=0.08,
        l2_leaf_reg=3,
        random_seed=42,
        verbose=False
    )
    cat_model.fit(X_train, y_train)
    cat_pred = safe_exp_transform(cat_model.predict(X_test))
    predictions['cat'] = np.maximum(cat_pred, 0)
    
    # ì•™ìƒë¸”
    ensemble_pred = (predictions['lgb'] * 0.4 + 
                    predictions['xgb'] * 0.4 + 
                    predictions['cat'] * 0.2)
    
    return ensemble_pred, predictions

def main():
    print("ìµœì¢… ëª¨ë¸ë§ ì‹œì‘")
    print("=" * 50)
    
    # ë°ì´í„° ì¤€ë¹„
    X_train, y_train, X_test, test_ids = prepare_data()
    
    # ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    models, scores = test_model_performance(X_train, y_train)
    
    print(f"\nì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for name, score in scores.items():
        print(f"{name.upper()}: {score:.4f}")
        if score <= 6.0:
            print(f"  âœ… ëª©í‘œ ë‹¬ì„±!")
        else:
            print(f"  ğŸ“ˆ ëª©í‘œê¹Œì§€ {score - 6.0:.4f} ê°œì„  í•„ìš”")
    
    best_score = min(scores.values())
    print(f"\nìµœê³  ì„±ëŠ¥: {best_score:.4f}")
    
    # ìµœì¢… ì˜ˆì¸¡
    ensemble_pred, individual_preds = train_final_models_and_predict(X_train, y_train, X_test)
    
    # ì œì¶œ íŒŒì¼ë“¤ ìƒì„±
    print(f"\nì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
    
    # ì•™ìƒë¸” ì œì¶œ íŒŒì¼
    submission = pd.DataFrame({
        'num_date_time': test_ids,
        'answer': ensemble_pred
    })
    submission.to_csv('submission.csv', index=False)
    print(f"ì•™ìƒë¸” ì œì¶œ íŒŒì¼: submission.csv")
    
    # ê°œë³„ ëª¨ë¸ ì œì¶œ íŒŒì¼ë“¤
    for name, pred in individual_preds.items():
        individual_submission = pd.DataFrame({
            'num_date_time': test_ids,
            'answer': pred
        })
        individual_submission.to_csv(f'submission_{name}.csv', index=False)
        print(f"{name.upper()} ì œì¶œ íŒŒì¼: submission_{name}.csv")
    
    print(f"\nìµœì¢… ê²°ê³¼:")
    print(f"ì•™ìƒë¸” ì˜ˆì¸¡ê°’ ë²”ìœ„: {ensemble_pred.min():.2f} ~ {ensemble_pred.max():.2f}")
    print(f"ì•™ìƒë¸” ì˜ˆì¸¡ê°’ í‰ê· : {ensemble_pred.mean():.2f}")
    
    if best_score <= 6.0:
        print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! SMAPE â‰¤ 6.0")
        print("ì œì¶œ íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"ëª©í‘œê¹Œì§€ {best_score - 6.0:.4f} ë” ê°œì„ ì´ í•„ìš”í•˜ì§€ë§Œ,")
        print("í˜„ì¬ ì„±ëŠ¥ë„ ê²½ìŸë ¥ ìˆëŠ” ìˆ˜ì¤€ì…ë‹ˆë‹¤!")

if __name__ == "__main__":
    main()