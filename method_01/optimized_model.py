#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœì í™”ëœ ëª¨ë¸ - SMAPE 6 ì´í•˜ ë‹¬ì„±
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
import optuna
import warnings
from sklearn.model_selection import KFold

warnings.filterwarnings('ignore')

def smape(y_true, y_pred, epsilon=1e-8):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denominator = np.maximum(denominator, epsilon)
    
    smape_val = np.mean(np.abs(y_true - y_pred) / denominator) * 100
    return smape_val

def safe_exp_transform(y_log):
    y_log_clipped = np.clip(y_log, -10, 10)
    return np.expm1(y_log_clipped)

def prepare_data():
    """ë°ì´í„° ì¤€ë¹„ ë° í”¼ì²˜ ì„ íƒ"""
    print("ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    train_df = pd.read_csv('../processed_data/train_processed_iqr_log.csv')
    test_df = pd.read_csv('../processed_data/test_processed_iqr_log.csv')
    
    # íƒ€ê²Ÿ ì„¤ì •
    y_train = train_df['power_transformed']
    
    # í”¼ì²˜ ì„ íƒ (ì¤‘ìš”í•œ í”¼ì²˜ë“¤ ìš°ì„ )
    exclude_cols = ['num_date_time', 'ì¼ì‹œ', 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)', 'ê±´ë¬¼ë²ˆí˜¸', 
                   'power_deseasonalized', 'power_transformed']
    
    feature_cols = [col for col in train_df.columns if col not in exclude_cols]
    
    # NaNì´ ë§ì€ ì»¬ëŸ¼ ì œê±°
    good_features = []
    for col in feature_cols:
        if train_df[col].notna().sum() > len(train_df) * 0.7:  # 70% ì´ìƒ valid
            good_features.append(col)
    
    print(f"ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(good_features)}")
    
    # Train ë°ì´í„°
    X_train = train_df[good_features].fillna(0)
    
    # Test ë°ì´í„° (Trainê³¼ ê³µí†µ í”¼ì²˜ë§Œ)
    test_features = [col for col in good_features if col in test_df.columns]
    X_test = test_df[test_features].fillna(0)
    
    print(f"Train í”¼ì²˜: {X_train.shape[1]}, Test í”¼ì²˜: {X_test.shape[1]}")
    
    return X_train, y_train, X_test, test_df['num_date_time']

def optimize_lightgbm(X_train, y_train, n_trials=50):
    """LightGBM ìµœì í™”"""
    print("LightGBM ìµœì í™” ì¤‘...")
    
    def objective(trial):
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'boosting_type': 'gbdt',
            'num_leaves': trial.suggest_int('num_leaves', 31, 300),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
            'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),
            'lambda_l2': trial.suggest_float('lambda_l2', 0, 10),
            'verbosity': -1,
            'random_state': 42
        }
        
        # ê°„ë‹¨í•œ time series cross validation
        scores = []
        kf = KFold(n_splits=3, shuffle=False)
        
        for train_idx, val_idx in kf.split(X_train):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            model = lgb.LGBMRegressor(**params, n_estimators=500)
            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], 
                     callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)])
            
            y_pred = model.predict(X_val)
            
            # SMAPE ê³„ì‚°
            y_val_orig = safe_exp_transform(y_val.values)
            y_pred_orig = safe_exp_transform(y_pred)
            y_pred_orig = np.maximum(y_pred_orig, 0)
            
            score = smape(y_val_orig, y_pred_orig)
            scores.append(score)
        
        return np.mean(scores)
    
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=n_trials, timeout=1200)  # 20ë¶„ ì œí•œ
    
    print(f"LightGBM ìµœì  SMAPE: {study.best_value:.4f}")
    return study.best_params, study.best_value

def optimize_xgboost(X_train, y_train, n_trials=50):
    """XGBoost ìµœì í™”"""
    print("XGBoost ìµœì í™” ì¤‘...")
    
    def objective(trial):
        params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),
            'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
            'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
            'random_state': 42,
            'verbosity': 0
        }
        
        scores = []
        kf = KFold(n_splits=3, shuffle=False)
        
        for train_idx, val_idx in kf.split(X_train):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            model = xgb.XGBRegressor(**params, n_estimators=500)
            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],
                     early_stopping_rounds=50, verbose=False)
            
            y_pred = model.predict(X_val)
            
            y_val_orig = safe_exp_transform(y_val.values)
            y_pred_orig = safe_exp_transform(y_pred)
            y_pred_orig = np.maximum(y_pred_orig, 0)
            
            score = smape(y_val_orig, y_pred_orig)
            scores.append(score)
        
        return np.mean(scores)
    
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=n_trials, timeout=1200)
    
    print(f"XGBoost ìµœì  SMAPE: {study.best_value:.4f}")
    return study.best_params, study.best_value

def train_and_predict(X_train, y_train, X_test, lgb_params, xgb_params):
    """ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡"""
    print("ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    
    predictions = {}
    
    # LightGBM
    print("LightGBM í›ˆë ¨...")
    lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=1000, verbosity=-1)
    lgb_model.fit(X_train, y_train)
    lgb_pred = lgb_model.predict(X_test)
    predictions['lgb'] = safe_exp_transform(lgb_pred)
    
    # XGBoost
    print("XGBoost í›ˆë ¨...")
    xgb_model = xgb.XGBRegressor(**xgb_params, n_estimators=1000, verbosity=0)
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)
    predictions['xgb'] = safe_exp_transform(xgb_pred)
    
    # CatBoost (ê°„ë‹¨í•œ íŒŒë¼ë¯¸í„°)
    print("CatBoost í›ˆë ¨...")
    cat_model = CatBoostRegressor(
        iterations=1000,
        depth=6,
        learning_rate=0.1,
        l2_leaf_reg=3,
        random_seed=42,
        verbose=False
    )
    cat_model.fit(X_train, y_train)
    cat_pred = cat_model.predict(X_test)
    predictions['cat'] = safe_exp_transform(cat_pred)
    
    # ì•™ìƒë¸” (ê°€ì¤‘ í‰ê· )
    ensemble_pred = (predictions['lgb'] * 0.4 + 
                    predictions['xgb'] * 0.4 + 
                    predictions['cat'] * 0.2)
    
    ensemble_pred = np.maximum(ensemble_pred, 0)
    
    print("ì˜ˆì¸¡ ì™„ë£Œ!")
    return ensemble_pred, predictions

def main():
    print("ìµœì í™”ëœ ëª¨ë¸ë§ ì‹œì‘")
    print("=" * 50)
    
    # ë°ì´í„° ì¤€ë¹„
    X_train, y_train, X_test, test_ids = prepare_data()
    
    # ëª¨ë¸ ìµœì í™”
    lgb_params, lgb_score = optimize_lightgbm(X_train, y_train, n_trials=30)
    xgb_params, xgb_score = optimize_xgboost(X_train, y_train, n_trials=30)
    
    print(f"\nìµœì í™” ê²°ê³¼:")
    print(f"LightGBM SMAPE: {lgb_score:.4f}")
    print(f"XGBoost SMAPE: {xgb_score:.4f}")
    
    best_score = min(lgb_score, xgb_score)
    print(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
    
    if best_score <= 6.0:
        print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! SMAPE â‰¤ 6.0")
    else:
        print(f"ëª©í‘œê¹Œì§€ {best_score - 6.0:.4f} ë” ê°œì„  í•„ìš”")
    
    # ìµœì¢… ì˜ˆì¸¡
    ensemble_pred, individual_preds = train_and_predict(
        X_train, y_train, X_test, lgb_params, xgb_params
    )
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = pd.DataFrame({
        'num_date_time': test_ids,
        'answer': ensemble_pred
    })
    
    submission_path = 'submission_optimized.csv'
    submission.to_csv(submission_path, index=False)
    
    print(f"\nì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
    print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {ensemble_pred.min():.2f} ~ {ensemble_pred.max():.2f}")
    print(f"ì˜ˆì¸¡ê°’ í‰ê· : {ensemble_pred.mean():.2f}")
    
    # ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ë„ ì €ì¥
    for name, pred in individual_preds.items():
        individual_submission = pd.DataFrame({
            'num_date_time': test_ids,
            'answer': pred
        })
        individual_submission.to_csv(f'submission_{name}.csv', index=False)
        print(f"ê°œë³„ ëª¨ë¸ {name} ì œì¶œ íŒŒì¼: submission_{name}.csv")

if __name__ == "__main__":
    main()