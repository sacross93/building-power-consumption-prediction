#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í–¥ìƒëœ ì „ë ¥ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ë§ - SMAPE 6 ì´í•˜ ë‹¬ì„± ëª©í‘œ
Author: Claude  
Date: 2025-07-27
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from catboost import CatBoostRegressor
import optuna
import warnings
import pickle
import os
import gc
import sys
from datetime import datetime
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.append('..')

# GPU ì§€ì› í™•ì¸
try:
    import torch
    GPU_AVAILABLE = torch.cuda.is_available()
    if GPU_AVAILABLE:
        print(f"GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
except ImportError:
    GPU_AVAILABLE = False
    print("PyTorch ì—†ìŒ - GPU ì‚¬ìš© ë¶ˆê°€")

# SMAPE ê³„ì‚° í•¨ìˆ˜
def smape(y_true, y_pred, epsilon=1e-8):
    """SMAPE (Symmetric Mean Absolute Percentage Error) ê³„ì‚°"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # ë¶„ëª¨ê°€ 0ì— ê°€ê¹Œìš´ ê²½ìš° ì²˜ë¦¬
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denominator = np.maximum(denominator, epsilon)
    
    smape_val = np.mean(np.abs(y_true - y_pred) / denominator) * 100
    return smape_val

def safe_exp_transform(y_log):
    """ì•ˆì „í•œ exp ì—­ë³€í™˜"""
    # ë„ˆë¬´ í° ê°’ í´ë¦¬í•‘í•˜ì—¬ overflow ë°©ì§€
    y_log_clipped = np.clip(y_log, -10, 10)
    return np.expm1(y_log_clipped)

def evaluate_predictions(y_true, y_pred, prefix=""):
    """ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€"""
    # ìŒìˆ˜ ì˜ˆì¸¡ê°’ ì²˜ë¦¬
    y_pred_positive = np.maximum(y_pred, 0)
    
    results = {
        'smape': smape(y_true, y_pred_positive),
        'mae': mean_absolute_error(y_true, y_pred_positive),
        'negative_predictions': (y_pred < 0).sum(),
        'zero_predictions': (y_pred_positive == 0).sum(),
        'mean_pred': np.mean(y_pred_positive),
        'std_pred': np.std(y_pred_positive),
        'min_pred': np.min(y_pred_positive),
        'max_pred': np.max(y_pred_positive)
    }
    
    print(f"\n{prefix} ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€:")
    print(f"SMAPE: {results['smape']:.4f}")
    print(f"MAE: {results['mae']:.4f}")
    print(f"ìŒìˆ˜ ì˜ˆì¸¡: {results['negative_predictions']}ê°œ")
    print(f"0 ì˜ˆì¸¡: {results['zero_predictions']}ê°œ")
    print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {results['min_pred']:.2f} ~ {results['max_pred']:.2f}")
    print(f"ì˜ˆì¸¡ê°’ í‰ê· : {results['mean_pred']:.2f} Â± {results['std_pred']:.2f}")
    
    return results

class EnhancedPowerPredictor:
    """í–¥ìƒëœ ì „ë ¥ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, use_gpu=True, random_state=42):
        self.use_gpu = use_gpu and GPU_AVAILABLE
        self.random_state = random_state
        self.models = {}
        self.best_params = {}
        self.cv_scores = {}
        self.feature_importance = {}
        
        # ê²°ê³¼ ì €ì¥ í´ë”
        os.makedirs('../models', exist_ok=True)
        os.makedirs('../results', exist_ok=True)
        
        print(f"GPU ì‚¬ìš©: {self.use_gpu}")
        
    def load_processed_data(self, data_version='iqr_log'):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_version}")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        train_path = f'../processed_data/train_processed_{data_version}.csv'
        test_path = f'../processed_data/test_processed_{data_version}.csv'
        features_path = f'../processed_data/feature_columns_{data_version}.txt'
        
        self.train_df = pd.read_csv(train_path)
        self.test_df = pd.read_csv(test_path)
        
        # í”¼ì²˜ ì»¬ëŸ¼ ë¡œë“œ
        with open(features_path, 'r', encoding='utf-8') as f:
            self.feature_columns = [line.strip() for line in f.readlines()]
        
        print(f"Train: {self.train_df.shape}, Test: {self.test_df.shape}")
        print(f"Features: {len(self.feature_columns)}ê°œ")
        
        return self.train_df, self.test_df, self.feature_columns
    
    def prepare_features(self):
        """í”¼ì²˜ ì¤€ë¹„"""
        print("í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
        available_features = [col for col in self.feature_columns if col in self.train_df.columns]
        
        self.X_train = self.train_df[available_features]
        self.y_train = self.train_df['power_transformed']  # log ë³€í™˜ëœ target
        
        # Test í”¼ì²˜ (trainê³¼ ë™ì¼í•œ ìˆœì„œë¡œ)
        test_available_features = [col for col in available_features if col in self.test_df.columns]
        self.X_test = self.test_df[test_available_features]
        
        print(f"Train features: {self.X_train.shape[1]}")
        print(f"Test features: {self.X_test.shape[1]}")
        
        # NaN ì²˜ë¦¬
        self.X_train = self.X_train.fillna(0)
        self.X_test = self.X_test.fillna(0)
        
        return self.X_train, self.y_train, self.X_test
    
    def create_validation_split(self, test_ratio=0.15):
        """ì‹œê³„ì—´ ê³ ë ¤í•œ validation split ìƒì„±"""
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        if 'ì¼ì‹œ' in self.train_df.columns:
            sorted_indices = self.train_df.sort_values('ì¼ì‹œ').index
        else:
            sorted_indices = self.train_df.index
        
        # ë§ˆì§€ë§‰ 15%ë¥¼ validationìœ¼ë¡œ ì‚¬ìš©
        split_idx = int(len(sorted_indices) * (1 - test_ratio))
        
        train_indices = sorted_indices[:split_idx]
        val_indices = sorted_indices[split_idx:]
        
        X_train_split = self.X_train.loc[train_indices]
        y_train_split = self.y_train.loc[train_indices]
        X_val_split = self.X_train.loc[val_indices]
        y_val_split = self.y_train.loc[val_indices]
        
        print(f"Train split: {len(X_train_split)}, Val split: {len(X_val_split)}")
        
        return X_train_split, X_val_split, y_train_split, y_val_split
    
    def optimize_lightgbm(self, n_trials=200):
        """LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("LightGBM ìµœì í™” ì‹œì‘...")
        
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        def objective(trial):
            params = {
                'objective': 'regression',
                'metric': 'None',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 20, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.3, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.3, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),
                'lambda_l1': trial.suggest_float('lambda_l1', 0, 20),
                'lambda_l2': trial.suggest_float('lambda_l2', 0, 20),
                'verbosity': -1,
                'random_state': self.random_state,
                'n_jobs': -1
            }
            
            # GPU ì„¤ì •
            if self.use_gpu:
                params.update({
                    'device_type': 'gpu',
                    'gpu_platform_id': 0,
                    'gpu_device_id': 0,
                    'max_bin': trial.suggest_int('max_bin', 63, 255),
                    'gpu_use_dp': False
                })
            
            try:
                train_set = lgb.Dataset(X_train_split, label=y_train_split)
                val_set = lgb.Dataset(X_val_split, label=y_val_split, reference=train_set)
                
                model = lgb.train(
                    params,
                    train_set,
                    num_boost_round=2000,
                    valid_sets=[val_set],
                    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
                )
                
                y_pred = model.predict(X_val_split)
                
                # SMAPE ê³„ì‚° (log ì—­ë³€í™˜ í›„)
                y_val_original = safe_exp_transform(y_val_split.values)
                y_pred_original = safe_exp_transform(y_pred)
                y_pred_original = np.maximum(y_pred_original, 0)
                
                smape_score = smape(y_val_original, y_pred_original)
                
                return smape_score
                
            except Exception as e:
                print(f"Trial ì—ëŸ¬: {e}")
                return float('inf')
        
        study = optuna.create_study(
            direction='minimize',
            pruner=optuna.pruners.MedianPruner(n_startup_trials=20, n_warmup_steps=10)
        )
        
        study.optimize(objective, n_trials=n_trials, timeout=3600)
        
        self.best_params['lightgbm'] = study.best_params
        self.cv_scores['lightgbm'] = study.best_value
        
        print(f"\nLightGBM ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in self.best_params['lightgbm'].items():
            print(f"  {key}: {value}")
        print(f"ìµœì  SMAPE: {self.cv_scores['lightgbm']:.4f}")
        
        return study.best_params
    
    def optimize_xgboost(self, n_trials=200):
        """XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("XGBoost ìµœì í™” ì‹œì‘...")
        
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        def objective(trial):
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 500, 3000),
                'subsample': trial.suggest_float('subsample', 0.4, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),
                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.4, 1.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),
                'random_state': self.random_state,
                'n_jobs': -1
            }
            
            # GPU ì„¤ì •
            if self.use_gpu:
                params.update({
                    'tree_method': 'hist',
                    'device': 'cuda',
                    'max_bin': trial.suggest_int('max_bin', 256, 1024)
                })
            
            try:
                dtrain = xgb.DMatrix(X_train_split, label=y_train_split)
                dval = xgb.DMatrix(X_val_split, label=y_val_split)
                
                model = xgb.train(
                    params,
                    dtrain,
                    evals=[(dval, 'eval')],
                    num_boost_round=params['n_estimators'],
                    early_stopping_rounds=100,
                    verbose_eval=0
                )
                
                y_pred = model.predict(dval)
                
                # SMAPE ê³„ì‚° (log ì—­ë³€í™˜ í›„)
                y_val_original = safe_exp_transform(y_val_split.values)
                y_pred_original = safe_exp_transform(y_pred)
                y_pred_original = np.maximum(y_pred_original, 0)
                
                smape_score = smape(y_val_original, y_pred_original)
                
                return smape_score
                
            except Exception as e:
                print(f"XGBoost Trial ì—ëŸ¬: {e}")
                return float('inf')
        
        study = optuna.create_study(
            direction='minimize',
            pruner=optuna.pruners.MedianPruner(n_startup_trials=20, n_warmup_steps=10)
        )
        
        study.optimize(objective, n_trials=n_trials, timeout=3600)
        
        self.best_params['xgboost'] = study.best_params
        self.cv_scores['xgboost'] = study.best_value
        
        print(f"\nXGBoost ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in self.best_params['xgboost'].items():
            print(f"  {key}: {value}")
        print(f"ìµœì  SMAPE: {self.cv_scores['xgboost']:.4f}")
        
        return study.best_params
    
    def optimize_catboost(self, n_trials=200):
        """CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("CatBoost ìµœì í™” ì‹œì‘...")
        
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        def objective(trial):
            params = {
                'iterations': trial.suggest_int('iterations', 500, 3000),
                'depth': trial.suggest_int('depth', 4, 12),
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 30),
                'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli']),
                'random_seed': self.random_state,
                'verbose': False,
                'thread_count': -1,
                'border_count': trial.suggest_int('border_count', 32, 255),
                'feature_border_type': trial.suggest_categorical('feature_border_type', ['Median', 'GreedyLogSum'])
            }
            
            # GPU ì„¤ì •
            if self.use_gpu:
                params.update({
                    'task_type': 'GPU',
                    'devices': '0',
                    'gpu_ram_part': 0.8
                })
            
            if params['bootstrap_type'] == 'Bayesian':
                params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)
            elif params['bootstrap_type'] == 'Bernoulli':
                params['subsample'] = trial.suggest_float('subsample', 0.4, 1)
            
            try:
                model = CatBoostRegressor(**params)
                
                model.fit(
                    X_train_split, y_train_split,
                    eval_set=(X_val_split, y_val_split),
                    early_stopping_rounds=100,
                    verbose=False
                )
                
                y_pred = model.predict(X_val_split)
                
                # SMAPE ê³„ì‚° (log ì—­ë³€í™˜ í›„)
                y_val_original = safe_exp_transform(y_val_split.values)
                y_pred_original = safe_exp_transform(y_pred)
                y_pred_original = np.maximum(y_pred_original, 0)
                
                smape_score = smape(y_val_original, y_pred_original)
                
                return smape_score
                
            except Exception as e:
                print(f"CatBoost Trial ì—ëŸ¬: {e}")
                return float('inf')
        
        study = optuna.create_study(
            direction='minimize',
            pruner=optuna.pruners.MedianPruner(n_startup_trials=20, n_warmup_steps=10)
        )
        
        study.optimize(objective, n_trials=n_trials, timeout=3600)
        
        self.best_params['catboost'] = study.best_params
        self.cv_scores['catboost'] = study.best_value
        
        print(f"\nCatBoost ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in self.best_params['catboost'].items():
            print(f"  {key}: {value}")
        print(f"ìµœì  SMAPE: {self.cv_scores['catboost']:.4f}")
        
        return study.best_params
    
    def train_final_models(self):
        """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ìµœì¢… ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘...")
        
        # Validation split
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        # LightGBM
        if 'lightgbm' in self.best_params:
            print("LightGBM ìµœì¢… ëª¨ë¸ í›ˆë ¨...")
            params = {
                'objective': 'regression',
                'metric': 'None',
                'boosting_type': 'gbdt',
                'verbosity': -1,
                'random_state': self.random_state,
                'n_jobs': -1
            }
            params.update(self.best_params['lightgbm'])
            
            if self.use_gpu:
                params.update({
                    'device_type': 'gpu',
                    'gpu_platform_id': 0,
                    'gpu_device_id': 0,
                    'gpu_use_dp': False
                })
            
            train_set = lgb.Dataset(self.X_train, label=self.y_train)
            self.models['lightgbm'] = lgb.train(
                params,
                train_set,
                num_boost_round=3000,
                callbacks=[lgb.log_evaluation(100)]
            )
        
        # XGBoost
        if 'xgboost' in self.best_params:
            print("XGBoost ìµœì¢… ëª¨ë¸ í›ˆë ¨...")
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'random_state': self.random_state,
                'n_jobs': -1
            }
            params.update(self.best_params['xgboost'])
            
            if self.use_gpu:
                params.update({
                    'tree_method': 'hist',
                    'device': 'cuda'
                })
            
            dtrain = xgb.DMatrix(self.X_train, label=self.y_train)
            self.models['xgboost'] = xgb.train(
                params,
                dtrain,
                num_boost_round=params.get('n_estimators', 2000),
                verbose_eval=100
            )
        
        # CatBoost
        if 'catboost' in self.best_params:
            print("CatBoost ìµœì¢… ëª¨ë¸ í›ˆë ¨...")
            params = {
                'random_seed': self.random_state,
                'verbose': 100,
                'thread_count': -1
            }
            params.update(self.best_params['catboost'])
            
            if self.use_gpu:
                params.update({
                    'task_type': 'GPU',
                    'devices': '0',
                    'gpu_ram_part': 0.8
                })
            
            self.models['catboost'] = CatBoostRegressor(**params)
            self.models['catboost'].fit(self.X_train, self.y_train)
        
        print("ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        
    def predict_ensemble(self, weights=None):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        print("ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        
        predictions = {}
        
        # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡
        for model_name, model in self.models.items():
            if model_name == 'lightgbm':
                y_pred_log = model.predict(self.X_test)
            elif model_name == 'xgboost':
                dtest = xgb.DMatrix(self.X_test)
                y_pred_log = model.predict(dtest)
            elif model_name == 'catboost':
                y_pred_log = model.predict(self.X_test)
            
            # Log ì—­ë³€í™˜
            y_pred_final = safe_exp_transform(y_pred_log)
            y_pred_final = np.maximum(y_pred_final, 0)
            
            predictions[model_name] = y_pred_final
            
            print(f"{model_name} ì˜ˆì¸¡ ì™„ë£Œ - ë²”ìœ„: {y_pred_final.min():.2f} ~ {y_pred_final.max():.2f}")
        
        # ì•™ìƒë¸” (ê°€ì¤‘í‰ê· )
        if weights is None:
            # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            total_score = sum(1/score for score in self.cv_scores.values())
            weights = {name: (1/score)/total_score for name, score in self.cv_scores.items()}
        
        print(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {weights}")
        
        ensemble_pred = np.zeros(len(self.X_test))
        for model_name, weight in weights.items():
            if model_name in predictions:
                ensemble_pred += weight * predictions[model_name]
        
        self.predictions = ensemble_pred
        self.individual_predictions = predictions
        
        print(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ: {len(self.predictions)}ê°œ")
        print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {self.predictions.min():.2f} ~ {self.predictions.max():.2f}")
        print(f"ì˜ˆì¸¡ê°’ í‰ê· : {self.predictions.mean():.2f}")
        
        return self.predictions
    
    def save_models(self, suffix=""):
        """ëª¨ë¸ë“¤ ì €ì¥"""
        model_path = f'../models/enhanced_models{suffix}.pkl'
        
        model_data = {
            'models': self.models,
            'best_params': self.best_params,
            'cv_scores': self.cv_scores,
            'feature_columns': self.feature_columns,
            'predictions': getattr(self, 'predictions', None),
            'individual_predictions': getattr(self, 'individual_predictions', None)
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
            
        print(f"ëª¨ë¸ ì €ì¥: {model_path}")
    
    def create_submission(self, suffix=""):
        """ì œì¶œ íŒŒì¼ ìƒì„±"""
        if not hasattr(self, 'predictions'):
            print("ì˜ˆì¸¡ì´ ë¨¼ì € ìˆ˜í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return None
        
        submission = pd.DataFrame({
            'num_date_time': self.test_df['num_date_time'],
            'answer': self.predictions
        })
        
        submission_path = f'submission_enhanced{suffix}.csv'
        submission.to_csv(submission_path, index=False)
        
        print(f"ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
        return submission


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("í–¥ìƒëœ ì „ë ¥ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ë§ ì‹œì‘")
    print("ëª©í‘œ: SMAPE â‰¤ 6.0")
    print("=" * 60)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    predictor = EnhancedPowerPredictor(use_gpu=True, random_state=42)
    
    # ë°ì´í„° ë¡œë“œ (ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ iqr_log ë²„ì „ ì‚¬ìš©)
    predictor.load_processed_data('iqr_log')
    
    # í”¼ì²˜ ì¤€ë¹„
    predictor.prepare_features()
    
    # ê° ëª¨ë¸ ìµœì í™” (ë³‘ë ¬ë¡œ ìˆ˜í–‰)
    print("\n=== ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ===")
    predictor.optimize_lightgbm(n_trials=100)
    predictor.optimize_xgboost(n_trials=100)
    predictor.optimize_catboost(n_trials=100)
    
    # ìµœì¢… ëª¨ë¸ í›ˆë ¨
    print("\n=== ìµœì¢… ëª¨ë¸ í›ˆë ¨ ===")
    predictor.train_final_models()
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    print("\n=== ì•™ìƒë¸” ì˜ˆì¸¡ ===")
    predictor.predict_ensemble()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ìµœì¢… ê²°ê³¼ ===")
    print("ëª¨ë¸ë³„ CV SMAPE:")
    for model_name, score in predictor.cv_scores.items():
        print(f"  {model_name}: {score:.4f}")
    
    best_score = min(predictor.cv_scores.values())
    print(f"\nìµœê³  ì„±ëŠ¥: {best_score:.4f}")
    
    if best_score <= 6.0:
        print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! SMAPE â‰¤ 6.0")
    else:
        print(f"ëª©í‘œê¹Œì§€ {best_score - 6.0:.4f} ë” ê°œì„  í•„ìš”")
    
    # ëª¨ë¸ ì €ì¥
    predictor.save_models("_v1")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = predictor.create_submission("_v1")
    
    print("\nëª¨ë¸ë§ ì™„ë£Œ!")


if __name__ == "__main__":
    main()