#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì „ë ¥ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ë§ - SMAPE 6 ì´í•˜ ë‹¬ì„± ëª©í‘œ
Author: Claude  
Date: 2025-07-27
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
from catboost import CatBoostRegressor
import optuna
import warnings
import pickle
import os
import gc
from datetime import datetime
from sklearn.metrics import mean_absolute_error

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')

# GPU ì§€ì› í™•ì¸
try:
    import torch
    GPU_AVAILABLE = torch.cuda.is_available()
    if GPU_AVAILABLE:
        print(f"GPU ê°ì§€ë¨: {torch.cuda.get_device_name(0)}")
except ImportError:
    GPU_AVAILABLE = False
    print("PyTorch ì—†ìŒ - GPU ì‚¬ìš© ë¶ˆê°€")

# SMAPE ê³„ì‚° í•¨ìˆ˜
def smape(y_true, y_pred, epsilon=1e-8):
    """SMAPE (Symmetric Mean Absolute Percentage Error) ê³„ì‚°"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # ë¶„ëª¨ê°€ 0ì— ê°€ê¹Œìš´ ê²½ìš° ì²˜ë¦¬
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denominator = np.maximum(denominator, epsilon)
    
    smape_val = np.mean(np.abs(y_true - y_pred) / denominator) * 100
    return smape_val

def safe_exp_transform(y_log):
    """ì•ˆì „í•œ exp ì—­ë³€í™˜"""
    # ë„ˆë¬´ í° ê°’ í´ë¦¬í•‘í•˜ì—¬ overflow ë°©ì§€
    y_log_clipped = np.clip(y_log, -10, 10)
    return np.expm1(y_log_clipped)


class FastPowerPredictor:
    """ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì „ë ¥ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, use_gpu=True, random_state=42):
        self.use_gpu = use_gpu and GPU_AVAILABLE
        self.random_state = random_state
        self.models = {}
        self.best_params = {}
        self.cv_scores = {}
        
        # ê²°ê³¼ ì €ì¥ í´ë”
        os.makedirs('../models', exist_ok=True)
        os.makedirs('../results', exist_ok=True)
        
        print(f"GPU ì‚¬ìš©: {self.use_gpu}")
        
    def load_processed_data(self, data_version='iqr_log'):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_version}")
        
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
        train_path = f'../processed_data/train_processed_{data_version}.csv'
        test_path = f'../processed_data/test_processed_{data_version}.csv'
        features_path = f'../processed_data/feature_columns_{data_version}.txt'
        
        self.train_df = pd.read_csv(train_path)
        self.test_df = pd.read_csv(test_path)
        
        # í”¼ì²˜ ì»¬ëŸ¼ ë¡œë“œ
        with open(features_path, 'r', encoding='utf-8') as f:
            self.feature_columns = [line.strip() for line in f.readlines()]
        
        print(f"Train: {self.train_df.shape}, Test: {self.test_df.shape}")
        print(f"Features: {len(self.feature_columns)}ê°œ")
        
        return self.train_df, self.test_df, self.feature_columns
    
    def prepare_features(self):
        """í”¼ì²˜ ì¤€ë¹„"""
        print("í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
        available_features = [col for col in self.feature_columns if col in self.train_df.columns]
        
        self.X_train = self.train_df[available_features]
        self.y_train = self.train_df['power_transformed']  # log ë³€í™˜ëœ target
        
        # Test í”¼ì²˜ (trainê³¼ ë™ì¼í•œ ìˆœì„œë¡œ)
        test_available_features = [col for col in available_features if col in self.test_df.columns]
        self.X_test = self.test_df[test_available_features]
        
        print(f"Train features: {self.X_train.shape[1]}")
        print(f"Test features: {self.X_test.shape[1]}")
        
        # NaN ì²˜ë¦¬
        self.X_train = self.X_train.fillna(0)
        self.X_test = self.X_test.fillna(0)
        
        return self.X_train, self.y_train, self.X_test
    
    def create_validation_split(self, test_ratio=0.15):
        """ì‹œê³„ì—´ ê³ ë ¤í•œ validation split ìƒì„±"""
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        if 'ì¼ì‹œ' in self.train_df.columns:
            sorted_indices = self.train_df.sort_values('ì¼ì‹œ').index
        else:
            sorted_indices = self.train_df.index
        
        # ë§ˆì§€ë§‰ 15%ë¥¼ validationìœ¼ë¡œ ì‚¬ìš©
        split_idx = int(len(sorted_indices) * (1 - test_ratio))
        
        train_indices = sorted_indices[:split_idx]
        val_indices = sorted_indices[split_idx:]
        
        X_train_split = self.X_train.loc[train_indices]
        y_train_split = self.y_train.loc[train_indices]
        X_val_split = self.X_train.loc[val_indices]
        y_val_split = self.y_train.loc[val_indices]
        
        print(f"Train split: {len(X_train_split)}, Val split: {len(X_val_split)}")
        
        return X_train_split, X_val_split, y_train_split, y_val_split
    
    def optimize_lightgbm(self, n_trials=50):
        """LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ê°„ë‹¨í•œ ë²„ì „)"""
        print("LightGBM ìµœì í™” ì‹œì‘...")
        
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        def objective(trial):
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 31, 200),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
                'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),
                'lambda_l2': trial.suggest_float('lambda_l2', 0, 10),
                'verbosity': -1,
                'random_state': self.random_state,
                'n_jobs': -1
            }
            
            # GPU ì„¤ì • (ì„ íƒì‚¬í•­)
            if self.use_gpu:
                try:
                    params.update({
                        'device_type': 'gpu',
                        'gpu_platform_id': 0,
                        'gpu_device_id': 0,
                    })
                except:
                    # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ fallback
                    pass
            
            try:
                # ê°„ë‹¨í•œ train/validation
                model = lgb.LGBMRegressor(**params, n_estimators=1000)
                model.fit(
                    X_train_split, y_train_split,
                    eval_set=[(X_val_split, y_val_split)],
                    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
                )
                
                y_pred = model.predict(X_val_split)
                
                # SMAPE ê³„ì‚° (log ì—­ë³€í™˜ í›„)
                y_val_original = safe_exp_transform(y_val_split.values)
                y_pred_original = safe_exp_transform(y_pred)
                y_pred_original = np.maximum(y_pred_original, 0)
                
                smape_score = smape(y_val_original, y_pred_original)
                
                return smape_score
                
            except Exception as e:
                print(f"Trial ì—ëŸ¬: {e}")
                return float('inf')
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=n_trials, timeout=1800)  # 30ë¶„ ì œí•œ
        
        self.best_params['lightgbm'] = study.best_params
        self.cv_scores['lightgbm'] = study.best_value
        
        print(f"\nLightGBM ìµœì  SMAPE: {self.cv_scores['lightgbm']:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params['lightgbm']}")
        
        return study.best_params
    
    def optimize_xgboost(self, n_trials=50):
        """XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("XGBoost ìµœì í™” ì‹œì‘...")
        
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        def objective(trial):
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                'n_estimators': 1000,
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                'random_state': self.random_state,
                'n_jobs': -1
            }
            
            # GPU ì„¤ì • (ì„ íƒì‚¬í•­)
            if self.use_gpu:
                try:
                    params.update({
                        'tree_method': 'hist',
                        'device': 'cuda'
                    })
                except:
                    # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ fallback
                    pass
            
            try:
                model = xgb.XGBRegressor(**params)
                model.fit(
                    X_train_split, y_train_split,
                    eval_set=[(X_val_split, y_val_split)],
                    early_stopping_rounds=100,
                    verbose=False
                )
                
                y_pred = model.predict(X_val_split)
                
                # SMAPE ê³„ì‚° (log ì—­ë³€í™˜ í›„)
                y_val_original = safe_exp_transform(y_val_split.values)
                y_pred_original = safe_exp_transform(y_pred)
                y_pred_original = np.maximum(y_pred_original, 0)
                
                smape_score = smape(y_val_original, y_pred_original)
                
                return smape_score
                
            except Exception as e:
                print(f"XGBoost Trial ì—ëŸ¬: {e}")
                return float('inf')
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=n_trials, timeout=1800)  # 30ë¶„ ì œí•œ
        
        self.best_params['xgboost'] = study.best_params
        self.cv_scores['xgboost'] = study.best_value
        
        print(f"\nXGBoost ìµœì  SMAPE: {self.cv_scores['xgboost']:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params['xgboost']}")
        
        return study.best_params
    
    def optimize_catboost(self, n_trials=50):
        """CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("CatBoost ìµœì í™” ì‹œì‘...")
        
        X_train_split, X_val_split, y_train_split, y_val_split = self.create_validation_split()
        
        def objective(trial):
            params = {
                'iterations': 1000,
                'depth': trial.suggest_int('depth', 4, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 20),
                'random_seed': self.random_state,
                'verbose': False,
                'early_stopping_rounds': 100
            }
            
            # GPU ì„¤ì • (ì„ íƒì‚¬í•­)
            if self.use_gpu:
                try:
                    params.update({
                        'task_type': 'GPU',
                        'devices': '0'
                    })
                except:
                    # GPU ì‹¤íŒ¨ì‹œ CPUë¡œ fallback
                    pass
            
            try:
                model = CatBoostRegressor(**params)
                
                model.fit(
                    X_train_split, y_train_split,
                    eval_set=(X_val_split, y_val_split),
                    verbose=False
                )
                
                y_pred = model.predict(X_val_split)
                
                # SMAPE ê³„ì‚° (log ì—­ë³€í™˜ í›„)
                y_val_original = safe_exp_transform(y_val_split.values)
                y_pred_original = safe_exp_transform(y_pred)
                y_pred_original = np.maximum(y_pred_original, 0)
                
                smape_score = smape(y_val_original, y_pred_original)
                
                return smape_score
                
            except Exception as e:
                print(f"CatBoost Trial ì—ëŸ¬: {e}")
                return float('inf')
        
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=n_trials, timeout=1800)  # 30ë¶„ ì œí•œ
        
        self.best_params['catboost'] = study.best_params
        self.cv_scores['catboost'] = study.best_value
        
        print(f"\nCatBoost ìµœì  SMAPE: {self.cv_scores['catboost']:.4f}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {self.best_params['catboost']}")
        
        return study.best_params
    
    def train_final_models(self):
        """ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ë“¤ í›ˆë ¨"""
        print("ìµœì¢… ëª¨ë¸ë“¤ í›ˆë ¨ ì¤‘...")
        
        # LightGBM
        if 'lightgbm' in self.best_params:
            print("LightGBM ìµœì¢… ëª¨ë¸ í›ˆë ¨...")
            params = self.best_params['lightgbm'].copy()
            params.update({
                'objective': 'regression',
                'metric': 'rmse',
                'verbosity': -1,
                'random_state': self.random_state,
                'n_jobs': -1,
                'n_estimators': 2000
            })
            
            if self.use_gpu:
                try:
                    params.update({
                        'device_type': 'gpu',
                        'gpu_platform_id': 0,
                        'gpu_device_id': 0
                    })
                except:
                    pass
            
            self.models['lightgbm'] = lgb.LGBMRegressor(**params)
            self.models['lightgbm'].fit(self.X_train, self.y_train)
        
        # XGBoost
        if 'xgboost' in self.best_params:
            print("XGBoost ìµœì¢… ëª¨ë¸ í›ˆë ¨...")
            params = self.best_params['xgboost'].copy()
            params.update({
                'objective': 'reg:squarederror',
                'random_state': self.random_state,
                'n_jobs': -1,
                'n_estimators': 2000
            })
            
            if self.use_gpu:
                try:
                    params.update({
                        'tree_method': 'hist',
                        'device': 'cuda'
                    })
                except:
                    pass
            
            self.models['xgboost'] = xgb.XGBRegressor(**params)
            self.models['xgboost'].fit(self.X_train, self.y_train)
        
        # CatBoost
        if 'catboost' in self.best_params:
            print("CatBoost ìµœì¢… ëª¨ë¸ í›ˆë ¨...")
            params = self.best_params['catboost'].copy()
            params.update({
                'random_seed': self.random_state,
                'verbose': False,
                'iterations': 2000
            })
            
            if self.use_gpu:
                try:
                    params.update({
                        'task_type': 'GPU',
                        'devices': '0'
                    })
                except:
                    pass
            
            self.models['catboost'] = CatBoostRegressor(**params)
            self.models['catboost'].fit(self.X_train, self.y_train)
        
        print("ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
    
    def predict_ensemble(self, weights=None):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        print("ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
        
        predictions = {}
        
        # ê° ëª¨ë¸ë¡œ ì˜ˆì¸¡
        for model_name, model in self.models.items():
            y_pred_log = model.predict(self.X_test)
            
            # Log ì—­ë³€í™˜
            y_pred_final = safe_exp_transform(y_pred_log)
            y_pred_final = np.maximum(y_pred_final, 0)
            
            predictions[model_name] = y_pred_final
            
            print(f"{model_name} ì˜ˆì¸¡ ì™„ë£Œ - ë²”ìœ„: {y_pred_final.min():.2f} ~ {y_pred_final.max():.2f}")
        
        # ì•™ìƒë¸” (ê°€ì¤‘í‰ê· )
        if weights is None:
            # ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
            total_score = sum(1/score for score in self.cv_scores.values())
            weights = {name: (1/score)/total_score for name, score in self.cv_scores.items()}
        
        print(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {weights}")
        
        ensemble_pred = np.zeros(len(self.X_test))
        for model_name, weight in weights.items():
            if model_name in predictions:
                ensemble_pred += weight * predictions[model_name]
        
        self.predictions = ensemble_pred
        self.individual_predictions = predictions
        
        print(f"ì•™ìƒë¸” ì˜ˆì¸¡ ì™„ë£Œ: {len(self.predictions)}ê°œ")
        print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {self.predictions.min():.2f} ~ {self.predictions.max():.2f}")
        print(f"ì˜ˆì¸¡ê°’ í‰ê· : {self.predictions.mean():.2f}")
        
        return self.predictions
    
    def create_submission(self, suffix=""):
        """ì œì¶œ íŒŒì¼ ìƒì„±"""
        if not hasattr(self, 'predictions'):
            print("ì˜ˆì¸¡ì´ ë¨¼ì € ìˆ˜í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return None
        
        submission = pd.DataFrame({
            'num_date_time': self.test_df['num_date_time'],
            'answer': self.predictions
        })
        
        submission_path = f'submission{suffix}.csv'
        submission.to_csv(submission_path, index=False)
        
        print(f"ì œì¶œ íŒŒì¼ ìƒì„±: {submission_path}")
        return submission


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ë¹ ë¥¸ ì „ë ¥ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ë§ ì‹œì‘")
    print("ëª©í‘œ: SMAPE â‰¤ 6.0")
    print("=" * 60)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    predictor = FastPowerPredictor(use_gpu=True, random_state=42)
    
    # ë°ì´í„° ë¡œë“œ - ì„¸ê°€ì§€ ë²„ì „ ëª¨ë‘ í…ŒìŠ¤íŠ¸
    best_score = float('inf')
    best_version = None
    
    for data_version in ['none_log', 'iqr_log', 'building_percentile_log']:
        print(f"\n{'='*60}")
        print(f"ë°ì´í„° ë²„ì „ í…ŒìŠ¤íŠ¸: {data_version}")
        print(f"{'='*60}")
        
        try:
            # ë°ì´í„° ë¡œë“œ
            predictor.load_processed_data(data_version)
            predictor.prepare_features()
            
            # LightGBMë§Œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
            predictor.optimize_lightgbm(n_trials=30)
            
            current_score = predictor.cv_scores['lightgbm']
            print(f"{data_version} - SMAPE: {current_score:.4f}")
            
            if current_score < best_score:
                best_score = current_score
                best_version = data_version
                
        except Exception as e:
            print(f"{data_version} ì‹¤íŒ¨: {e}")
            continue
    
    print(f"\nìµœê³  ì„±ëŠ¥ ë°ì´í„° ë²„ì „: {best_version} (SMAPE: {best_score:.4f})")
    
    # ìµœê³  ì„±ëŠ¥ ë²„ì „ìœ¼ë¡œ ì „ì²´ ëª¨ë¸ í›ˆë ¨
    print(f"\n{'='*60}")
    print(f"ìµœê³  ì„±ëŠ¥ ë²„ì „ìœ¼ë¡œ ì „ì²´ ëª¨ë¸ ìµœì í™”: {best_version}")
    print(f"{'='*60}")
    
    predictor = FastPowerPredictor(use_gpu=True, random_state=42)
    predictor.load_processed_data(best_version)
    predictor.prepare_features()
    
    # ëª¨ë“  ëª¨ë¸ ìµœì í™”
    predictor.optimize_lightgbm(n_trials=100)
    predictor.optimize_xgboost(n_trials=100)
    predictor.optimize_catboost(n_trials=100)
    
    # ìµœì¢… ëª¨ë¸ í›ˆë ¨
    predictor.train_final_models()
    
    # ì•™ìƒë¸” ì˜ˆì¸¡
    predictor.predict_ensemble()
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ìµœì¢… ê²°ê³¼")
    print(f"{'='*60}")
    print("ëª¨ë¸ë³„ CV SMAPE:")
    for model_name, score in predictor.cv_scores.items():
        print(f"  {model_name}: {score:.4f}")
    
    best_individual_score = min(predictor.cv_scores.values())
    print(f"\nìµœê³  ê°œë³„ ëª¨ë¸ ì„±ëŠ¥: {best_individual_score:.4f}")
    
    if best_individual_score <= 6.0:
        print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! SMAPE â‰¤ 6.0")
    else:
        print(f"ëª©í‘œê¹Œì§€ {best_individual_score - 6.0:.4f} ë” ê°œì„  í•„ìš”")
    
    # ì œì¶œ íŒŒì¼ ìƒì„±
    submission = predictor.create_submission(f"_{best_version}_final")
    
    print("\nëª¨ë¸ë§ ì™„ë£Œ!")
    print(f"ì œì¶œ íŒŒì¼: submission_{best_version}_final.csv")


if __name__ == "__main__":
    main()