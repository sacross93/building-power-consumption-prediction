#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ì „ë ¥ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ - method_05 ê¸°ë²• ì ìš©
ëª©í‘œ: SMAPE 10 ì´í•˜ ë‹¬ì„±
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge
import optuna
import warnings
from datetime import datetime

warnings.filterwarnings('ignore')

def smape(y_true, y_pred, epsilon=1e-8):
    """SMAPE ê³„ì‚°"""
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    numerator = np.abs(y_pred - y_true)
    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    mask = denominator > epsilon
    smape_values = np.zeros_like(numerator, dtype=float)
    smape_values[mask] = numerator[mask] / denominator[mask]
    
    return 100.0 * np.mean(smape_values)

class ImprovedPowerPredictor:
    """ê°œì„ ëœ ì „ë ¥ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.models = {}
        self.best_params = {}
        self.feature_importance = {}
        
    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì¤€ë¹„"""
        print("ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        train_df = pd.read_csv('../data/train.csv', encoding='utf-8-sig')
        test_df = pd.read_csv('../data/test.csv', encoding='utf-8-sig')
        building_info = pd.read_csv('../data/building_info.csv', encoding='utf-8-sig')
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        for df in [train_df, test_df, building_info]:
            df.columns = df.columns.str.strip()
        
        # ê±´ë¬¼ ì •ë³´ ë³‘í•©
        train_df = train_df.merge(building_info, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
        test_df = test_df.merge(building_info, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
        
        print(f"Train: {train_df.shape}, Test: {test_df.shape}")
        return train_df, test_df, building_info
    
    def advanced_feature_engineering(self, train_df, test_df):
        """ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ - method_05 ê¸°ë²• ì ìš©"""
        print("ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
        
        # 1. ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ìƒì„±
        for df in [train_df, test_df]:
            df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
            df['year'] = df['ì¼ì‹œ'].dt.year
            df['month'] = df['ì¼ì‹œ'].dt.month
            df['day'] = df['ì¼ì‹œ'].dt.day
            df['hour'] = df['ì¼ì‹œ'].dt.hour
            df['weekday'] = df['ì¼ì‹œ'].dt.weekday
            df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)
        
        # 2. ê±´ë¬¼ ì •ë³´ ì „ì²˜ë¦¬
        numeric_building_cols = ['ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 
                               'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']
        
        for col in numeric_building_cols:
            # '-' ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ í›„ ì¤‘ì•™ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            for df in [train_df, test_df]:
                df[col] = df[col].replace('-', np.nan)
                df[col] = pd.to_numeric(df[col], errors='coerce')
                median_val = train_df[col].median()
                df[col] = df[col].fillna(median_val)
        
        # 3. ê±´ë¬¼ë³„ í†µê³„ í”¼ì²˜ ìƒì„± (method_05ì˜ í•µì‹¬ ê¸°ë²•)
        print("ê±´ë¬¼ë³„ í†µê³„ í”¼ì²˜ ìƒì„±...")
        
        # ì „ì²´ ê±´ë¬¼ë³„ í‰ê· 
        building_mean = train_df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].mean()
        
        # ê±´ë¬¼ë³„ ì‹œê°„ëŒ€ë³„ í‰ê· 
        bld_hour_mean = (
            train_df.groupby(['ê±´ë¬¼ë²ˆí˜¸', 'hour'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
            .mean()
            .reset_index()
            .rename(columns={'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'bld_hour_mean'})
        )
        train_df = train_df.merge(bld_hour_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'hour'], how='left')
        test_df = test_df.merge(bld_hour_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'hour'], how='left')
        test_df['bld_hour_mean'] = test_df['bld_hour_mean'].fillna(
            test_df['ê±´ë¬¼ë²ˆí˜¸'].map(building_mean)
        )
        
        # ê±´ë¬¼ë³„ ìš”ì¼ë³„ í‰ê· 
        bld_weekday_mean = (
            train_df.groupby(['ê±´ë¬¼ë²ˆí˜¸', 'weekday'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
            .mean()
            .reset_index()
            .rename(columns={'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'bld_weekday_mean'})
        )
        train_df = train_df.merge(bld_weekday_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'weekday'], how='left')
        test_df = test_df.merge(bld_weekday_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'weekday'], how='left')
        test_df['bld_weekday_mean'] = test_df['bld_weekday_mean'].fillna(
            test_df['ê±´ë¬¼ë²ˆí˜¸'].map(building_mean)
        )
        
        # ê±´ë¬¼ë³„ ì›”ë³„ í‰ê· 
        bld_month_mean = (
            train_df.groupby(['ê±´ë¬¼ë²ˆí˜¸', 'month'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
            .mean()
            .reset_index()
            .rename(columns={'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)': 'bld_month_mean'})
        )
        train_df = train_df.merge(bld_month_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'month'], how='left')
        test_df = test_df.merge(bld_month_mean, on=['ê±´ë¬¼ë²ˆí˜¸', 'month'], how='left')
        test_df['bld_month_mean'] = test_df['bld_month_mean'].fillna(
            test_df['ê±´ë¬¼ë²ˆí˜¸'].map(building_mean)
        )
        
        # 4. ëˆ„ë½ ë°ì´í„° ëŒ€ì²´ (method_05 ê¸°ë²•)
        print("ëˆ„ë½ ë°ì´í„° ëŒ€ì²´ ì¤‘...")
        
        # 8ì›” ë°ì´í„°ì˜ ì‹œê°„ëŒ€ë³„ í‰ê· ìœ¼ë¡œ ì¼ì¡°/ì¼ì‚¬ëŸ‰ ì¶”ì •
        train_august = train_df[train_df['month'] == 8]
        avg_sunshine = train_august.groupby('hour')['ì¼ì¡°(hr)'].mean()
        avg_solar = train_august.groupby('hour')['ì¼ì‚¬(MJ/m2)'].mean()
        
        # Trainì—ëŠ” ì›ë³¸ ë°ì´í„° ì‚¬ìš©, Testì—ëŠ” ì¶”ì •ê°’ ì‚¬ìš©
        train_df['sunshine_est'] = train_df['ì¼ì¡°(hr)']
        train_df['solar_est'] = train_df['ì¼ì‚¬(MJ/m2)']
        test_df['sunshine_est'] = test_df['hour'].map(avg_sunshine)
        test_df['solar_est'] = test_df['hour'].map(avg_solar)
        
        # 5. ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±
        print("ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±...")
        
        for df in [train_df, test_df]:
            # ê¸°ìƒ ìƒí˜¸ì‘ìš©
            df['humidity_temp'] = df['ìŠµë„(%)'] * df['ê¸°ì˜¨(Â°C)']
            df['rain_wind'] = df['ê°•ìˆ˜ëŸ‰(mm)'] * df['í’ì†(m/s)']
            df['temp_wind'] = df['ê¸°ì˜¨(Â°C)'] * df['í’ì†(m/s)']
            
            # ê±´ë¬¼ ê´€ë ¨ ë¹„ìœ¨
            df['cooling_area_ratio'] = df['ëƒ‰ë°©ë©´ì (m2)'] / df['ì—°ë©´ì (m2)']
            df['pv_per_area'] = df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'] / df['ì—°ë©´ì (m2)']
            df['ess_per_area'] = df['ESSì €ì¥ìš©ëŸ‰(kWh)'] / df['ì—°ë©´ì (m2)']
            
            # ê¸°ìƒê³¼ ê±´ë¬¼ ìƒí˜¸ì‘ìš©
            df['temp_area'] = df['ê¸°ì˜¨(Â°C)'] * df['ì—°ë©´ì (m2)']
            df['humidity_cooling_area'] = df['ìŠµë„(%)'] * df['ëƒ‰ë°©ë©´ì (m2)']
        
        # 6. Weather lag features ì¶”ê°€
        print("Weather lag features ìƒì„±...")
        
        weather_cols = ['ê¸°ì˜¨(Â°C)', 'ìŠµë„(%)', 'í’ì†(m/s)', 'ê°•ìˆ˜ëŸ‰(mm)']
        
        # 1ì‹œê°„, 3ì‹œê°„ ì „ ê¸°ìƒ ë°ì´í„°
        for col in weather_cols:
            for lag in [1, 3]:
                train_df[f'{col}_lag_{lag}h'] = train_df.groupby('ê±´ë¬¼ë²ˆí˜¸')[col].shift(lag)
                # TestëŠ” ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
                if len(train_df) > 0:
                    last_values = train_df.groupby('ê±´ë¬¼ë²ˆí˜¸')[col].tail(lag).reset_index(drop=True)
                    test_df[f'{col}_lag_{lag}h'] = test_df.groupby('ê±´ë¬¼ë²ˆí˜¸')[col].transform(lambda x: x.iloc[0])
        
        # 7. Rolling statistics
        print("Rolling statistics ìƒì„±...")
        
        for col in weather_cols:
            for window in [6, 12, 24]:
                train_df[f'{col}_rolling_mean_{window}h'] = (
                    train_df.groupby('ê±´ë¬¼ë²ˆí˜¸')[col]
                    .rolling(window=window, min_periods=1)
                    .mean()
                    .reset_index(0, drop=True)
                )
                # TestëŠ” ê° ê±´ë¬¼ë³„ ìµœê·¼ í‰ê· ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
                building_recent_mean = {}
                for building in train_df['ê±´ë¬¼ë²ˆí˜¸'].unique():
                    building_data = train_df[train_df['ê±´ë¬¼ë²ˆí˜¸'] == building][col].tail(window)
                    building_recent_mean[building] = building_data.mean()
                
                test_df[f'{col}_rolling_mean_{window}h'] = test_df['ê±´ë¬¼ë²ˆí˜¸'].map(building_recent_mean)
        
        print(f"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ - Train: {train_df.shape[1]}ê°œ ì»¬ëŸ¼, Test: {test_df.shape[1]}ê°œ ì»¬ëŸ¼")
        
        return train_df, test_df
    
    def prepare_features_for_modeling(self, train_df, test_df):
        """ëª¨ë¸ë§ì„ ìœ„í•œ í”¼ì²˜ ì¤€ë¹„"""
        print("ëª¨ë¸ë§ìš© í”¼ì²˜ ì¤€ë¹„...")
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤
        exclude_cols = ['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)', 'ì¼ì‹œ', 'num_date_time']
        
        # ê³µí†µ í”¼ì²˜ ì„ íƒ
        feature_cols = [col for col in train_df.columns 
                       if col not in exclude_cols and col in test_df.columns]
        
        X_train = train_df[feature_cols].copy()
        y_train = train_df['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
        X_test = test_df[feature_cols].copy()
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
        categorical_cols = ['ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•']
        for col in categorical_cols:
            if col in X_train.columns:
                X_train[col] = X_train[col].astype(str)
                X_test[col] = X_test[col].astype(str)
        
        # NaN ì²˜ë¦¬
        X_train = X_train.fillna(0)
        X_test = X_test.fillna(0)
        
        print(f"ìµœì¢… í”¼ì²˜ ìˆ˜: {len(feature_cols)}")
        print(f"ë²”ì£¼í˜• í”¼ì²˜: {[col for col in categorical_cols if col in feature_cols]}")
        
        return X_train, y_train, X_test, feature_cols, categorical_cols
    
    def chronological_split(self, train_df, validation_days=7):
        """ì‹œê³„ì—´ ê³ ë ¤í•œ chronological split"""
        print(f"Chronological split - ë§ˆì§€ë§‰ {validation_days}ì¼ì„ validationìœ¼ë¡œ ì‚¬ìš©")
        
        train_df = train_df.sort_values('ì¼ì‹œ')
        cutoff_date = train_df['ì¼ì‹œ'].max() - pd.Timedelta(days=validation_days)
        
        train_mask = train_df['ì¼ì‹œ'] < cutoff_date
        val_mask = ~train_mask
        
        print(f"Train samples: {train_mask.sum()}")
        print(f"Validation samples: {val_mask.sum()}")
        print(f"Cutoff date: {cutoff_date}")
        
        return train_mask, val_mask
    
    def optimize_models(self, X_train, y_train, categorical_cols, n_trials=100):
        """ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("ëª¨ë¸ ìµœì í™” ì‹œì‘...")
        
        # Chronological split for validation
        train_df_temp = pd.concat([X_train, y_train], axis=1)
        train_df_temp['ì¼ì‹œ'] = pd.to_datetime(train_df_temp.index.map(
            lambda x: f"2024-06-01 00:00:00"  # ì„ì‹œ ë‚ ì§œ (ì‹¤ì œë¡œëŠ” ì›ë³¸ ë°ì´í„° ì‚¬ìš©)
        )) + pd.to_timedelta(train_df_temp.index, unit='H')
        
        # ê°„ë‹¨í•œ split (ë§ˆì§€ë§‰ 20%)
        split_idx = int(len(X_train) * 0.8)
        X_tr, X_val = X_train.iloc[:split_idx], X_train.iloc[split_idx:]
        y_tr, y_val = y_train.iloc[:split_idx], y_train.iloc[split_idx:]
        
        # OneHot Encoder ì¤€ë¹„
        categorical_features = [col for col in categorical_cols if col in X_train.columns]
        preprocessor = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough'
        )
        
        X_tr_processed = preprocessor.fit_transform(X_tr)
        X_val_processed = preprocessor.transform(X_val)
        
        best_models = {}
        
        # 1. LightGBM ìµœì í™”
        print("LightGBM ìµœì í™”...")
        def lgb_objective(trial):
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'num_leaves': trial.suggest_int('num_leaves', 100, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
                'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
                'lambda_l1': trial.suggest_float('lambda_l1', 0, 5),
                'lambda_l2': trial.suggest_float('lambda_l2', 0, 5),
                'verbosity': -1,
                'random_state': self.random_state
            }
            
            model = lgb.LGBMRegressor(**params, n_estimators=1000)
            model.fit(X_tr_processed, y_tr, 
                     eval_set=[(X_val_processed, y_val)],
                     callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)])
            
            y_pred = model.predict(X_val_processed)
            return smape(y_val.values, y_pred)
        
        study_lgb = optuna.create_study(direction='minimize')
        study_lgb.optimize(lgb_objective, n_trials=n_trials//3)
        
        self.best_params['lgb'] = study_lgb.best_params
        best_models['lgb'] = study_lgb.best_value
        
        # 2. XGBoost ìµœì í™” (method_05 ìŠ¤íƒ€ì¼)
        print("XGBoost ìµœì í™”...")
        def xgb_objective(trial):
            params = {
                'max_depth': trial.suggest_int('max_depth', 8, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),
                'subsample': trial.suggest_float('subsample', 0.7, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),
                'random_state': self.random_state
            }
            
            model = xgb.XGBRegressor(**params)
            model.fit(X_tr_processed, y_tr, 
                     eval_set=[(X_val_processed, y_val)],
                     early_stopping_rounds=100, verbose=False)
            
            y_pred = model.predict(X_val_processed)
            return smape(y_val.values, y_pred)
        
        study_xgb = optuna.create_study(direction='minimize')
        study_xgb.optimize(xgb_objective, n_trials=n_trials//3)
        
        self.best_params['xgb'] = study_xgb.best_params
        best_models['xgb'] = study_xgb.best_value
        
        # 3. CatBoost ìµœì í™”
        print("CatBoost ìµœì í™”...")
        def cat_objective(trial):
            params = {
                'depth': trial.suggest_int('depth', 6, 12),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                'iterations': trial.suggest_int('iterations', 1000, 3000),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
                'random_seed': self.random_state,
                'verbose': False
            }
            
            model = CatBoostRegressor(**params)
            model.fit(X_tr_processed, y_tr, 
                     eval_set=(X_val_processed, y_val),
                     early_stopping_rounds=100, verbose=False)
            
            y_pred = model.predict(X_val_processed)
            return smape(y_val.values, y_pred)
        
        study_cat = optuna.create_study(direction='minimize')
        study_cat.optimize(cat_objective, n_trials=n_trials//3)
        
        self.best_params['cat'] = study_cat.best_params
        best_models['cat'] = study_cat.best_value
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nìµœì í™” ê²°ê³¼:")
        for model_name, score in best_models.items():
            print(f"{model_name.upper()}: {score:.4f}")
        
        best_model = min(best_models, key=best_models.get)
        print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()} (SMAPE: {best_models[best_model]:.4f})")
        
        return preprocessor, best_models
    
    def train_ensemble_models(self, X_train, y_train, X_test, preprocessor):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡"""
        print("ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨...")
        
        X_train_processed = preprocessor.transform(X_train)
        X_test_processed = preprocessor.transform(X_test)
        
        predictions = {}
        
        # LightGBM
        lgb_params = self.best_params['lgb']
        lgb_params.update({
            'objective': 'regression',
            'metric': 'rmse',
            'verbosity': -1,
            'random_state': self.random_state,
            'n_estimators': 2000
        })
        
        lgb_model = lgb.LGBMRegressor(**lgb_params)
        lgb_model.fit(X_train_processed, y_train)
        predictions['lgb'] = lgb_model.predict(X_test_processed)
        
        # XGBoost
        xgb_params = self.best_params['xgb']
        xgb_params.update({
            'random_state': self.random_state
        })
        
        xgb_model = xgb.XGBRegressor(**xgb_params)
        xgb_model.fit(X_train_processed, y_train)
        predictions['xgb'] = xgb_model.predict(X_test_processed)
        
        # CatBoost
        cat_params = self.best_params['cat']
        cat_params.update({
            'random_seed': self.random_state,
            'verbose': False
        })
        
        cat_model = CatBoostRegressor(**cat_params)
        cat_model.fit(X_train_processed, y_train)
        predictions['cat'] = cat_model.predict(X_test_processed)
        
        # ê°€ì¤‘ ì•™ìƒë¸” (ì„±ëŠ¥ ê¸°ë°˜)
        best_scores = {
            'lgb': 10.0,  # ì„ì‹œê°’, ì‹¤ì œë¡œëŠ” CV ì ìˆ˜ ì‚¬ìš©
            'xgb': 8.0,
            'cat': 12.0
        }
        
        # ì—­ìˆ˜ ê°€ì¤‘ì¹˜ (ë‚®ì€ SMAPEì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)
        total_weight = sum(1/score for score in best_scores.values())
        weights = {name: (1/score)/total_weight for name, score in best_scores.items()}
        
        ensemble_pred = np.zeros(len(predictions['lgb']))
        for model_name, weight in weights.items():
            ensemble_pred += weight * predictions[model_name]
        
        # ìŒìˆ˜ í´ë¦¬í•‘
        ensemble_pred = np.maximum(ensemble_pred, 0)
        
        print(f"ì•™ìƒë¸” ê°€ì¤‘ì¹˜: {weights}")
        print(f"ì˜ˆì¸¡ê°’ ë²”ìœ„: {ensemble_pred.min():.2f} ~ {ensemble_pred.max():.2f}")
        
        return ensemble_pred, predictions
    
    def run_complete_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=" * 60)
        print("ê°œì„ ëœ ì „ë ¥ì†Œë¹„ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ì‹¤í–‰")
        print("ëª©í‘œ: SMAPE 10 ì´í•˜")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        train_df, test_df, building_info = self.load_and_prepare_data()
        
        # 2. ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        train_df, test_df = self.advanced_feature_engineering(train_df, test_df)
        
        # 3. ëª¨ë¸ë§ìš© í”¼ì²˜ ì¤€ë¹„
        X_train, y_train, X_test, feature_cols, categorical_cols = (
            self.prepare_features_for_modeling(train_df, test_df)
        )
        
        # 4. ëª¨ë¸ ìµœì í™”
        preprocessor, best_scores = self.optimize_models(
            X_train, y_train, categorical_cols, n_trials=60
        )
        
        # 5. ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
        ensemble_pred, individual_preds = self.train_ensemble_models(
            X_train, y_train, X_test, preprocessor
        )
        
        # 6. ì œì¶œ íŒŒì¼ ìƒì„±
        submission = pd.DataFrame({
            'num_date_time': test_df['num_date_time'],
            'answer': ensemble_pred
        })
        
        submission.to_csv('submission_improved.csv', index=False)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*60}")
        print("ìµœì¢… ê²°ê³¼")
        print(f"{'='*60}")
        
        best_individual_score = min(best_scores.values())
        print(f"ìµœê³  ê°œë³„ ëª¨ë¸ ì„±ëŠ¥: {best_individual_score:.4f}")
        
        if best_individual_score <= 10.0:
            print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! SMAPE â‰¤ 10.0")
        else:
            print(f"ëª©í‘œê¹Œì§€ {best_individual_score - 10.0:.4f} ë” ê°œì„  í•„ìš”")
        
        print(f"ì‚¬ìš©ëœ í”¼ì²˜ ìˆ˜: {len(feature_cols)}")
        print("ì œì¶œ íŒŒì¼: submission_improved.csv")
        
        return best_individual_score

def main():
    predictor = ImprovedPowerPredictor(random_state=42)
    final_score = predictor.run_complete_pipeline()
    
    print(f"\nìµœì¢… ê²€ì¦ SMAPE: {final_score:.4f}")
    if final_score <= 10.0:
        print("ì„±ê³µì ìœ¼ë¡œ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤!")
    else:
        print(f"ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ({final_score - 10.0:.4f} ë”)")

if __name__ == "__main__":
    main()